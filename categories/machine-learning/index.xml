<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Machine Learning on aiken&#39;s blog</title>
    <link>https://hugotest-phi.vercel.app/categories/machine-learning/</link>
    <description>Recent content in Machine Learning on aiken&#39;s blog</description>
    <generator>Hugo -- 0.137.0</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 27 Jan 2024 15:42:36 +0000</lastBuildDate>
    <atom:link href="https://hugotest-phi.vercel.app/categories/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>StableDiffusionWebUI鉴权设计</title>
      <link>https://hugotest-phi.vercel.app/posts/stablediffusionwebui%E9%89%B4%E6%9D%83%E8%AE%BE%E8%AE%A1/</link>
      <pubDate>Sat, 27 Jan 2024 15:42:36 +0000</pubDate>
      <guid>https://hugotest-phi.vercel.app/posts/stablediffusionwebui%E9%89%B4%E6%9D%83%E8%AE%BE%E8%AE%A1/</guid>
      <description>&lt;p&gt;[&amp;gt; [!summary]+&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;this article purpose is to build an authority page for stable diffusion webui using nginx &amp;amp; python/js. Which can publish my personal stable diffusion server. Wrote by GPT(try).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;
&lt;div class=&#34;post-img-view&#34;&gt;
  &lt;a data-fancybox=&#34;gallery&#34; href=&#34;https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/3070PC/20240127161828.png&#34;&gt;
    &lt;img alt=&#34;image.png&#34; loading=&#34;lazy&#34; src=&#34;https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/3070PC/20240127161828.png&#34;class=&#34;responsive-image&#34; src=&#34;https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/3070PC/20240127161828.png&#34; style=&#34;display: block; margin: 0 auto;&#34;
      alt=&#34;image.png&#34;  /&gt;
  &lt;/a&gt;
&lt;/div&gt;


&lt;script&gt;
  document.addEventListener(&#34;DOMContentLoaded&#34;, function() {
      var images = document.querySelectorAll(&#34;.responsive-image&#34;);
      var maxHeight = window.innerHeight / 2.5;
      images.forEach(function(image) {
          image.style.maxHeight = maxHeight + &#34;px&#34;;
      });
  });
&lt;/script&gt;
&lt;/p&gt;
&lt;h3 id=&#34;introduction&#34;&gt;Introduction&lt;/h3&gt;
&lt;p&gt;In the digital age, the security and user-friendliness of web services are not just conveniences; they are necessities. Balancing robust security protocols with an engaging user experience is key to maintaining both the integrity and popularity of any online service. This blog post dives into the intricacies of securing web services using Nginx for authentication, coupled with designing an appealing frontend. Our journey begins with a practical scenario:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;publishing a stable diffusion webUI service, accessible only to an authenticated audience.&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;setting-up-nginx-for-secure-authentication&#34;&gt;Setting Up Nginx for Secure Authentication&lt;/h3&gt;
&lt;p&gt;Nginx excels in serving web pages and as a reverse proxy, providing enhanced security through authentication mechanisms. Let’s explore a typical Nginx configuration for secure authentication:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;/verify_token&lt;/strong&gt;: This block forwards authentication requests to a dedicated server. By excluding the request body and focusing on essential headers, it ensures that only valid, authenticated requests proceed.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-conf&#34; data-lang=&#34;conf&#34;&gt;location = /verify_token {
    proxy_pass http://{your_auth_server}:2424;
    proxy_pass_request_body off;
    proxy_set_header Content-Length &amp;#34;&amp;#34;;
    proxy_set_header X-Original-URI $request_uri;
    proxy_set_header X-Original-Remote-Addr $remote_addr;
    proxy_set_header X-Original-Host $host;
}
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;/login&lt;/strong&gt;: Catering to login requests, this configuration forwards the necessary details to the authentication server, preserving crucial information about the request&amp;rsquo;s origin.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-conf&#34; data-lang=&#34;conf&#34;&gt;location /login {
    proxy_pass http://{your_auth_server}:2424;
    proxy_set_header Host $host;
    proxy_set_header X-Real-IP $remote_addr;
    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    proxy_set_header X-Forwarded-Proto $scheme;
}
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Error Handling (@error401)&lt;/strong&gt;: A clever redirect mechanism that guides unauthenticated users to the login page, keeping the original URL intact.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-conf&#34; data-lang=&#34;conf&#34;&gt;location @error401 {
    return 302 {your_domain}/login;
}
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Root Location (/)&lt;/strong&gt;: The gateway to your service, which rigorously checks each request for authentication, granting access only to verified users.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-conf&#34; data-lang=&#34;conf&#34;&gt;location / {
    auth_request /verify_token;
    error_page 401 = @error401;
    proxy_pass http://{your_server}:2323/;
    proxy_http_version 1.1;
    proxy_set_header Upgrade $http_upgrade;
    proxy_set_header Connection &amp;#39;upgrade&amp;#39;;
    proxy_set_header Host $host;
    proxy_cache_bypass $http_upgrade;
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This setup not only fortifies your service against unauthorized access but also maintains a seamless user experience, redirecting unauthenticated users without hassle.&lt;/p&gt;</description>
    </item>
    <item>
      <title>AIGC05 Stable Diffusion Model Training</title>
      <link>https://hugotest-phi.vercel.app/posts/stablediffusiontraining/</link>
      <pubDate>Sat, 06 May 2023 23:43:41 +0000</pubDate>
      <guid>https://hugotest-phi.vercel.app/posts/stablediffusiontraining/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;该章节主要介绍 Stable-Diffusion 中模型的训练，考虑到硬件条件的限制，实际上这里介绍的训练，都是针对大模型的各种微调技术（Lora，Dreambooth，HyperNetwork, &amp;hellip;），这里会以 LoRA 模型的训练为主。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;参考文献：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.gamelook.com.cn/2023/04/514936&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AIGC教程：Stable Diffusion精进，如何训练特定画风LoRA模型？ | 游戏大观 | GameLook.com.cn&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cnblogs.com/wangiqngpei557/p/17301360.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;stable diffusion打造自己专属的LORA模型 - 王清培 - 博客园 (cnblogs.com)&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kohya-ss/sd-scripts/blob/main/train_README-zh.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;sd-scripts/train_README-zh.md at main · kohya-ss/sd-scripts · GitHub&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;train-lora&#34;&gt;Train LoRA&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;LoRA 的优势就是其模型更小，且更加模块化；也就是说其的训练成本和要求都更低，同时使用代价小，可以作为某种风格插件或者角色插件来使用。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://huggingface.co/blog/zh/lora&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;使用 LoRA 进行 Stable Diffusion 的高效参数微调 (huggingface.co)&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.09685&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[2106.09685] LoRA: Low-Rank Adaptation of Large Language Models (arxiv.org)&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;
&lt;div class=&#34;post-img-view&#34;&gt;
  &lt;a data-fancybox=&#34;gallery&#34; href=&#34;https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/mac/20230706171541.png&#34;&gt;
    &lt;img alt=&#34;image.png&#34; loading=&#34;lazy&#34; src=&#34;https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/mac/20230706171541.png&#34;class=&#34;responsive-image&#34; src=&#34;https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/mac/20230706171541.png&#34; style=&#34;display: block; margin: 0 auto;&#34;
      alt=&#34;image.png&#34;  /&gt;
  &lt;/a&gt;
&lt;/div&gt;


&lt;script&gt;
  document.addEventListener(&#34;DOMContentLoaded&#34;, function() {
      var images = document.querySelectorAll(&#34;.responsive-image&#34;);
      var maxHeight = window.innerHeight / 2.5;
      images.forEach(function(image) {
          image.style.maxHeight = maxHeight + &#34;px&#34;;
      });
  });
&lt;/script&gt;
&lt;/p&gt;
&lt;p&gt;其中蓝色的是预训练好的源网络，而橙色的是新加的网络，通过控制 R 的宽度（文章主要论证了大模型的参数可能存在较低维度的秩，因此可以使用较小的 R 来对大模型的参数造成有效的影响），可以有效的减少需要训练的网络的 Size。&lt;/p&gt;
&lt;h3 id=&#34;事前准备&#34;&gt;事前准备&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;这里只介绍本地训练，训练也可以在 Colab Notebook 等在线训练集群中进行，这里就不进行介绍了&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol&gt;
&lt;li&gt;WebUI + 想训练的基础 SD 模型&lt;/li&gt;
&lt;li&gt;&lt;code&gt;.txt&lt;/code&gt; 带说明的文本文件&lt;/li&gt;
&lt;li&gt;Training Repo（&lt;a href=&#34;https://github.com/kohya-ss/sd-scripts&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;sd-script&lt;/a&gt;
、&lt;a href=&#34;https://github.com/Akegarasu/lora-scripts&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;lora-script&lt;/a&gt;
）&lt;/li&gt;
&lt;li&gt;数据集准备（准备好训练图像）&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;训练包准备&#34;&gt;训练包准备&lt;/h3&gt;
&lt;p&gt;这里我们使用 lora-script 来进行模型训练，lora-script 实际上是 sd-script 之外在包了一层，新增了一些可视化的功能和一些其他的脚本，让 sd-script 更加易用，它调用 sd 中的脚本来实现训练，但是封装了一些注释和整理，此外还支持的 tensorboard 可视化。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;sd-script 本身包含了训练 lora、dreambooth、text-embedding、UNet、Text Encoder、图像生成、模型转换等多种功能。lora-script 还是主要专注于 LoRA 训练&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;查看 repo 也能知道 lora-script 中包含了 sd-script，所以我们部署的时候只需&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;git clone --recurse-submodules https://github.com/Akegarasu/lora-scripts
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;即可将需要的库安装下来，然后安装环境和相关以来只需要执行 &lt;code&gt;.\install.ps1&lt;/code&gt; 即可（该脚本有 cn 版本，但是可能会出现问题），其会安装 sd-scripts 和 lora-scripts 需要的库。具体的可以参考相关 repo（sd-script 详细说明，lora-script 有简化版说明）。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;安装的时候可能会出现虚拟环境未激活的问题，我们可以提前在改目录执行一次 python -m venv venv 一次即可。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Finish.&lt;/p&gt;</description>
    </item>
    <item>
      <title>AIGC04 Stable Diffusion Write Prompt Better</title>
      <link>https://hugotest-phi.vercel.app/posts/stablediffusionprompt/</link>
      <pubDate>Wed, 26 Apr 2023 21:22:38 +0000</pubDate>
      <guid>https://hugotest-phi.vercel.app/posts/stablediffusionprompt/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;该章节主要包括 Promot 生成和部分工作流的分析，旨在了解如何写出更好的关键词，如何生成更好的图片，当我们不知道怎么描述的时候也可以将该工作交给 ChatGPT，让其为我们攥写一般基础的提示词&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;prompt-编写范式&#34;&gt;Prompt 编写范式&lt;/h2&gt;
&lt;p&gt;参考资料：&lt;a href=&#34;https://zhuanlan.zhihu.com/p/619247417?utm_id=0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;【Stable Diffusion】Prompt&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;通常编写可以遵照以下的类别进行组织，主要有 &lt;code&gt;&amp;lt;质量控制&amp;gt; + &amp;lt;前置&amp;gt; + &amp;lt;主体&amp;gt; + &amp;lt;场景词&amp;gt;&lt;/code&gt; 几类，其中分别包括以下的几类词：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;质量控制&lt;/strong&gt;：画质、镜头效果、光照效果&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;前置词&lt;/strong&gt;：画风、艺术家、风格&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;主体&lt;/strong&gt;：人物&amp;amp;对象、姿势、服装、道具&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;场景&lt;/strong&gt;：环境、背景、细节&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Additional Network&lt;/strong&gt;：载入额外模型&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;分割符号：&lt;/strong&gt; 各个关键词之间用 &lt;code&gt;,&lt;/code&gt; 分割，且对应的权重从前到后依次递减，因此在编写关键词的时候也要注意先后顺序。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;权重加权符号&lt;/strong&gt;：各种括号代表各种不同的加权系数，这里建议用 &lt;code&gt;(prompt: weight)&lt;/code&gt; 统一来编写提示词的权重规则，整体可读性会更好。&lt;/p&gt;
&lt;p&gt;这里的 weight 指的是权重变成原本的 weight 倍，就可以调整加强或减弱。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;各个括号的默认系数如下: () -&amp;gt; 1.1 ; {} -&amp;gt; 1.05 ; &lt;code&gt;[]&lt;/code&gt; -&amp;gt; 0.952
可以通过(())进行叠加即 1.1*1.1&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    <item>
      <title>AIGC03 Stable Diffusion Control Net</title>
      <link>https://hugotest-phi.vercel.app/posts/stablediffusioncontrolnet/</link>
      <pubDate>Wed, 26 Apr 2023 21:19:41 +0000</pubDate>
      <guid>https://hugotest-phi.vercel.app/posts/stablediffusioncontrolnet/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;ControlNet 是 Stable Diffusion 最强力的插件之一，它能够控制 SD 的整个扩散过程，包括让 AI 参考动作/骨架/线条/景深，从而更精准的生成图片。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://ivonblog.com/posts/stable-diffusion-webui-manuals/extensions/controlnet/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ControlNet 按照骨架動作繪圖 | Stable Diffusion WebUI使用手冊&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://ivonblog.com/posts/stable-diffusion-webui-manuals/extensions/posex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;骨架人偶 PoseX | Stable Diffusion WebUI 使用手冊&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://ivonblog.com/posts/stable-diffusion-webui-manuals/extensions/latent-couple/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;生成多個人物 Latent Couple | Stable Diffusion WebUI使用手冊&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;拓展地址：&lt;a href=&#34;https://github.com/Mikubill/sd-webui-controlnet&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mikubill/sd-webui-controlnet: WebUI extension for ControlNet (github.com)&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;ControlNet 地址：&lt;a href=&#34;https://github.com/lllyasviel/ControlNet&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;lllyasviel/ControlNet: Let us control diffusion models! (github.com)&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;模型地址：&lt;a href=&#34;https://huggingface.co/lllyasviel/ControlNet-v1-1/tree/main&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;lllyasviel/ControlNet-v1-1 at main (huggingface.co)&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    <item>
      <title>AIGC02 Stable Diffusion 基础功能介绍</title>
      <link>https://hugotest-phi.vercel.app/posts/stable-diffusion-%E5%9F%BA%E7%A1%80%E4%BD%BF%E7%94%A8/</link>
      <pubDate>Wed, 26 Apr 2023 11:03:56 +0000</pubDate>
      <guid>https://hugotest-phi.vercel.app/posts/stable-diffusion-%E5%9F%BA%E7%A1%80%E4%BD%BF%E7%94%A8/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;本篇章介绍关于 Stable DIffusion 的一些基础概念和 WebUI 的基本功能元素，同时介绍一些启动项和模型加载的东西。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;启动项设置局域网&#34;&gt;启动项设置（局域网）&lt;/h2&gt;
&lt;p&gt;最常用的启动项是 &lt;code&gt;--listen&lt;/code&gt;，通过该启动项允许局域网内的其他设备通过 ip 和端口访问部署好的 Stable Diffusion 服务。而设置启动项的方式有以下几种：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;命令行执行启动脚本的时候携带&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-powershell&#34; data-lang=&#34;powershell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;./&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;webui&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;py&#34;&gt;bat&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;-listen&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c&#34;&gt;# ./webui.sh --listen&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;修改主入口脚本中的启动选项 &lt;code&gt;vim launch.py&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 修改下面这一行的参数, 将&amp;#34; &amp;#34;中填入需要的参数&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# commandline_args = os.environ.get(&amp;#39;COMMANDLINE_ARGS&amp;#39;, &amp;#34;&amp;#34;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;commandline_args&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;os&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;environ&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;COMMANDLINE_ARGS&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;--listen&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;其他的启动项介绍可以参考：&lt;a href=&#34;https://ivonblog.com/posts/stable-diffusion-webui-manuals/installation/command-line-arguments-and-settings/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2.3. 命令列引數 | Stable Diffusion WebUI使用手冊(正體中文)｜Ivon的部落格 (ivonblog.com)&lt;/a&gt;
&lt;/li&gt;
&lt;/ol&gt;</description>
    </item>
    <item>
      <title>AIGC01 Stable Diffusion and midjourney Setup</title>
      <link>https://hugotest-phi.vercel.app/posts/stable-diffusion/</link>
      <pubDate>Wed, 19 Apr 2023 16:00:00 +0000</pubDate>
      <guid>https://hugotest-phi.vercel.app/posts/stable-diffusion/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;This Chapter introduce how to set up stable diffusion and mid-journey, and record some problem I meet when I deploy it.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;deprecated-midjourney&#34;&gt;(Deprecated) midjourney&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;由于 midjourney 现需要付费使用，同时没有开源，因此我们讲一笔带过该部分内容，该部分内容大多转载于  &lt;a href=&#34;https://www.uisdc.com/midjourney&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;超详细！AI 绘画神器 Midjourney 基础使用手册&lt;/a&gt;
&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://midjourney.com/home/?callbackUrl=%2Fapp%2F&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;midjourney&lt;/a&gt;
 的安装步骤主要分成以下的几步：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;点击 Join the Beta 注册账号，注册完会跳转到；&lt;/li&gt;
&lt;li&gt;Discord 首页，亲自创建自己的服务器，仅供我和我的朋友使用；&lt;/li&gt;
&lt;li&gt;下载客户端，在默认对话界面讯在或开始新的对话，输入 Midjourney Bot，添加到服务器&lt;/li&gt;
&lt;li&gt;付费开启体验。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;deprecated-dreamstudio&#34;&gt;(Deprecated) DreamStudio&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;说是可以本地部署，但是实际体验非常不好，应该只是部署了 Webui，然后调用官方提供的免费 API；所以有时候生成不出来，但是又不报错，不知道是不是使用姿势有问题，反正很屎。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/Stability-AI/StableStudio&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/Stability-AI/StableStudio&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;装好 npm 和 yarn&lt;/li&gt;
&lt;li&gt;参考 quick start，git clone -&amp;gt; (cd) yarn 安装 -&amp;gt; yarn dev 部署在本地端口上。&lt;/li&gt;
&lt;li&gt;官网注册账号-&amp;gt; 获取 API -&amp;gt; 填入并在最上方转到 Generate 页面即可。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;stable-diffusion-部署专题&#34;&gt;Stable Diffusion 部署专题&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;该部分作为 Intro，仅介绍 Stable Diffusion 的安装和部署，以及一些启用参数等，具体的使用在后面的文章进行进一步的讲解。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Fine Tuning</title>
      <link>https://hugotest-phi.vercel.app/posts/finetune/</link>
      <pubDate>Tue, 08 Feb 2022 14:31:37 +0000</pubDate>
      <guid>https://hugotest-phi.vercel.app/posts/finetune/</guid>
      <description>&lt;p&gt;@Langs: python, torch
@reference: d2l-pytorch，&lt;a href=&#34;https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;transfer_torch&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;This Note focus on the code part.
模型微调和模型预训练，在Pytorch中的使用方式对比汇总。&lt;/p&gt;
&lt;h2 id=&#34;how-to-design-the-fine-tune&#34;&gt;How to Design the Fine Tune&lt;/h2&gt;
&lt;p&gt;这一部分主要集中于我们对于微调任务的拆解，有几种不同的预训练和微调的方式，在不同的情景下，对应的参数应该怎么设置和调整是问题的重点。&lt;/p&gt;
&lt;p&gt;
&lt;div class=&#34;post-img-view&#34;&gt;
  &lt;a data-fancybox=&#34;gallery&#34; href=&#34;https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/3070imgs/20211205143153.png&#34;&gt;
    &lt;img alt=&#34;WorkFlow&#34; loading=&#34;lazy&#34; src=&#34;https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/3070imgs/20211205143153.png&#34;class=&#34;responsive-image&#34; src=&#34;https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/3070imgs/20211205143153.png&#34; style=&#34;display: block; margin: 0 auto;&#34;
      alt=&#34;WorkFlow&#34;  /&gt;
  &lt;/a&gt;
&lt;/div&gt;


&lt;script&gt;
  document.addEventListener(&#34;DOMContentLoaded&#34;, function() {
      var images = document.querySelectorAll(&#34;.responsive-image&#34;);
      var maxHeight = window.innerHeight / 2.5;
      images.forEach(function(image) {
          image.style.maxHeight = maxHeight + &#34;px&#34;;
      });
  });
&lt;/script&gt;
&lt;/p&gt;
&lt;p&gt;基于这种Transfer的策略，我们能够学习到一个更通用，泛化能力更强，有助于识别边缘，色彩，等等有助于下游任务的通用特征提取。&lt;/p&gt;
&lt;p&gt;在Transfer任务中，有几种不同的调整方式：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;固定Bakcbone，只训练Classifier&lt;/li&gt;
&lt;li&gt;同步微调网络&lt;/li&gt;
&lt;li&gt;区分学习率，微调Backbone，训练Classifirer&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;为了实现这几种不同的Transfer方式，需要用到以下的几种方式：梯度截断，lr区分设置等。&lt;/p&gt;
&lt;h2 id=&#34;code-part&#34;&gt;Code Part&lt;/h2&gt;
&lt;h3 id=&#34;不同lr设置&#34;&gt;不同lr设置&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;微调Backbone，训练Classifier&lt;/strong&gt;作为最经典的Transfer设定，在Code上也较为复杂，所以我们首先举个这种例子。&lt;/p&gt;
&lt;!-- more --&gt;
&lt;p&gt;相关的文档可以参考：&lt;a href=&#34;https://pytorch-cn.readthedocs.io/zh/latest/package_references/torch-optim/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;torch.optim&lt;/a&gt;
&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;14
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;15
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;16
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;17
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;18
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;19
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;20
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;21
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;22
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;23
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;24
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;25
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;26
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# get dataset&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;train_img&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torchvision&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;datasets&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ImageFolder&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;os&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;path&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;join&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data_dir&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;train&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# get new model&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;pretrained_new&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;expand_dim&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dim&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;out_dim&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;re_init&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# pre train it 定义一个用于微调的函数&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# pytorch可以通过字典的形式来区分对设置lr&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;train_fine_tuning&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;net&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;learning_rate&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;batch_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;128&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;num_epoch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;diff_lr&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;	&lt;span class=&#34;c1&#34;&gt;# set dataloader&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;	&lt;span class=&#34;n&#34;&gt;train_iter&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;utils&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Dataloader&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;train_img&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;batch_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;batch_size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;shuffle&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;	&lt;span class=&#34;n&#34;&gt;test_iter&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;...&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;	
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;	&lt;span class=&#34;c1&#34;&gt;# set loss&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;	&lt;span class=&#34;n&#34;&gt;loss&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;CrossEntropyLoss&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;reduction&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;none&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;	
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;	&lt;span class=&#34;c1&#34;&gt;# set diff lr for diff part of it &lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;	&lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;diff_lr&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;		&lt;span class=&#34;n&#34;&gt;params_1x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;param&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;param&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;net&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;name_parameters&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;name&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;not&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;fc.weight&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;fc.bias&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;		&lt;span class=&#34;n&#34;&gt;trainer&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;optim&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;SGD&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([{&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;params&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;params_1x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;								  &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;params&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;net&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fc&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;parameters&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(),&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;								  &lt;span class=&#34;s1&#34;&gt;&amp;#39;lr&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;learning_rate&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}],&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;								  &lt;span class=&#34;n&#34;&gt;lr&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;learning_rate&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;weight_decay&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.001&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;								 &lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;	&lt;span class=&#34;k&#34;&gt;else&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;		&lt;span class=&#34;n&#34;&gt;trainer&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;optim&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;SGD&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;net&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;parameters&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;lr&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;learning_rate&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;weight_decay&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.001&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;同时不用担心，scheduler可以将我们的两组lr同时进行更新，可以基于下面的代码进行测试&lt;/p&gt;</description>
    </item>
    <item>
      <title>IL Collection</title>
      <link>https://hugotest-phi.vercel.app/posts/il-collection/</link>
      <pubDate>Tue, 04 Jan 2022 01:38:04 +0000</pubDate>
      <guid>https://hugotest-phi.vercel.app/posts/il-collection/</guid>
      <description>&lt;p&gt;@AikenHong 2022&lt;/p&gt;
&lt;p&gt;[[Draft/IL 总结]]: Thx 2 wyz to provide some clus for learnning Incremental Learning.&lt;/p&gt;
&lt;p&gt;In this Doc, we may add some related knowledge distill works which is used to design our Incremental Structure.
在这个文档中，我们可能还会添加一些知识蒸馏的相关工作的文献，这些实际上对于我的增量学习架构有一个比较大的启发&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.csdn.net/weixin_36474809/article/details/116176371&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DER&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;SPPR 没有 get 到方法到底是怎么做的&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introduction-&#34;&gt;Introduction 👿&lt;/h2&gt;
&lt;p&gt;在很多视觉应用中，需要在保留旧知识的基础上学习新知识，==举个例子==，理想的情况是，我们可以保留之前学习的参数，而不发生==灾难性遗忘==，或者我们基于之前的数据进行协同训练，灾难性遗忘是 IL 中最核心的问题。&lt;/p&gt;
&lt;p&gt;Incremental 的基本过程可以表示如下&lt;sub&gt;[4]&lt;/sub&gt;：

&lt;div class=&#34;post-img-view&#34;&gt;
  &lt;a data-fancybox=&#34;gallery&#34; href=&#34;https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/20220106101003.png&#34;&gt;
    &lt;img alt=&#34;dsa&#34; loading=&#34;lazy&#34; src=&#34;https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/20220106101003.png&#34;class=&#34;responsive-image&#34; src=&#34;https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/20220106101003.png&#34; style=&#34;display: block; margin: 0 auto;&#34;
      alt=&#34;dsa&#34;  /&gt;
  &lt;/a&gt;
&lt;/div&gt;


&lt;script&gt;
  document.addEventListener(&#34;DOMContentLoaded&#34;, function() {
      var images = document.querySelectorAll(&#34;.responsive-image&#34;);
      var maxHeight = window.innerHeight / 2.5;
      images.forEach(function(image) {
          image.style.maxHeight = maxHeight + &#34;px&#34;;
      });
  });
&lt;/script&gt;
&lt;/p&gt;</description>
    </item>
    <item>
      <title>WYZ-IL-Collection</title>
      <link>https://hugotest-phi.vercel.app/posts/il-wyz/</link>
      <pubDate>Mon, 03 Jan 2022 10:41:56 +0000</pubDate>
      <guid>https://hugotest-phi.vercel.app/posts/il-wyz/</guid>
      <description>&lt;p&gt;: hammer: 王耀智&lt;/p&gt;
&lt;h2 id=&#34;regularization-系列方法&#34;&gt;Regularization 系列方法&lt;/h2&gt;
&lt;p&gt;这类方法旨在添加一些正则化损失来解决 &lt;code&gt;catastrophic forgetting&lt;/code&gt; 的问题。&lt;/p&gt;
&lt;h3 id=&#34;weight-regularization&#34;&gt;Weight Regularization&lt;/h3&gt;
&lt;p&gt;这类方法一般是对网络中每个参数的重要性进行评估，根据每个参数的重要性和梯度信息更新参数。&lt;/p&gt;
&lt;p&gt;典型的文章为 &lt;a href=&#34;https://www.pnas.org/content/pnas/114/13/3521.full.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EWC&lt;/a&gt;
 .&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;PS: 这类文章我也没有读过。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;data-regularization&#34;&gt;Data Regularization&lt;/h3&gt;
&lt;p&gt;这类方法专注于记住特征表示，通常是结合 Hinton 的知识蒸馏损失函数使得模型记住旧类别的知识，解决 catastrophic forgetting。&lt;/p&gt;
&lt;p&gt;推荐以下几篇文章：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;LwF&lt;/code&gt;(Learning without forgetting)，这篇文章在我看来是增量学习的开山之作，第一次给增量学习找到了一个比较好的方向，也是第一次将知识蒸馏应用到增量学习上；&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2004.13513&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PODNet CVPR2020&lt;/a&gt;
 ，这篇文章最大的贡献在我看来是设计了一个全新的蒸馏损失函数，最终结果也是达到了当时的sota，甚至目前也是几个榜单的sota。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;rehearsal-系列方法&#34;&gt;Rehearsal 系列方法&lt;/h2&gt;
&lt;p&gt;这类方法主要的想法是使用一些旧类别的数据，在新类别到来时使用新旧数据一起训练模型，根据旧类别数据的真假分为以下两种方法。&lt;/p&gt;
&lt;!-- more --&gt;
&lt;h3 id=&#34;pseudo-rehearsal&#34;&gt;Pseudo rehearsal&lt;/h3&gt;
&lt;p&gt;这类方法通常是在学习旧类别的同时，训练一个生成模型，可以生成旧的类别数据，在新类别数据到来时，生成相当数量的旧类别数据，一起训练新模型。&lt;/p&gt;
&lt;p&gt;这里推荐一篇文章：Continual learning with deep generative replay。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;PS：这个小类别的论文我也没有太关注，个人不是很推荐这类方法。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;save-real-data&#34;&gt;Save real data&lt;/h3&gt;
&lt;p&gt;这类方法是开辟一个内存空间，空间中保存旧类别的少部分训练数据，在新类别到来时，使用内存空间的数据与新数据共同学习，按照对空间的使用方法不同可分为：&lt;/p&gt;
&lt;h4 id=&#34;exemplar-rehearsal&#34;&gt;Exemplar Rehearsal&lt;/h4&gt;
&lt;p&gt;这类方法是将新旧数据混合，共同作为训练数据，一起训练模型，使得模型能够保持旧类别的知识。&lt;/p&gt;
&lt;p&gt;但是在训练过程中新旧数据的类别数量是不均衡的，这也催生了我下面会说到的一大类解决方法。&lt;/p&gt;
&lt;p&gt;这种方法推荐的论文是 &lt;code&gt;iCaRL&lt;/code&gt;，这篇论文是 exemplar rehearsal 的开山之作，第一次提出了内存空间这个概念，也提出了一个非常有效的内存选择策略(herb)，并且也是第一个使用特征作为分类依据的方法，我个人认为是继 LwF 之后又一个将 IL 推到一个新的高度的方法。&lt;/p&gt;
&lt;h4 id=&#34;gradient-rectification&#34;&gt;Gradient Rectification&lt;/h4&gt;
&lt;p&gt;这类方法我称之为 Gradient Rectification，其主要思路是模型每次更新的梯度由 shared gradient 和 task-specific gradient 组成。分别代表所有类别的共性信息和某一个类别的特性信息，在新类别学习时借助内存空间中的数据获得旧类别的两项梯度，在更新时对梯度进行修正，力求做到不增加共享梯度代表的损失，尽量减少类别特定梯度代表的损失。&lt;/p&gt;</description>
    </item>
    <item>
      <title>LT Collection</title>
      <link>https://hugotest-phi.vercel.app/posts/lt-collection/</link>
      <pubDate>Wed, 22 Dec 2021 14:36:16 +0000</pubDate>
      <guid>https://hugotest-phi.vercel.app/posts/lt-collection/</guid>
      <description>&lt;h1 id=&#34;lt-collections&#34;&gt;LT-Collections&lt;/h1&gt;
&lt;p&gt;@AikenHong 2021&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/mitming/OpenLT&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Code of must of those methods&lt;/a&gt;

We will analysis those tricks on LT situation, and Analysis why it works.
在进行LT矫正的任务中，有几种常见的trick在各种模型中被使用，我们会对这几种不同的trick进行介绍和分析。&lt;/p&gt;
&lt;p&gt;其实在数据量少这一方面LT和Few-Shot是有一定的OverLap的,可以参考以下那边的思路perhaps&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;
&lt;div class=&#34;post-img-view&#34;&gt;
  &lt;a data-fancybox=&#34;gallery&#34; href=&#34;https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/labimg/20211217165531.png&#34;&gt;
    &lt;img alt=&#34;LT&#34; loading=&#34;lazy&#34; src=&#34;https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/labimg/20211217165531.png&#34;class=&#34;responsive-image&#34; src=&#34;https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/labimg/20211217165531.png&#34; style=&#34;display: block; margin: 0 auto;&#34;
      alt=&#34;LT&#34;  /&gt;
  &lt;/a&gt;
&lt;/div&gt;


&lt;script&gt;
  document.addEventListener(&#34;DOMContentLoaded&#34;, function() {
      var images = document.querySelectorAll(&#34;.responsive-image&#34;);
      var maxHeight = window.innerHeight / 2.5;
      images.forEach(function(image) {
          image.style.maxHeight = maxHeight + &#34;px&#34;;
      });
  });
&lt;/script&gt;

通常情况下这种严重的类别不平衡问题会使得模型严重过拟合于头部，而在尾部欠拟合&lt;/p&gt;
&lt;p&gt;首先介绍 &lt;a href=&#34;https://zhuanlan.zhihu.com/p/416315017&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;bag of tricks&lt;/a&gt;
 这篇论文中总结了一些常用的Trick，并组合出了最佳的一套trick&lt;/p&gt;
&lt;p&gt;经过该文实验总结，Trick组合应该是&lt;sub&gt;[1]`&lt;/sub&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在前几个epoch应用input mixup数据增强，然后后面fine-tuning;&lt;/li&gt;
&lt;li&gt;(基于CAM的)重采样来重新训练分类器;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;实际上就是MixUp + Two-Stage的策略，后续对&lt;strong&gt;Mix-up&lt;/strong&gt;这个策略带来的作用要进行补充了解一下&lt;/p&gt;
&lt;h2 id=&#34;rebalance&#34;&gt;Rebalance&lt;/h2&gt;
&lt;!-- more --&gt;
&lt;p&gt;对于ReBalance的方法，实际上就是从 &lt;code&gt;data&lt;/code&gt;和 &lt;code&gt;update&lt;/code&gt;两个角度来缓解Unbalance本身，通过从数据量上达到重新均衡，或者基于Loss使得bp过程中赋予Tail更高的权重来达到优化过程的平衡。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Loss-NCE</title>
      <link>https://hugotest-phi.vercel.app/posts/loss-nce/</link>
      <pubDate>Wed, 22 Dec 2021 13:39:55 +0000</pubDate>
      <guid>https://hugotest-phi.vercel.app/posts/loss-nce/</guid>
      <description>&lt;p&gt;@AikenHong 2021&lt;/p&gt;
&lt;p&gt;Noise Contrastive Estimation Loss = NCE Loss 噪声对比估计损失，这里的Noise实际上就是Negative Samples.
该损失被广泛的用于对比学习的任务，而对比学习广泛的作为自监督学习的无监督子任务用来训练一个良好的特征提取器，于是对于对比学习的目标和效用的理解十分关键。&lt;/p&gt;
&lt;h2 id=&#34;whats-nce-loss&#34;&gt;What&amp;rsquo;s NCE Loss&lt;/h2&gt;
&lt;p&gt;在介绍NCE之前我们可以将其和CE进行一个简单的对比，虽然名称上不是同一个CE，但是在数学表达上却有很相近的地方（softmax-kind of loss）&lt;/p&gt;
&lt;p&gt;首先softmax，他保证所有的值加起来为一，结合onehot的ce，实际上&lt;code&gt;j==gt&lt;/code&gt;的情况下外层+log也就是ceLoss，也就是 $logSoftmax$&lt;/p&gt;
&lt;div&gt;
$$ 
S_j = \frac{e^{a_j}}{\sum_{k=1}^N e^{a_k}}
 $$
&lt;/div&gt;
&lt;p&gt;然后看infoNCE，基础的对比学习损失可以写成：&lt;/p&gt;
&lt;div&gt;
$$ 
L_{contrast} = \mathbb{E}[-\log\frac{e^{f_x^T f_y/T}}{e^{f_x^T f_y/T} + \sum_i e^{f_x^T f_{y_-^i}/T}}]
 $$
&lt;/div&gt;
&lt;p&gt;其中 $f_x^T f_y^T$ 为 $sim(x,y)$ 时即转化为带 $T$ 的NCE，即InforNCE.&lt;/p&gt;
&lt;p&gt;分子是正例对的相似度，分母是正例对+所有负例对的相似度，最小化infoNCE loss，就是去最大化分子的同时最小化分母，也就是最大化正例对的相似度，最小化负例对的相似度。&lt;/p&gt;
&lt;p&gt;从该形式上看，和soft的CE形式上是统一的，当我们把分母看作概率和自身以及和其他的相似性，这样和NCE在形式上和简化后的CE实现了统一。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;但是我不认为这和label smooth 后的CE有相关性，而是和原始的CE经由One-hot简化后结构上有相似性。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;how-it-works&#34;&gt;How it Works&lt;/h2&gt;
&lt;p&gt;NCE的思想是&lt;strong&gt;拉近相似的样本，推开不相近的样本&lt;/strong&gt;，从而学习到一个好的&lt;strong&gt;语义表示空间&lt;/strong&gt;，这一点上实际上和度量学习的思想是一样的，只是对比学习通常作用在无监督或者自监督的语境中，度量学习这是有监督的。&lt;/p&gt;
&lt;p&gt;考虑之前人脸匹配的研究，使用 &amp;ldquo;Alignment and Uniformity on the Hypersphere&amp;quot;中的Alignment and Uniformity，就是一个更好理解他的角度&lt;/p&gt;
&lt;!-- more --&gt;
&lt;div&gt;
$$ 
\begin{gathered}
L_{\text {contrast }}=\mathbb{E}\left[-\log \frac{e^{f_{x}^{T} f_{y} / \tau}}{e^{f_{x}^{T} f_{y} / \tau}+\sum_{i} e^{T_{x}^{T} f_{y_{i}}^{-} / \tau}}\right] \\
=\mathbb{E}\left[-f_{x}^{T} f_{y} / \tau\right]+\mathbb{E}\left[\log \left(e^{f_{x}^{T} f_{y} / \tau}+\sum_{i} e^{f_{x}^{T} f_{y_{i}^{-} / \tau}}\right)\right] \\
\mathbb{P}\left[\left(f_{x}=f_{y}\right)\right]=1 \underbrace{\mathbb{E}\left[-f_{x}^{T} f_{y} / \tau\right]}_{\text {positive alignment }}+\underbrace{\mathbb{E}\left[\log \left(e^{1 / \tau}+\sum_{i} e^{f_{x}^{T} f_{y_{i}}-/ \tau}\right)\right]}_{\text {uniformity }}
\end{gathered}
 $$
&lt;/div&gt;
&lt;p&gt;公式经过上面的推导就可以看成下的两个部分，其中&lt;strong&gt;alignment&lt;/strong&gt;只与&lt;strong&gt;positive pair&lt;/strong&gt;有关，相反&lt;strong&gt;Uniformity&lt;/strong&gt;只与&lt;strong&gt;negative pair&lt;/strong&gt;相关，希望所有的点都能尽可能的分布在uni hypersphere上。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Loss-Smooth(Sharpen)</title>
      <link>https://hugotest-phi.vercel.app/posts/loss-smoothsharpen/</link>
      <pubDate>Fri, 17 Dec 2021 03:35:27 +0000</pubDate>
      <guid>https://hugotest-phi.vercel.app/posts/loss-smoothsharpen/</guid>
      <description>&lt;p&gt;@AikenHong 2021
@topic&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;smooth label (inception v2)&lt;/li&gt;
&lt;li&gt;when does label smoothing help (nips 2019)&lt;/li&gt;
&lt;li&gt;sharpen in semi-supervised in the future&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/seominseok0429/label-smoothing-visualization-pytorch?utm_source=catalyzex.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;offical code github&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;不是一个通用的方法，在很多的任务上反而会导致掉点的现象，可以简单分析一下，汲取一下思想和Sharpen做对比，在这篇文章中，我们可以结合之前的人脸对比损失来进行分析。&lt;/p&gt;
&lt;h2 id=&#34;whats-the-smooth-label&#34;&gt;What&amp;rsquo;s the smooth label&lt;/h2&gt;
&lt;p&gt;首先介绍在图像分类任务中对logits和Hard label做ce得到我们的损失，可以表现为如下的形式：&lt;/p&gt;
&lt;div&gt;
$$ 
Loss = -\sum^{K}_{i=1}p_i \log(q_i)
 $$
&lt;/div&gt;
&lt;p&gt;由于我们的标签是一个hard label，实际上可以转化成一个one-hot，即&lt;/p&gt;
&lt;div&gt;
$$ 
\begin{equation}
p_i = \left\{
\begin{array}{c1}
1 &amp; i==gt \\
0 &amp; i!=gt \\
\end{array} \right.
\end{equation}
 $$
&lt;/div&gt;
&lt;p&gt;而soft label实际上做的是将 1的位置变为 $1-\alpha$ ，其他位置设置为 $\alpha/(K-1)$ ，然后再去求CE，&lt;/p&gt;
&lt;p&gt;Hinton论文中给出该损失对特征分布的作用测试图：

&lt;div class=&#34;post-img-view&#34;&gt;
  &lt;a data-fancybox=&#34;gallery&#34; href=&#34;https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/labimg/20211216194040.png&#34;&gt;
    &lt;img loading=&#34;lazy&#34; src=&#34;https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/labimg/20211216194040.png&#34;class=&#34;responsive-image&#34; src=&#34;https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/labimg/20211216194040.png&#34; style=&#34;display: block; margin: 0 auto;&#34;
      alt=&#34;&#34;  /&gt;
  &lt;/a&gt;
&lt;/div&gt;


&lt;script&gt;
  document.addEventListener(&#34;DOMContentLoaded&#34;, function() {
      var images = document.querySelectorAll(&#34;.responsive-image&#34;);
      var maxHeight = window.innerHeight / 2.5;
      images.forEach(function(image) {
          image.style.maxHeight = maxHeight + &#34;px&#34;;
      });
  });
&lt;/script&gt;
&lt;/p&gt;</description>
    </item>
    <item>
      <title>Training Strategy</title>
      <link>https://hugotest-phi.vercel.app/posts/nerualnetworktraining/</link>
      <pubDate>Thu, 16 Dec 2021 08:34:44 +0000</pubDate>
      <guid>https://hugotest-phi.vercel.app/posts/nerualnetworktraining/</guid>
      <description>&lt;p&gt;@Aiken 2020，&lt;/p&gt;
&lt;p&gt;主要针对神经网络的训练过程中的一些基础策略的调整，比如当训练的曲线出现一定的问题的时候，我们应该怎么去调整我们训练过程中的策略。&lt;/p&gt;
&lt;p&gt;参数调整过程中最重要的就是优化器（优化或者说是下降算法）和学习率（优化算法的核心参数），此外像是数据增强策略还是Normalization策略，都能极大的影响一个模型的好坏。&lt;/p&gt;
&lt;h2 id=&#34;优化器&#34;&gt;优化器&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://wizardforcel.gitbooks.io/learn-dl-with-pytorch-liaoxingyu/content/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Some Material&lt;/a&gt;

实际上虽然有很多的优化算法，但是到最后最常用的还是 SGD+Mon 和 Adam两种：&lt;/p&gt;
&lt;p&gt;Adam主要的有事在于自适应学习率，他对我们设计的学习率实际上没有那么敏感，但是在具体实验中往往不会有调的好的SGD那么好，只是在SGD的参数调整中会比较费劲。&lt;/p&gt;
&lt;p&gt;但是有了根据patient调整lr的scheduler后，我们基本上可以使用SGD做一个较为简单的调整，只要设计好初始的lr的实验以及用来调整学习率的参数值。&lt;/p&gt;
&lt;h2 id=&#34;学习率&#34;&gt;学习率&lt;/h2&gt;
&lt;p&gt;$\omega^{n} \leftarrow \omega^{n}-\eta \frac{\partial L}{\partial \omega^{n}}$ 其中的权重就是学习率lr，&lt;/p&gt;
&lt;p&gt;==Basic==&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;&lt;/th&gt;
          &lt;th&gt;学习率大&lt;/th&gt;
          &lt;th&gt;学习率小&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;学习速度&lt;/td&gt;
          &lt;td&gt;快&lt;/td&gt;
          &lt;td&gt;慢&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;使用情景&lt;/td&gt;
          &lt;td&gt;刚开始训练时&lt;/td&gt;
          &lt;td&gt;一定的次数过后&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;副作用&lt;/td&gt;
          &lt;td&gt;1. Loss爆炸 2.振荡&lt;/td&gt;
          &lt;td&gt;1.过拟合 2.收敛速度慢&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;学习率的基本设置&#34;&gt;学习率的基本设置&lt;/h3&gt;
&lt;!-- more --&gt;
&lt;p&gt;在训练过程中，一般根据训练轮数设置动态变化的学习率。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;刚开始训练时：学习率以 0.01 ~ 0.001 为宜。&lt;/li&gt;
&lt;li&gt;一定轮数过后：逐渐减缓。&lt;/li&gt;
&lt;li&gt;接近训练结束：学习速率的衰减应该在100倍以上。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Note：&lt;/strong&gt;
如果是 &lt;strong&gt;迁移学习&lt;/strong&gt; ，由于模型已在原始数据上收敛，此时应设置较小学习率 (≤10−4) 在新数据上进行 &lt;strong&gt;微调&lt;/strong&gt; 。&lt;/p&gt;
&lt;h3 id=&#34;学习率变化方法&#34;&gt;学习率变化方法&lt;/h3&gt;
&lt;p&gt;==warm up==&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.zhihu.com/question/338066667/answer/771252708&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;warm up为什么有用&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;warm up衰减策略与上述的策略有些不同，它是先从一个极低的学习率开始增加，增加到某一个值后再逐渐减少, 这点上倒是和Cosine Anneal LR有一定的相似之处，将这两种结合起来是一种常见的训练策略：&lt;/p&gt;
&lt;p&gt;这样训练模型更加稳定，因为在刚开始时模型的参数都是随机初始化的，此时如果学习率应该取小一点，这样就不会使模型一下子跑偏。&lt;/p&gt;
&lt;p&gt;而这样的跑偏对于&lt;strong&gt;大模型&lt;/strong&gt;而言，可能是导致很严重的影响，后面收敛了也可能不会达到最佳的效果，一开始的跑偏，可能会造成准确率在后面的严重结果。

&lt;div class=&#34;post-img-view&#34;&gt;
  &lt;a data-fancybox=&#34;gallery&#34; href=&#34;https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/3070imgs/20211216001833.png&#34;&gt;
    &lt;img alt=&#34;warmup&#34; loading=&#34;lazy&#34; src=&#34;https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/3070imgs/20211216001833.png&#34;class=&#34;responsive-image&#34; src=&#34;https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/3070imgs/20211216001833.png&#34; style=&#34;display: block; margin: 0 auto;&#34;
      alt=&#34;warmup&#34;  /&gt;
  &lt;/a&gt;
&lt;/div&gt;


&lt;script&gt;
  document.addEventListener(&#34;DOMContentLoaded&#34;, function() {
      var images = document.querySelectorAll(&#34;.responsive-image&#34;);
      var maxHeight = window.innerHeight / 2.5;
      images.forEach(function(image) {
          image.style.maxHeight = maxHeight + &#34;px&#34;;
      });
  });
&lt;/script&gt;
&lt;/p&gt;</description>
    </item>
    <item>
      <title>Loss-WhyZero</title>
      <link>https://hugotest-phi.vercel.app/posts/loss-whyzero/</link>
      <pubDate>Fri, 10 Dec 2021 08:24:46 +0000</pubDate>
      <guid>https://hugotest-phi.vercel.app/posts/loss-whyzero/</guid>
      <description>&lt;h1 id=&#34;loss-why-zero-loss&#34;&gt;Loss :Why Zero Loss？&lt;/h1&gt;
&lt;p&gt;@Comments: ICML2020 《Do We Need Zero Training Loss After Achieving Zero Training Error》&lt;/p&gt;
&lt;p&gt;@Noteby：AikenHong2021&lt;/p&gt;
&lt;p&gt;如何解决训练损失下降，但是验证损失上升的问题（过拟合like）的问题，该文章实际上可以作为我们损失设计中的一个trick，只需要简单的一行代码，来提升代码的泛化能力；&lt;/p&gt;
&lt;p&gt;
&lt;div class=&#34;post-img-view&#34;&gt;
  &lt;a data-fancybox=&#34;gallery&#34; href=&#34;https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20211026211602.png&#34;&gt;
    &lt;img alt=&#34;img&#34; loading=&#34;lazy&#34; src=&#34;https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20211026211602.png&#34;class=&#34;responsive-image&#34; src=&#34;https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20211026211602.png&#34; style=&#34;display: block; margin: 0 auto;&#34;
      alt=&#34;img&#34;  /&gt;
  &lt;/a&gt;
&lt;/div&gt;


&lt;script&gt;
  document.addEventListener(&#34;DOMContentLoaded&#34;, function() {
      var images = document.querySelectorAll(&#34;.responsive-image&#34;);
      var maxHeight = window.innerHeight / 2.5;
      images.forEach(function(image) {
          image.style.maxHeight = maxHeight + &#34;px&#34;;
      });
  });
&lt;/script&gt;
&lt;/p&gt;
&lt;p&gt;这张图体现了本文的灵魂（思路），主要体现在我们在算法趋于稳定后继续训练可能验证损失会反而上升；&lt;/p&gt;
&lt;p&gt;所以本文提出了一种flooding方法，当我们training loss 大于阈值的时候我们使其正常下降，当低于阈值的时候，flooding的设计会反过来使得梯度上升，让训练损失保持在flooding附近，让模型持续进行random walk，希望模型最终能优化到一个平坦的损失区域，这样发现test loss进一步的进行下降。&lt;/p&gt;
&lt;p&gt;理解：&lt;/p&gt;
&lt;p&gt;当我们的训练损失低到一定的程度，然后随着lr的下降，模型会很难跳出当前的极小值，这种情况下我们的泛化能力也会被限制住，采用这种方法在牺牲测试精度的同时能提升算法的泛化能力。&lt;/p&gt;
&lt;p&gt;损失公式表示如下&lt;/p&gt;
&lt;div&gt;
$$ 
\widetilde{J}(\theta) = |J(\theta) - b| +b
 $$
&lt;/div&gt;
&lt;p&gt;
&lt;div class=&#34;post-img-view&#34;&gt;
  &lt;a data-fancybox=&#34;gallery&#34; href=&#34;https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20211027104636.jpg&#34;&gt;
    &lt;img alt=&#34;v2-084a8f00d7349a94540fc7ad3a9433b0_r&#34; loading=&#34;lazy&#34; src=&#34;https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20211027104636.jpg&#34;class=&#34;responsive-image&#34; src=&#34;https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20211027104636.jpg&#34; style=&#34;display: block; margin: 0 auto;&#34;
      alt=&#34;v2-084a8f00d7349a94540fc7ad3a9433b0_r&#34;  /&gt;
  &lt;/a&gt;
&lt;/div&gt;


&lt;script&gt;
  document.addEventListener(&#34;DOMContentLoaded&#34;, function() {
      var images = document.querySelectorAll(&#34;.responsive-image&#34;);
      var maxHeight = window.innerHeight / 2.5;
      images.forEach(function(image) {
          image.style.maxHeight = maxHeight + &#34;px&#34;;
      });
  });
&lt;/script&gt;
&lt;/p&gt;</description>
    </item>
    <item>
      <title>UniFramework 01</title>
      <link>https://hugotest-phi.vercel.app/posts/uniframework/</link>
      <pubDate>Sat, 04 Dec 2021 01:43:30 +0000</pubDate>
      <guid>https://hugotest-phi.vercel.app/posts/uniframework/</guid>
      <description>&lt;head&gt;
    
    &lt;script src=&#34;https://mirrors.sustech.edu.cn/cdnjs/ajax/libs/crypto-js/3.1.9-1/crypto-js.js&#34;&gt;&lt;/script&gt;
&lt;/head&gt;





&lt;div class=&#34;hugo-encryptor-container&#34;&gt;
  &lt;div class=&#34;hugo-encryptor-prompt&#34;&gt;
    
      &lt;p&gt;文章的部分内容被密码保护：&lt;/p&gt;
    
  &lt;/div&gt;
  &lt;div class=&#34;hugo-encryptor-form&#34;&gt;
    &lt;input
      class=&#34;hugo-encryptor-input&#34;
      placeholder=&#39;请输入密码&#39;
    /&gt;
    &lt;input
      class=&#34;hugo-encryptor-button&#34;
      type=&#34;button&#34;
      value=&#39;CLICK&#39;
      onclick=&#34;_click_handler(this)&#34;
    /&gt;
  &lt;/div&gt;
  &lt;div
    class=&#34;hugo-encryptor-cipher-text&#34;
    data-password=&#34;aikenhong_blog&#34;
    style=&#34;display: none;&#34;
  &gt;
    &lt;span style=&#34;display: none;&#34;&gt;--- DON&#39;T MODIFY THIS LINE ---&lt;/span&gt;
    &lt;p&gt;@aiken 2021  Framework&lt;/p&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Try To make structure universal，编写一个自己的通用的架构，框架化，满足通过不同的model文件和特殊配置文件就能实现不同的模型的一个架构。&lt;/p&gt;
&lt;p&gt;只是一个初步的框架集成，还有很多没有完善的地方，目前测试了ResNet18 跑Cifar10，没有什么问题，如果有什么可以改进的地方，或者你实现了一些Feature，&lt;em&gt;&lt;strong&gt;*欢迎进行交流*&lt;/strong&gt;&lt;/em&gt;！（私下联系我最好啦！）&lt;/p&gt;
&lt;p&gt;感谢帮助&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;还有一些可以参数化或者可视化的地方，由于时间关系目前还没有修改，有兴趣的可以自己先添加一下&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;暂时只集成了分类的模块，后续可能会随缘扩展&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;本框架主要希望实现的是：易读性，可拓展性，以及简洁；&lt;/p&gt;
&lt;p&gt;希望将重要的，可变的参数都尽量的分离出来，通过配置文件和命令行参数去定义和运行我们的网络，在这种情况下实现一个较好的工作流程。&lt;/p&gt;
&lt;h2 id=&#34;final-project-design&#34;&gt;Final Project Design&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;PURPOSE：新类发现和模型自主更新&lt;/strong&gt;；同时希望能够解决&lt;strong&gt;长尾分布&lt;/strong&gt;的数据情景；&lt;/p&gt;
&lt;p&gt;**ANALYSIS：**为了实现这种模型的自主更新过程，将整体的流程分成两个部分&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;启动（start）：&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- more --&gt;
&lt;p&gt;self supervissed 等方法无监督的学习特征提取网络（这种方式是否会对Unbalance产生增益）&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;初始化预测模型：&lt;/strong&gt;
基于&lt;strong&gt;Unbalance&lt;/strong&gt;的数据训练一个基础的分类模型，在输出分类结果的同时需要输出对应的预测&lt;strong&gt;置信度&lt;/strong&gt;，这两个其实都是一些简单的Trick，而最重要的是Backbone的分类效果需要得到保证，同时&lt;strong&gt;Backbone需要支撑后续的模型蒸馏&lt;/strong&gt;更新。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;
&lt;div class=&#34;post-img-view&#34;&gt;
  &lt;a data-fancybox=&#34;gallery&#34; href=&#34;https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210921164616.png&#34;&gt;
    &lt;img alt=&#34;image-20210921164615348&#34; loading=&#34;lazy&#34; src=&#34;https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210921164616.png&#34;class=&#34;responsive-image&#34; src=&#34;https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210921164616.png&#34; style=&#34;display: block; margin: 0 auto;&#34;
      alt=&#34;image-20210921164615348&#34;  /&gt;
  &lt;/a&gt;
&lt;/div&gt;


&lt;script&gt;
  document.addEventListener(&#34;DOMContentLoaded&#34;, function() {
      var images = document.querySelectorAll(&#34;.responsive-image&#34;);
      var maxHeight = window.innerHeight / 2.5;
      images.forEach(function(image) {
          image.style.maxHeight = maxHeight + &#34;px&#34;;
      });
  });
&lt;/script&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;模型的自主更新和迭代：&lt;/strong&gt;
Online：在线运行推断模型，通过&lt;strong&gt;置信度输出筛选&lt;/strong&gt;出新类样本，将样本在&lt;strong&gt;样本池&lt;/strong&gt;中收集
Offline：基于样本池的规模和评估触发离线更新：&lt;strong&gt;伪标签生成模型&lt;/strong&gt;；&lt;strong&gt;模型蒸馏和更新&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;创新点：自主新类发现和学习&lt;/p&gt;</description>
    </item>
    <item>
      <title>FSL前期调研</title>
      <link>https://hugotest-phi.vercel.app/posts/fsl%E5%89%8D%E6%9C%9F%E8%B0%83%E7%A0%94/</link>
      <pubDate>Mon, 29 Nov 2021 13:12:05 +0000</pubDate>
      <guid>https://hugotest-phi.vercel.app/posts/fsl%E5%89%8D%E6%9C%9F%E8%B0%83%E7%A0%94/</guid>
      <description>&lt;head&gt;
    
    &lt;script src=&#34;https://mirrors.sustech.edu.cn/cdnjs/ajax/libs/crypto-js/3.1.9-1/crypto-js.js&#34;&gt;&lt;/script&gt;
&lt;/head&gt;





&lt;div class=&#34;hugo-encryptor-container&#34;&gt;
  &lt;div class=&#34;hugo-encryptor-prompt&#34;&gt;
    
      &lt;p&gt;文章的部分内容被密码保护：&lt;/p&gt;
    
  &lt;/div&gt;
  &lt;div class=&#34;hugo-encryptor-form&#34;&gt;
    &lt;input
      class=&#34;hugo-encryptor-input&#34;
      placeholder=&#39;请输入密码&#39;
    /&gt;
    &lt;input
      class=&#34;hugo-encryptor-button&#34;
      type=&#34;button&#34;
      value=&#39;CLICK&#39;
      onclick=&#34;_click_handler(this)&#34;
    /&gt;
  &lt;/div&gt;
  &lt;div
    class=&#34;hugo-encryptor-cipher-text&#34;
    data-password=&#34;aikenhong_blog&#34;
    style=&#34;display: none;&#34;
  &gt;
    &lt;span style=&#34;display: none;&#34;&gt;--- DON&#39;T MODIFY THIS LINE ---&lt;/span&gt;
    &lt;h2 id=&#34;主要是limited-labels--few-samples--data-programing&#34;&gt;主要是limited labels &amp;amp; Few Samples &amp;amp; Data programing&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;&lt;del&gt;Weakly supervised learning&lt;/del&gt;&lt;br&gt;
&lt;del&gt;semi-supervised in video field&lt;/del&gt;&lt;br&gt;
if we can recoding this work?&lt;br&gt;
&lt;del&gt;多指标下降（LOSS的耦合或者循环的选择）、相关的CV最新论文等等会在后续关注&lt;/del&gt;&lt;br&gt;
&lt;del&gt;元学习、浅层神经网络的概念等等&lt;/del&gt;  &lt;del&gt;semi-supervised&lt;/del&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;part1-limited-labels-base-on-lifeifeis-reference&#34;&gt;PART1 Limited Labels （base on LiFeiFei‘s reference）&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;in this part we may list the paper which is useful for my recoding.&lt;/em&gt;&lt;br&gt;
还有一些其他重要的可能在对论文进行重新精读的时候要记得注意reference：就比如说在loss变换和决策树生成那一块。&lt;br&gt;
&lt;em&gt;distant supervision(it&amp;rsquo;s kind of early) can be another baseline for our method, we need to understand how this method work for that situation&lt;/em&gt;&lt;br&gt;
distant supervisor到底是什么机制可以去CSDN什么的看一下&lt;/p&gt;</description>
    </item>
    <item>
      <title>IL-MgSvF</title>
      <link>https://hugotest-phi.vercel.app/posts/il-mgsvf/</link>
      <pubDate>Mon, 29 Nov 2021 13:12:05 +0000</pubDate>
      <guid>https://hugotest-phi.vercel.app/posts/il-mgsvf/</guid>
      <description>&lt;p&gt;@Author &amp;amp; Paper：&lt;a href=&#34;https://arxiv.org/pdf/2006.15524.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Arxiv&lt;/a&gt;

@Note：Aikenhong 2021/11/12&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://blog.csdn.net/cp_oldy/article/details/111714896&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Other’s Note 1 &lt;/a&gt;
&lt;/p&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;旧知识的缓慢忘记和新知识的快速适应的困境&lt;/strong&gt;：主要探讨Incremental中的Old和New的相互牵制和适应的问题，&lt;/p&gt;
&lt;p&gt;旧知识的缓慢遗忘会导致对新任务的欠拟合，而快速适应会导致灾难性的遗忘，如何对这两种策略之间进行权衡，是一个重要的问题。&lt;/p&gt;
&lt;p&gt;
&lt;div class=&#34;post-img-view&#34;&gt;
  &lt;a data-fancybox=&#34;gallery&#34; href=&#34;https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20211112114701.png&#34;&gt;
    &lt;img alt=&#34;image-20211112110043089&#34; loading=&#34;lazy&#34; src=&#34;https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20211112114701.png&#34;class=&#34;responsive-image&#34; src=&#34;https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20211112114701.png&#34; style=&#34;display: block; margin: 0 auto;&#34;
      alt=&#34;image-20211112110043089&#34;  /&gt;
  &lt;/a&gt;
&lt;/div&gt;


&lt;script&gt;
  document.addEventListener(&#34;DOMContentLoaded&#34;, function() {
      var images = document.querySelectorAll(&#34;.responsive-image&#34;);
      var maxHeight = window.innerHeight / 2.5;
      images.forEach(function(image) {
          image.style.maxHeight = maxHeight + &#34;px&#34;;
      });
  });
&lt;/script&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;多尺度混合&lt;/strong&gt;的解决这个问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Intra-space： 新类别的特征在同一个特征空间中&lt;/li&gt;
&lt;li&gt;inter-saoce：新旧类别的特征在不同的特征空间中&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;本文提出的&lt;strong&gt;多粒度策略&lt;/strong&gt;：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;提出了一种频率感知的正则化操作，加速空间内的增量学习能力&lt;/li&gt;
&lt;li&gt;新的特征空间组合操作，提升空间间的学习性能&lt;/li&gt;
&lt;/ol&gt;
&lt;!-- more --&gt;
&lt;blockquote&gt;
&lt;p&gt;实际上新类和旧类的特征最好是通过自监督或者无监督的特征学习方法归化到同一个特征空间中，在这种情况下对Classifier进行调整可能是一种更好的策略。通过混合特征空间来得到一个泛化能力更高的特征表示器。&lt;/p&gt;
&lt;p&gt;传统的策略：无论是累加还是进行数据混合进行梯度计算，这种方式应该是将类别之间的梯度进行直接的叠加。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;是否可以自行混合不同类别之间的学习梯度？通过对梯度的下降方程求解来得到一个旧类和新类之间的更好的下降方法。&lt;/li&gt;
&lt;li&gt;具体的操作上就是对step进行处理，通过mixdataset对梯度进行分开计算&lt;/li&gt;
&lt;li&gt;在混合策略上可以考虑梯度的下降方向，对不同的维度进行加权计算？&lt;/li&gt;
&lt;li&gt;上述的策略难以实施的点在于框架中的梯度是自动计算的，我们可以对损失进行加权，但是很难重新计算不同节点之间的梯度值&lt;/li&gt;
&lt;li&gt;退而求其次的方法就是对新旧类的损失进行加权处理, 或者直接的混合数据&lt;/li&gt;
&lt;li&gt;如果我们能获取梯度的方向, 或许我们能在每次迭代的过程中获得一个更好的加权值&lt;/li&gt;
&lt;li&gt;首先可以尝试对梯度进行获取,&lt;a href=&#34;https://zhuanlan.zhihu.com/p/168443176&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Grad&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;我们在蒸馏的过程中通过MLP对不同的类别进行聚类划分, 这种方式的聚类和传统机器学习聚类的优劣又如何对比解释呢.&lt;/li&gt;
&lt;li&gt;能不能用PCA方法或者multi-head策略来对特征进行处理, 这种类似因果的方式来分析特征中的&lt;strong&gt;冗余维度&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;上述的分析基于Mix Guide make error 的想法, 实际上还有一个问题就是Feature’s capabliity 不足的问题&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;New Key Word： Few-Shot class-incremental Learning&lt;/p&gt;</description>
    </item>
    <item>
      <title>SSL-MoCov3</title>
      <link>https://hugotest-phi.vercel.app/posts/ssl-mocov3/</link>
      <pubDate>Mon, 29 Nov 2021 13:12:05 +0000</pubDate>
      <guid>https://hugotest-phi.vercel.app/posts/ssl-mocov3/</guid>
      <description>&lt;p&gt;@Aiken 2021&lt;/p&gt;
&lt;p&gt;恺明大神对自监督学习+transformer的实证研究，针对Transformer再自监督学习学习框架中的训练不稳定问题提出了&lt;strong&gt;Random Patch Projection&lt;/strong&gt;的解决方案。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/waqkJkwqxU-7utfNnwr2Gg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Article&lt;/a&gt;
；&lt;a href=&#34;https://arxiv.org/abs/2104.02057&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;
；&lt;/p&gt;
&lt;h2 id=&#34;motivation&#34;&gt;Motivation&lt;/h2&gt;
&lt;p&gt;ViT的方法在自监督学习的任务中，精度下降的主要原因是由于算法的不稳定性，容易陷入局部的最优值，本文主要聚焦于&lt;strong&gt;采用视觉领域的自监督框架进行Transformer的训练&lt;/strong&gt;，CNN的训练方法已经是一个比较明确约定俗称的方法，而Transformer的训练架构实际上还没有被完全的构建。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Survey for Few-Shot Learning</title>
      <link>https://hugotest-phi.vercel.app/posts/fsl-collection/</link>
      <pubDate>Mon, 29 Nov 2021 13:12:05 +0000</pubDate>
      <guid>https://hugotest-phi.vercel.app/posts/fsl-collection/</guid>
      <description>&lt;p&gt;@aikenhong 2020
@h.aiken.970@gmail.com&lt;/p&gt;
&lt;p&gt;另一个综述文章：https://zhuanlan.zhihu.com/p/61215293
对该文中一些内容有一些补充，可以看看&lt;/p&gt;
&lt;p&gt;FSL简介：https://blog.csdn.net/xhw205/article/details/79491649&lt;/p&gt;
&lt;p&gt;GCN用于FSL：https://blog.csdn.net/qq_36022260/article/details/93753532&lt;/p&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;FSL的根本目的就是弥合人工智能和人类之间的鸿沟，从少量带有监督信息的示例中学习。像人类一样有很高的泛化能力。这也能解决在实际应用场景中，数据难以收集或者大型数据难以建立的情景。&lt;/p&gt;
&lt;p&gt;FSL的&lt;strong&gt;核心问题&lt;/strong&gt;是：经验风险最小化器不可靠；那么如何&lt;strong&gt;使用先验知识&lt;/strong&gt;去解决这个问题？&lt;/p&gt;
&lt;p&gt;三个主要的角度：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;数据：使用先验知识增强数据的监督经验&lt;/li&gt;
&lt;li&gt;模型：使用先验知识来降低假设空间&lt;/li&gt;
&lt;li&gt;算法：使用先验知识来改变搜索最佳假设（来进行搜索？)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;现阶段针对FSL提出的一些相关的机器学习方法：
&lt;code&gt;meta-learning;&lt;/code&gt; &lt;code&gt;embedding learning;&lt;/code&gt;  &lt;code&gt;generative modeling etc.&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;本文的主要工作：&lt;/strong&gt;&lt;/p&gt;
&lt;!-- more --&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;基于FSL的原有设定，在现阶段的FSL发展上给出正式定义，同时阐明具体目标以及解决方式&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;通过具体示例列举和FSL的相关学习问题，比较了相关性和差异性，更好的区分问题&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;指出核心问题：经验风险最小化器不可靠，这提供了更系统有组织的改进FSL的方向。
经验风险最小化器👉：基于ML中的错误分解来分析的&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;整理，更好的理解&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;未来方向&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;notation-and-terminology&#34;&gt;Notation and Terminology&lt;/h2&gt;
&lt;p&gt;
&lt;div class=&#34;post-img-view&#34;&gt;
  &lt;a data-fancybox=&#34;gallery&#34; href=&#34;https://raw.githubusercontent.com/AikenH/md-image/master/img/image-20200519154522883.png&#34;&gt;
    &lt;img alt=&#34; &#34; loading=&#34;lazy&#34; src=&#34;https://raw.githubusercontent.com/AikenH/md-image/master/img/image-20200519154522883.png&#34;class=&#34;responsive-image&#34; src=&#34;https://raw.githubusercontent.com/AikenH/md-image/master/img/image-20200519154522883.png&#34; style=&#34;display: block; margin: 0 auto;&#34;
      alt=&#34; &#34;  /&gt;
  &lt;/a&gt;
&lt;/div&gt;


&lt;script&gt;
  document.addEventListener(&#34;DOMContentLoaded&#34;, function() {
      var images = document.querySelectorAll(&#34;.responsive-image&#34;);
      var maxHeight = window.innerHeight / 2.5;
      images.forEach(function(image) {
          image.style.maxHeight = maxHeight + &#34;px&#34;;
      });
  });
&lt;/script&gt;

一般基于参数方法（因为非参数方法需要大量数据），在假设空间中搜索最优假设，并基于基于标签的Loss Function 来衡量效果。&lt;/p&gt;
&lt;h2 id=&#34;main-body&#34;&gt;Main Body&lt;/h2&gt;
&lt;h3 id=&#34;overview&#34;&gt;Overview&lt;/h3&gt;
&lt;p&gt;2.1：具体定义&amp;amp;示例 2.2：相关问题和FSL的相关性和差异 2.3：核心问题 2.4:现有的方法如何处理这个问题&lt;/p&gt;</description>
    </item>
    <item>
      <title>Data Augmentation</title>
      <link>https://hugotest-phi.vercel.app/posts/dataaugmentation/</link>
      <pubDate>Sun, 28 Nov 2021 06:24:20 +0000</pubDate>
      <guid>https://hugotest-phi.vercel.app/posts/dataaugmentation/</guid>
      <description>&lt;p&gt;intergrate with those augmentation method.&lt;/p&gt;
&lt;p&gt;this doc will&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Record those theory and the effect after transformation&lt;/li&gt;
&lt;li&gt;Show the codes for ez use&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And the complete &lt;code&gt;.py&lt;/code&gt; will be intergrate in my classification pipeline&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;reference&lt;/strong&gt; below:arrow_down_small:, if use them,start it for respect for his work.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/aleju/imgaug#documentation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;aleju/imgaug&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;:star:&lt;a href=&#34;https://github.com/albumentations-team/albumentations&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;albumentations-team/albumentations: &lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://pytorch.org/vision/stable/transforms.html#transforms-on-pil-image-and-torch-tensor&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;torchvision&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://pillow.readthedocs.io/en/stable/reference/ImageEnhance.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PIL/ImageEnhance CCBS&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;opencv&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;principle&#34;&gt;Principle&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Principle 1&lt;/strong&gt; of coding: Don’t reinvent the wheel unless it’s needed&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;具体而言，仅在函数的拓展性较差，无法对其定制化，满足我们的日常需求的时候，我们会自行编写函数从而满足我们的需求，否则我们直接引用已知的库，提升我们的实现效率。&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- more --&gt;
&lt;p&gt;&lt;strong&gt;Principle 2&lt;/strong&gt; of coding 图像增强的两种使用方式：&lt;/p&gt;</description>
    </item>
    <item>
      <title>MLFlow</title>
      <link>https://hugotest-phi.vercel.app/posts/mlflow/</link>
      <pubDate>Sun, 28 Nov 2021 06:24:19 +0000</pubDate>
      <guid>https://hugotest-phi.vercel.app/posts/mlflow/</guid>
      <description>&lt;h1 id=&#34;mlflow-机器学习系统的使用&#34;&gt;MLFlow 机器学习系统的使用&lt;/h1&gt;
&lt;p&gt;@Aiken 2020&lt;/p&gt;
&lt;p&gt;&lt;em&gt;基于Python开发的DAG数据工作流系统，面向机器学习，支持Spark并行环境和K8S容器集群；&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;MLFlow主要解决了三个问题，也就是三个我们可能会需要使用的功能：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Tracking&lt;/strong&gt;：跟踪实验训练结果，记录算法参数，模型结果和运行效果等等；&lt;/li&gt;
&lt;li&gt;Projects：对所有的算法项目有一套标准的projects概念，记录下代码版本，参数和运行环境这些东西，并且projects是可以拟合所有的算法框架的；&lt;/li&gt;
&lt;li&gt;Models：解决的是打包和部署模型的这样一个行为，提供json接口给后续的flsk框架等等进行使用&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;基本部署&#34;&gt;基本部署&lt;/h2&gt;
&lt;p&gt;INSTALL：&lt;/p&gt;
&lt;p&gt;DEPLOY：&lt;/p&gt;
&lt;h2 id=&#34;tracking-实验版本跟踪&#34;&gt;Tracking 实验版本跟踪&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Tracking&lt;/strong&gt;为本次描述的重点，来做一个训练过程中的版本管理，记录每一次训练的参数和变量信息等等，这样便于后续的恢复和实验信息的整理。便于统计和管理。使用的时候好像也是需要代码嵌入的部分，就是需要在代码中调用MLFlow的API。&lt;/p&gt;
&lt;p&gt;但是在Tracking的时候有一个比较重要的点在于，这个方法和&lt;code&gt;Tensorboard&lt;/code&gt;对原模型的参数的嵌入和Logging记录中&lt;u&gt;会不会产生冲突&lt;/u&gt;，同时两个方法之间是不是有什么overlap；关键的问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;这两个API能不能进行混合使用&lt;/li&gt;
&lt;li&gt;怎么统一和区分两个方法的应用情景&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;!-- more --&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://mlflow.org/docs/latest/tracking.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://mlflow.org/docs/latest/tracking.html&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://mlflow.org/docs/latest/projects.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://mlflow.org/docs/latest/projects.html&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/mlflow/mlflow&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/mlflow/mlflow&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.csdn.net/chenhuipin1173/article/details/100913909&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://blog.csdn.net/chenhuipin1173/article/details/100913909&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://my.oschina.net/u/2306127/blog/1825638&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://my.oschina.net/u/2306127/blog/1825638&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.zhihu.com/question/280162556&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.zhihu.com/question/280162556&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Hard Task Sampling</title>
      <link>https://hugotest-phi.vercel.app/posts/hardtask/</link>
      <pubDate>Sun, 28 Nov 2021 06:24:17 +0000</pubDate>
      <guid>https://hugotest-phi.vercel.app/posts/hardtask/</guid>
      <description>&lt;h1 id=&#34;trickhard-task&#34;&gt;Trick：Hard Task&lt;/h1&gt;
&lt;p&gt;思路来源于Meta-Tranfer-Learning，基本思路是在Meta-Learning的每一次Meta-Test的时候，会从预训练错误率比较高的Task中再次采样，增加那些task的训练次数。也就是难题多做的策略。&lt;/p&gt;
&lt;h2 id=&#34;基本思路&#34;&gt;基本思路&lt;/h2&gt;
&lt;h3 id=&#34;对比adaboost&#34;&gt;对比Adaboost&lt;/h3&gt;
&lt;p&gt;这样的思路其实和AdaBoost的想法是有一定的异曲同工之妙的，或者说其实就是AdaBoost的思路：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Adaboost&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/39972832&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;参考笔记&lt;/a&gt;
，从该笔记中我们可以看到，AdaBoost的基本思路如下：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Boosting算法的工作机制是首先从训练集用初始权重训练出一个弱学习器1，根据弱学习的学习误差率表现来&lt;strong&gt;更新训练样本的权重&lt;/strong&gt;，使得之前弱学习器1学习误差率高的训练样本点的权重变高，使得这些&lt;strong&gt;误差率高的点&lt;/strong&gt;在后面的弱学习器2中&lt;strong&gt;得到更多的重视&lt;/strong&gt;。然后基于调整权重后的训练集来训练弱学习器2.，如此重复进行，直到弱学习器数达到事先指定的数目T，最终将这T个弱学习器通过集合策略进行整合，得到最终的强学习器.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;和Meta-Transfer-Learning对比一下，我们可以发现，这个方法实际上就是讲Transfer Learning的与训练网络当成弱学习器1，然后通过弱学习器1的训练样本权重，来增大Hard-Task的配比（也就是增加任务的权重）完全一致。&lt;/p&gt;
&lt;h3 id=&#34;具体实现&#34;&gt;具体实现&lt;/h3&gt;
&lt;p&gt;实现上主要是，样本sample的过程，就是如何在进行参数选择后和原本的Dataloader，结合起来。在这里我们主要参考MTL中的方法，进行网络的构建处理。&lt;/p&gt;
&lt;p&gt;第一部分：&lt;strong&gt;sampler构建&lt;/strong&gt;，为了后续Dataloader中进行数据的采样，需要构建一个这样的sampler，关键在于index的对应关系，以及最后输出的是index的集合。&lt;/p&gt;
&lt;!-- more --&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;14
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;15
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;16
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;17
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;18
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;19
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;20
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;21
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;22
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;23
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;24
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;25
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;26
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;27
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;28
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;29
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;30
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;31
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;32
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;33
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;34
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;35
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;36
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;37
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;torch&lt;/span&gt; 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;numpy&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;np&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 注意的点，我们需要确定我们batch数目，cls数目和每次每个cls选出多少个数据per&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 紧接着定义一个sample，sample输出的是对应原dataset中的数据的index，&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;class&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;CatagoriesSampler&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;():&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;fm&#34;&gt;__init__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;label&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;n_batch&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;n_cls&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;n_per&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;n_batch&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;n_batch&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;n_cls&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;n_cls&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;n_per&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;n_per&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;label&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;array&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;label&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;c1&#34;&gt;# 根据不同的label输入情况，我们可可能需要找到每个label对应的样本的index，将其整合在一起。如下（option）&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;m_idx&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;range&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;max&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;label&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;idx&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;argwhere&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;label&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;==&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;reshape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;idx&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;from_numpy&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;idx&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;m_idx&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;append&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;idx&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;   
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;	&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;fm&#34;&gt;__len__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;c1&#34;&gt;# 要注意一下这里数据的长度是根据我们要输出的batch数目决定的&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;n_batch&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;fm&#34;&gt;__iter__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;c1&#34;&gt;# 直接定义每次采样的时候的batch输出&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i_batch&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;range&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;n_batch&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;batch&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;classes&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;randperm&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;m_idx&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))[:&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;n_cls&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;c&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;classes&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                &lt;span class=&#34;c1&#34;&gt;# 随机选择出的类标签&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                &lt;span class=&#34;n&#34;&gt;l&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;m_idx&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;c&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                &lt;span class=&#34;c1&#34;&gt;# 随机选择样本&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                &lt;span class=&#34;n&#34;&gt;random_pos&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;randperm&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;l&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))[:&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;n_per&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                &lt;span class=&#34;n&#34;&gt;batch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;append&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;l&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;random_pos&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;c1&#34;&gt;# stack t and reshape的作用👇&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;c1&#34;&gt;# stack 变成n_cls * n_per , t转置，reshape（-1）变成行向量&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;batch&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;stack&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;batch&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;reshape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;k&#34;&gt;yield&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;batch&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;第二部分：直接调用部分&lt;/p&gt;</description>
    </item>
    <item>
      <title>并行训练</title>
      <link>https://hugotest-phi.vercel.app/posts/%E5%B9%B6%E8%A1%8C%E8%AE%AD%E7%BB%83%E7%9A%84%E7%90%86%E8%A7%A3/</link>
      <pubDate>Sun, 28 Nov 2021 02:20:31 +0000</pubDate>
      <guid>https://hugotest-phi.vercel.app/posts/%E5%B9%B6%E8%A1%8C%E8%AE%AD%E7%BB%83%E7%9A%84%E7%90%86%E8%A7%A3/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://lilianweng.github.io/lil-log/2021/09/24/train-large-neural-networks.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;How to Train Really Large Models on Many GPUs? (lilianweng.github.io)&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;对于浮点运算，模型参数的存储和中间计算输出（梯度和优化器状态）的存储的在 GPU 内存上的大量需求使得我们需要并行化，下面我们参考一些常用的并行化范式：&lt;/p&gt;
&lt;!-- more --&gt;
&lt;h2 id=&#34;数据并行策略&#34;&gt;&lt;strong&gt;数据并行&lt;/strong&gt;策略：&lt;/h2&gt;
&lt;p&gt;在Multi-GPUs上Duplicate模型，然后分别feed数据，进行运算，每个batch同步或者异步的进行多卡间的梯度传递和模型同步。&lt;/p&gt;
&lt;p&gt;同步可能会导致每个batch需要停止训练，异步则是可能会使用陈旧的梯度进行一段时间的训练，增加了计算时间。&lt;/p&gt;
&lt;p&gt;而在PT1.5以来，使用一种中间的方式：每隔x次迭代，进行多卡间的全局同步梯度一次，使用这种梯度积累的策略，根据计算图来进行计算和通信调度的优化，提高吞吐量。&lt;/p&gt;
&lt;h2 id=&#34;模型并行范式&#34;&gt;&lt;strong&gt;模型并行&lt;/strong&gt;范式：&lt;/h2&gt;
&lt;p&gt;是为了解决单模型过大无法存储在单一的Node上的问题，但是这样会有GPU间的顺序依赖，虽然减少了内存的占用和计算量，但是这种IO的需求导致计算资源的利用率严重不足。&lt;/p&gt;
&lt;p&gt;在这种pipeline中，就存在利用率的bubble，也就是空白的部分&lt;/p&gt;
&lt;p&gt;
&lt;div class=&#34;post-img-view&#34;&gt;
  &lt;a data-fancybox=&#34;gallery&#34; href=&#34;https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20211111171158.png&#34;&gt;
    &lt;img alt=&#34;image-20211111165251900&#34; loading=&#34;lazy&#34; src=&#34;https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20211111171158.png&#34;class=&#34;responsive-image&#34; src=&#34;https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20211111171158.png&#34; style=&#34;display: block; margin: 0 auto;&#34;
      alt=&#34;image-20211111165251900&#34;  /&gt;
  &lt;/a&gt;
&lt;/div&gt;


&lt;script&gt;
  document.addEventListener(&#34;DOMContentLoaded&#34;, function() {
      var images = document.querySelectorAll(&#34;.responsive-image&#34;);
      var maxHeight = window.innerHeight / 2.5;
      images.forEach(function(image) {
          image.style.maxHeight = maxHeight + &#34;px&#34;;
      });
  });
&lt;/script&gt;
&lt;/p&gt;
&lt;h2 id=&#34;pipeline并行策略&#34;&gt;&lt;strong&gt;Pipeline并行&lt;/strong&gt;策略：&lt;/h2&gt;
&lt;p&gt;混合模型和数据并行的策略，来减少低效时间的泡沫，也就是，将一个batch切分成多个小batch，然后分发到每个node上，减少相应的等待时间，只要我们对计算量和运行速度有合理的把握，就能极大的降低这个inefficient time bubbles. 多个mini-batch的梯度聚集起来最后同步更新. 最有情况下甚至可以忽略气泡的开销&lt;/p&gt;
&lt;div&gt;
$$ 
1- \frac{2md}{(2m+2(d-1))d} = \frac{d-1}{m+d-1}
 $$
&lt;/div&gt;
&lt;p&gt;m个mini-batch和d个分布, bubble的比例将如上述所示&lt;/p&gt;
&lt;p&gt;
&lt;div class=&#34;post-img-view&#34;&gt;
  &lt;a data-fancybox=&#34;gallery&#34; href=&#34;https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20211111170038.png&#34;&gt;
    &lt;img alt=&#34;image-20211111170037307&#34; loading=&#34;lazy&#34; src=&#34;https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20211111170038.png&#34;class=&#34;responsive-image&#34; src=&#34;https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20211111170038.png&#34; style=&#34;display: block; margin: 0 auto;&#34;
      alt=&#34;image-20211111170037307&#34;  /&gt;
  &lt;/a&gt;
&lt;/div&gt;


&lt;script&gt;
  document.addEventListener(&#34;DOMContentLoaded&#34;, function() {
      var images = document.querySelectorAll(&#34;.responsive-image&#34;);
      var maxHeight = window.innerHeight / 2.5;
      images.forEach(function(image) {
          image.style.maxHeight = maxHeight + &#34;px&#34;;
      });
  });
&lt;/script&gt;
&lt;/p&gt;</description>
    </item>
    <item>
      <title>MIM-V-simMIM</title>
      <link>https://hugotest-phi.vercel.app/posts/mim-v-simmim/</link>
      <pubDate>Tue, 23 Nov 2021 06:38:19 +0000</pubDate>
      <guid>https://hugotest-phi.vercel.app/posts/mim-v-simmim/</guid>
      <description>&lt;p&gt;@Author： MSRA Zhenda Xie
@Source：&lt;a href=&#34;arxiv.org/abs/2111.09886&#34;&gt;Arxiv&lt;/a&gt;
， &lt;a href=&#34;https://github.com/microsoft/SimMIM&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Code TBP&lt;/a&gt;
，&lt;a href=&#34;https://mp.weixin.qq.com/s/4YVYM9lPYghtZFhyOGnERw&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Blog_CVer&lt;/a&gt;

@Read：AikenHong 2021.11.22&lt;/p&gt;
&lt;p&gt;“What I cannot create, I do not understand.” — Richard Feynman&lt;/p&gt;
&lt;h2 id=&#34;intro--simple-conclusion&#34;&gt;Intro &amp;amp; Simple Conclusion&lt;/h2&gt;
&lt;h3 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;继MAE和iBoT之后，MSRA也提出了一个图像掩码建模的新框架，SimMIM，该方法简化了最近这些提出的方法，不需要特殊设计，作者也验证了不需要那些特殊设计就已经能让模型展现出优秀的学习能力&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;采用中等大小的掩码块（32），对输入图像进行随机掩码，能使其成为强大的代理任务（pretext task）&lt;/li&gt;
&lt;li&gt;直接回归预测原始像素的RGB值的效果并不比复杂设计的Patch分类方法差&lt;/li&gt;
&lt;li&gt;Projector Head可以是轻量的Linear Layer，效果并不一定比MLP（多层）的差&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;p&gt;通过这种MIM方法可以实现在大量无标注的数据上得到一个表征能力up的通用特征模型，这种方式的backbone可以广泛的应用到图像上的各种子任务中（按照NLP）的经验来说，而为了类似的方式在图像上的大放异彩，我们首先需要分析Vision和Language的不同&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;图像有更强的局部关系&lt;/strong&gt;：相互靠近的像素是高度相关和近似的，我们可以通过简单的copy padding复制一部分缺失&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;视觉信号是原始，低层次的，而文本分词是高级概念&lt;/strong&gt;：对低层次信号的预测是否对高层次的视觉识别任务有用呢？&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;视觉信号是连续的，而文本的分词是离散的&lt;/strong&gt;： 如何基于分类的掩码语言建模方法来处理连续的视觉信号&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;theoretical-design&#34;&gt;Theoretical Design&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;掩码选择&lt;/strong&gt;：同样的掩码的策略还是基于Patch进行的，对于掩码的设计来说，太大的掩码快或者太密集的掩码快，可能会导致找不到附近的像素来预测，实验证明32是一个具有竞争力的size，和文本任务的信息冗余程度不同也带来了覆盖比的选择，NLP通常是0.15，而在V中，32size可以支持0.1-0.7的覆盖率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;任务选择&lt;/strong&gt;：使用原始像素的回归任务，因为回归任务和具有有序性的视觉信号的连续性很好的吻合。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;预测头选择&lt;/strong&gt;：使用轻量的预测头如（linear），迁移性能与繁琐的预测头相似或者略好，同时训练上更加的块。虽&lt;strong&gt;然较大的头或更高的分辨率通常会导致更强的生成能力，但这种更强的能力不一定有利于下游的微调任务&lt;/strong&gt;。&lt;/p&gt;
&lt;!-- more --&gt;
&lt;p&gt;
&lt;div class=&#34;post-img-view&#34;&gt;
  &lt;a data-fancybox=&#34;gallery&#34; href=&#34;https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20211123104311&#34;&gt;
    &lt;img alt=&#34;图片&#34; loading=&#34;lazy&#34; src=&#34;https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20211123104311&#34;class=&#34;responsive-image&#34; src=&#34;https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20211123104311&#34; style=&#34;display: block; margin: 0 auto;&#34;
      alt=&#34;图片&#34;  /&gt;
  &lt;/a&gt;
&lt;/div&gt;


&lt;script&gt;
  document.addEventListener(&#34;DOMContentLoaded&#34;, function() {
      var images = document.querySelectorAll(&#34;.responsive-image&#34;);
      var maxHeight = window.innerHeight / 2.5;
      images.forEach(function(image) {
          image.style.maxHeight = maxHeight + &#34;px&#34;;
      });
  });
&lt;/script&gt;
&lt;/p&gt;</description>
    </item>
    <item>
      <title>Transformer</title>
      <link>https://hugotest-phi.vercel.app/posts/transformer/</link>
      <pubDate>Tue, 23 Nov 2021 06:38:19 +0000</pubDate>
      <guid>https://hugotest-phi.vercel.app/posts/transformer/</guid>
      <description>&lt;p&gt;@aikenhong 2021&lt;/p&gt;
&lt;p&gt;References For Transformer:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;NLP &lt;a href=&#34;https://lilianweng.github.io/lil-log/2020/04/07/the-transformer-family.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Transformer Family (lilianweng.github.io)&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;VIT &lt;a href=&#34;https://mp.weixin.qq.com/s?__biz=MjM5ODExNDA2MA==&amp;amp;mid=2449941486&amp;amp;idx=1&amp;amp;sn=336a47a31f4b4ff0f6cd8e2fc3cb184a&amp;amp;chksm=b13c258d864bac9b32d10ec36a058d77cc7cf90e066e76ae476fd2fde1b54256cd608a559bb6&amp;amp;mpshare=1&amp;amp;scene=23&amp;amp;srcid=1101rcBaNzO4pu00PCPsJOAl&amp;amp;sharer_sharetime=1635744838591&amp;amp;sharer_shareid=ec299f1c891fc72cd699f8eaeb8a0cd5#rd&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Transformer眼中世界 VS CNN眼中世界&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;李沐 NLP &lt;a href=&#34;https://www.bilibili.com/video/BV1pu411o7BE?spm_id_from=333.999.0.0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Transformer论文精读&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;Suveys &lt;a href=&#34;https://mp.weixin.qq.com/s?__biz=MzUxNjcxMjQxNg==&amp;amp;mid=2247514162&amp;amp;idx=2&amp;amp;sn=d094eecbfd91ca1e478c41e29f2b98d5&amp;amp;scene=21#wechat_redirect&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;cver1&lt;/a&gt;
， &lt;a href=&#34;https://mp.weixin.qq.com/s?__biz=MzUxNjcxMjQxNg==&amp;amp;mid=2247514982&amp;amp;idx=2&amp;amp;sn=7e38021234b7ab5455429e4485128efd&amp;amp;chksm=f9a1c9e9ced640ff045d1c4fe9d4e98a785602d980b25df4fa18477dd2b4b829ed4fc3fd028f&amp;amp;scene=21#wechat_redirect&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;cver2&lt;/a&gt;
，&lt;a href=&#34;https://mp.weixin.qq.com/s/_th7rXfZDuSu2xo7gdPp0w&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;cver3&lt;/a&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This blog will divided into several part : lil&amp;rsquo;s blog, the survey for ViT, we using those article to help us understand the transformer.&lt;/p&gt;
&lt;p&gt;综述我们以最新的一篇为准进行阅读，其他的可能后续进行查缺补漏把，如无必要，勿增烦恼。&lt;/p&gt;
&lt;h2 id=&#34;intro导言&#34;&gt;Intro导言&lt;/h2&gt;
&lt;p&gt;主要参考文章2来进行我们简单的导入&lt;/p&gt;
&lt;h3 id=&#34;基本问题&#34;&gt;基本问题&lt;/h3&gt;
&lt;p&gt;Transformer原本是NLP中的重要模型, 作为LSTM的后继者, 用于处理Seq2Seq的数据类型和情景, 若是要将Transformer运用到Vision的领域中, 首要的问题就是如何:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;将Image作为序列化的Token输入Transform中&lt;/strong&gt; , 而达成这个目的主要有三种典型的方法:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;像素点作为token,&lt;/li&gt;
&lt;li&gt;使用VAE离散化图片作为token再输入&lt;/li&gt;
&lt;li&gt;ViT: 将图片切为一个个&lt;code&gt;Patch&lt;/code&gt;在经过线性的&lt;code&gt;projector&lt;/code&gt;之后组成一个&lt;code&gt;embedding&lt;/code&gt;表示进行交互&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;
&lt;div class=&#34;post-img-view&#34;&gt;
  &lt;a data-fancybox=&#34;gallery&#34; href=&#34;https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/3070imgs/20211120010516&#34;&gt;
    &lt;img alt=&#34;图片&#34; loading=&#34;lazy&#34; src=&#34;https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/3070imgs/20211120010516&#34;class=&#34;responsive-image&#34; src=&#34;https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/3070imgs/20211120010516&#34; style=&#34;display: block; margin: 0 auto;&#34;
      alt=&#34;图片&#34;  /&gt;
  &lt;/a&gt;
&lt;/div&gt;


&lt;script&gt;
  document.addEventListener(&#34;DOMContentLoaded&#34;, function() {
      var images = document.querySelectorAll(&#34;.responsive-image&#34;);
      var maxHeight = window.innerHeight / 2.5;
      images.forEach(function(image) {
          image.style.maxHeight = maxHeight + &#34;px&#34;;
      });
  });
&lt;/script&gt;
&lt;/p&gt;</description>
    </item>
    <item>
      <title>MIM-V-iBOT</title>
      <link>https://hugotest-phi.vercel.app/posts/mim-v-ibot/</link>
      <pubDate>Thu, 18 Nov 2021 06:35:52 +0000</pubDate>
      <guid>https://hugotest-phi.vercel.app/posts/mim-v-ibot/</guid>
      <description>&lt;p&gt;@Read: AikenHong 2021&lt;/p&gt;
&lt;p&gt;@Author: &lt;a href=&#34;https://arxiv.org/abs/2111.07832&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/abs/2111.07832&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;@解读：&lt;a href=&#34;https://mp.weixin.qq.com/s/x4yEfg9eqW6x3Ehxm1HkRA&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Machine Heart&lt;/a&gt;
&lt;/p&gt;
&lt;h2 id=&#34;基本思想&#34;&gt;基本思想&lt;/h2&gt;
&lt;p&gt;基于NLP中的MLM(Masked Language Model)的核心训练目标: 也就是遮住文本的一部分, 然后通过模型去预测和补全, 这一过程是模型学到泛化的特征, 使用这种方法来进行大规模的与训练范式.&lt;/p&gt;
&lt;p&gt;在基本的思想上和MAE采用的是一样的设计, 但是本文中坐着认为visual tokenizer的设计才是其中的关键.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;不同于 NLP 中 tokenization 通过离线的词频分析即可将语料编码为含高语义的分词，图像 patch 是连续分布的且存在大量冗余的底层细节信息。而作者认为一个能够提取图像 patch 中高层语义的 tokenizer 可帮助模型避免学习到冗余的这些细节信息。作者认为视觉的 tokenizer 应该具备两个属性：（a）具备完整表征连续图像内容的能力；(b) 像 NLP 中的 tokenizer 一样具备高层语义。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;文中对tokenizer的设计为一个知识蒸馏的过程:&lt;/p&gt;
&lt;p&gt;
&lt;div class=&#34;post-img-view&#34;&gt;
  &lt;a data-fancybox=&#34;gallery&#34; href=&#34;https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/3070imgs/20211118151616.png&#34;&gt;
    &lt;img alt=&#34;image-20211118151613545&#34; loading=&#34;lazy&#34; src=&#34;https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/3070imgs/20211118151616.png&#34;class=&#34;responsive-image&#34; src=&#34;https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/3070imgs/20211118151616.png&#34; style=&#34;display: block; margin: 0 auto;&#34;
      alt=&#34;image-20211118151613545&#34;  /&gt;
  &lt;/a&gt;
&lt;/div&gt;


&lt;script&gt;
  document.addEventListener(&#34;DOMContentLoaded&#34;, function() {
      var images = document.querySelectorAll(&#34;.responsive-image&#34;);
      var maxHeight = window.innerHeight / 2.5;
      images.forEach(function(image) {
          image.style.maxHeight = maxHeight + &#34;px&#34;;
      });
  });
&lt;/script&gt;
&lt;/p&gt;
&lt;p&gt;文中使用这种在线tokenizer同时来监督这样的MIM过程, 也就是两部分协同学习, 能够较好的保证语义的同时并将图像内容转化为连续的特征分布, 具体的, tokenizer和目标网络狗狗想网络结构, 有移动平均来得到实际的tokenizer.&lt;/p&gt;</description>
    </item>
    <item>
      <title>MIM-V-MAE</title>
      <link>https://hugotest-phi.vercel.app/posts/mim-v-mae/</link>
      <pubDate>Mon, 15 Nov 2021 12:25:45 +0000</pubDate>
      <guid>https://hugotest-phi.vercel.app/posts/mim-v-mae/</guid>
      <description>&lt;p&gt;@Author：Facebook AI Research-Kaiming He
&lt;a href=&#34;https://zhuanlan.zhihu.com/p/432663453&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kaiming-MAE&lt;/a&gt;
&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;总而言之这是一种大模型的训练方法, 通过在少量数据的基础上实现大模型的训练.&lt;/p&gt;
&lt;p&gt;整体的架构上是参考了NLP中的AutoEncoder机制，将原图切分patch，用mask掩盖原图，通过少量可见的Patch进行Encoder后和Mask融合，再通过&lt;strong&gt;非对称&lt;/strong&gt;的Decoder进行pixel的还原。&lt;/p&gt;
&lt;p&gt;这种设计的有点在于mask的scala是可变的，同时这种mask能减少我们训练过程中对显存和计算复杂度的损耗，同时问题本身是一个比较复杂的问题，得以训练复杂的大模型，这种方式最终呈现的效果就是训练的效率高且效益好。&lt;/p&gt;
&lt;p&gt;体现了自监督学习在这方面的优越性，同时这种方法得以实现也是由于ViT模型对于CNN模型的取代，才使得这种序列化切块的方式容易实现和验证。&lt;/p&gt;
&lt;p&gt;这种方式在最终体现了自监督学习对于有监督与训练的优越性，使用这种方式能够更好的得到一个模型的通用表征。&lt;/p&gt;
&lt;p&gt;在这里论文中也说明了和NLP的不同点以及这样的模型对于decoder的要求实际上是比NLP更高的&lt;/p&gt;
&lt;p&gt;
&lt;div class=&#34;post-img-view&#34;&gt;
  &lt;a data-fancybox=&#34;gallery&#34; href=&#34;https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20211115113546.png&#34;&gt;
    &lt;img alt=&#34;image-20211115113542074&#34; loading=&#34;lazy&#34; src=&#34;https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20211115113546.png&#34;class=&#34;responsive-image&#34; src=&#34;https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20211115113546.png&#34; style=&#34;display: block; margin: 0 auto;&#34;
      alt=&#34;image-20211115113542074&#34;  /&gt;
  &lt;/a&gt;
&lt;/div&gt;


&lt;script&gt;
  document.addEventListener(&#34;DOMContentLoaded&#34;, function() {
      var images = document.querySelectorAll(&#34;.responsive-image&#34;);
      var maxHeight = window.innerHeight / 2.5;
      images.forEach(function(image) {
          image.style.maxHeight = maxHeight + &#34;px&#34;;
      });
  });
&lt;/script&gt;
&lt;/p&gt;
&lt;h2 id=&#34;experiment&#34;&gt;experiment&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Masking&lt;/strong&gt;：对于输入的图像进行均匀的切分并均匀的随机采样&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;MAE encoder&lt;/strong&gt;: 简单的ViT模型，对输入图像进行编码后和Mask进行混合得到一个完整的令牌集合，从而确保Decode能够得到对应的位置信息。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;MAE decoder&lt;/strong&gt;：轻量级的架构，可以独立于编码器进行设计，我们使用更窄更浅的网络，计算量比编码器10%更小，这样能够更快的进行训练。解码器的最后一层是先行投影，输出的数量==补丁中像素值的数量，最后会resize层原图的维度。&lt;/p&gt;</description>
    </item>
    <item>
      <title>OWL-survey</title>
      <link>https://hugotest-phi.vercel.app/posts/owl-survey/</link>
      <pubDate>Fri, 12 Nov 2021 09:40:46 +0000</pubDate>
      <guid>https://hugotest-phi.vercel.app/posts/owl-survey/</guid>
      <description>&lt;p&gt;@AikenHong2021 OWL&lt;/p&gt;
&lt;p&gt;分析现有的OWL特点，和当前自己的研究做一个区分，也汲取一下别人的研究的要点，&lt;/p&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;arxiv @ &lt;a href=&#34;https://arxiv.org/pdf/2102.07848.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;self-supervised feature improve open-world learning&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/374268236&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arxiv&lt;/a&gt;
 @ &lt;a href=&#34;https://arxiv.org/pdf/2102.03526.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;open-world semi-supervised learning&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;arxiv @ &lt;a href=&#34;https://arxiv.org/pdf/2011.12906.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;open-world learning without labels&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;arxiv @ &lt;a href=&#34;https://arxiv.org/pdf/1801.05609.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;unseen class discovery in open-world classification&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;arxiv @ &lt;a href=&#34;https://arxiv.org/pdf/2109.06628.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Open-World Active Learning with Stacking Ensemble for Self-Driving Cars&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://dl.acm.org/doi/pdf/10.1145/3308558.3313644&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;www&lt;/a&gt;
 @ &lt;a href=&#34;https://blog.csdn.net/u011150266/article/details/118242627&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;open-world learning and application to product classification&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;cvpr @ &lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2021/papers/Mancini_Open_World_Compositional_Zero-Shot_Learning_CVPR_2021_paper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;open world composition zero-shot learning&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2103.02603.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;cvpr&lt;/a&gt;
 @ &lt;a href=&#34;https://zhuanlan.zhihu.com/p/356272271&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Towards Open World Object Detection&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;[cvpr](&lt;a href=&#34;https://openaccess.thecvf.com/content_CVPR_2019/papers/Liu_Large-Scale_Long-Tailed_Recognition_in_an_Open_World_CVPR_2019_paper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Large-Scale Long-Tailed Recognition in an Open World (thecvf.com)&lt;/a&gt;
) @ &lt;a href=&#34;https://github.com/zhmiao/OpenLongTailRecognition-OLTR&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Large-Scale Long-Tailed Recognition in an Open World&lt;/a&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;h2 id=&#34;papers&#34;&gt;Papers&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Mulit Open world Learning Definition&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>SS_OD_SoftTeacher</title>
      <link>https://hugotest-phi.vercel.app/posts/ss_od_softteacher/</link>
      <pubDate>Sat, 09 Oct 2021 02:30:08 +0000</pubDate>
      <guid>https://hugotest-phi.vercel.app/posts/ss_od_softteacher/</guid>
      <description>&lt;p&gt;@ Article: ICML from Microsoft &amp;amp; Huazhong Keda
@ Code: &lt;a href=&#34;https://github.com/microsoft/SoftTeacher&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;

@ Noteby: Aikenhong
@ Time: 20210914&lt;/p&gt;
&lt;h2 id=&#34;abstrast-and-intro&#34;&gt;Abstrast and Intro&lt;/h2&gt;
&lt;p&gt;in the session we will using describe the main idea of this article.&lt;/p&gt;
&lt;p&gt;这篇文章的重点在于Soft Teacher，也就是用pseudo label做为弱标注，逐步提高伪标签的可靠性。&lt;/p&gt;
&lt;p&gt;不同于多阶段的方法，端到端的方法再训练中逐步的提升伪标签的质量从而再去benifit目标检测的质量。
这样E2E的框架主要依赖于两部分技术:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;soft teacher: 每个未标记边界框的分类损失由教师网络产生的分类分数进行加权&lt;/li&gt;
&lt;li&gt;box jitter 窗口抖动: 选择可靠的伪框来学习框回归&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在目标检测上获得SOTA的效果;&lt;/p&gt;
&lt;h3 id=&#34;multi-stage&#34;&gt;Multi-Stage&lt;/h3&gt;
&lt;p&gt;在半监督的情况下，关注的主要是基于伪标签的方法，是目前的SOTA，以往的方法采用多阶段的方式。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;使用标记数据训练初始检测器&lt;/li&gt;
&lt;li&gt;未标记数据的伪标记，同时基于伪标签进行重新训练&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;局限&lt;/strong&gt;：初始少量标注的局限，初始的检测器的伪标签质量&lt;/p&gt;
&lt;h3 id=&#34;end-to-end&#34;&gt;End to End&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Soft Teacher&lt;/strong&gt;基本思路：对未标记的图像进行标记，然后通过标记的几个伪标签训练检测器.&lt;/p&gt;
&lt;!-- more --&gt;
&lt;p&gt;具体而言：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;采样标注和未标注图片形成Batch&lt;/li&gt;
&lt;li&gt;双模型：检测（student）、标记（teacher）&lt;/li&gt;
&lt;li&gt;EMA：T模型是S模型的EMA&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这种方式避免了多阶段方案实现上的复杂，同时实现了飞轮效应==S、T相互加强;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;此外Soft Teacher&lt;/strong&gt;直接对学生模型生成的所有候选框进行评估，而不是使用伪框来为这些候选框进行分类回归。
这样能使用更多的直接监督信息&lt;/p&gt;
&lt;p&gt;具体而言：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;使用高阈值来分割前景，确保不会错误的将背景分类成前景，确保正伪标签的高精度；&lt;/li&gt;
&lt;li&gt;使用可靠性度量来加权背景候选的损失；&lt;/li&gt;
&lt;li&gt;教师模型产生的检测分数可以很好的作为可靠性度量&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Box Jitter&lt;/strong&gt;为了更可靠的训练学生网络的本地分支，指的是：&lt;/p&gt;</description>
    </item>
    <item>
      <title>StyleGAN</title>
      <link>https://hugotest-phi.vercel.app/posts/stylegan/</link>
      <pubDate>Sun, 03 Oct 2021 13:16:40 +0000</pubDate>
      <guid>https://hugotest-phi.vercel.app/posts/stylegan/</guid>
      <description>&lt;h1 id=&#34;stylegan-v1&#34;&gt;StyleGAN V1&lt;/h1&gt;
&lt;p&gt;@AikenHong 2020 10.8&lt;/p&gt;
&lt;p&gt;《A Style-Based Generator Architecture for Generative Adversarial Networks》&lt;/p&gt;
&lt;h2 id=&#34;related-work&#34;&gt;Related Work：&lt;/h2&gt;
&lt;p&gt;继承的文献工作： ProGAN
参考解读：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.csdn.net/a312863063/article/details/88795147&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;《其中子链接值得一看》&lt;/a&gt;
（包括源码解析啥的）（甚至还有GAN的笔记）&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.gwylab.com/pdf/Note_StyleGAN.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;《StyleGan源码解析和拓展应用》&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cuijiahua.com/blog/2020/07/dl-22.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;《秃头生成器1》&lt;/a&gt;
&lt;a href=&#34;https://cloud.tencent.com/developer/article/1658228&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;《秃头生成器2》 &lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/swlh/hairstyle-transfer-semantic-editing-gan-latent-code-b3a6ccf91e82&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NO.3&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Contribution（Problem）：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;解纠缠：Mapping Network&lt;/li&gt;
&lt;li&gt;Noise Generator&lt;/li&gt;
&lt;li&gt;AdaIN before all conv&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;structure&#34;&gt;Structure：&lt;/h2&gt;
&lt;p&gt;
&lt;div class=&#34;post-img-view&#34;&gt;
  &lt;a data-fancybox=&#34;gallery&#34; href=&#34;https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210930135941.png&#34;&gt;
    &lt;img alt=&#34;image-20210930135938114&#34; loading=&#34;lazy&#34; src=&#34;https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210930135941.png&#34;class=&#34;responsive-image&#34; src=&#34;https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210930135941.png&#34; style=&#34;display: block; margin: 0 auto;&#34;
      alt=&#34;image-20210930135938114&#34;  /&gt;
  &lt;/a&gt;
&lt;/div&gt;


&lt;script&gt;
  document.addEventListener(&#34;DOMContentLoaded&#34;, function() {
      var images = document.querySelectorAll(&#34;.responsive-image&#34;);
      var maxHeight = window.innerHeight / 2.5;
      images.forEach(function(image) {
          image.style.maxHeight = maxHeight + &#34;px&#34;;
      });
  });
&lt;/script&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;div class=&#34;post-img-view&#34;&gt;
  &lt;a data-fancybox=&#34;gallery&#34; href=&#34;https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210930161259.png&#34;&gt;
    &lt;img alt=&#34;image-20210930161258031&#34; loading=&#34;lazy&#34; src=&#34;https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210930161259.png&#34;class=&#34;responsive-image&#34; src=&#34;https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210930161259.png&#34; style=&#34;display: block; margin: 0 auto;&#34;
      alt=&#34;image-20210930161258031&#34;  /&gt;
  &lt;/a&gt;
&lt;/div&gt;


&lt;script&gt;
  document.addEventListener(&#34;DOMContentLoaded&#34;, function() {
      var images = document.querySelectorAll(&#34;.responsive-image&#34;);
      var maxHeight = window.innerHeight / 2.5;
      images.forEach(function(image) {
          image.style.maxHeight = maxHeight + &#34;px&#34;;
      });
  });
&lt;/script&gt;
&lt;/p&gt;</description>
    </item>
    <item>
      <title>OW Object Detector</title>
      <link>https://hugotest-phi.vercel.app/posts/ow-od/</link>
      <pubDate>Tue, 28 Sep 2021 13:44:20 +0000</pubDate>
      <guid>https://hugotest-phi.vercel.app/posts/ow-od/</guid>
      <description>&lt;p&gt;@Aiken 2021&lt;/p&gt;
&lt;p&gt;框架撞车系列，主要看看这一篇论文中怎么解决如下的问题👇，并从中借鉴和优化的我框架设计&lt;/p&gt;
&lt;h2 id=&#34;思路分析&#34;&gt;思路分析&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;模型实现的主要的两个TASK：&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Open Set Learning ： 在没有明确监督的时候，将尚未引入的目标类别识别为未知&lt;/li&gt;
&lt;li&gt;Incremental Learning：类别增量学习&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;实现这两个问题的主要思路：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;自动标注&lt;/strong&gt;：借鉴RPN的class-agnostic，以及检测和分类的显著性指标的差异，找到并自动标注NewClass&lt;/li&gt;
&lt;li&gt;**对比聚类：**使用prototype feature来进行聚类，同时计算Distance损失
it seems like contain a unknown prototype.&lt;/li&gt;
&lt;li&gt;**energy based：**亥姆霍兹自由能公式？&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;
&lt;div class=&#34;post-img-view&#34;&gt;
  &lt;a data-fancybox=&#34;gallery&#34; href=&#34;https://raw.githubusercontent.com/AikenH/md-image/master/img/image-20210412171723896.png&#34;&gt;
    &lt;img alt=&#34;image-20210412171723896&#34; loading=&#34;lazy&#34; src=&#34;https://raw.githubusercontent.com/AikenH/md-image/master/img/image-20210412171723896.png&#34;class=&#34;responsive-image&#34; src=&#34;https://raw.githubusercontent.com/AikenH/md-image/master/img/image-20210412171723896.png&#34; style=&#34;display: block; margin: 0 auto;&#34;
      alt=&#34;image-20210412171723896&#34;  /&gt;
  &lt;/a&gt;
&lt;/div&gt;


&lt;script&gt;
  document.addEventListener(&#34;DOMContentLoaded&#34;, function() {
      var images = document.querySelectorAll(&#34;.responsive-image&#34;);
      var maxHeight = window.innerHeight / 2.5;
      images.forEach(function(image) {
          image.style.maxHeight = maxHeight + &#34;px&#34;;
      });
  });
&lt;/script&gt;
&lt;/p&gt;
&lt;h3 id=&#34;energy-based&#34;&gt;&lt;strong&gt;ENERGY BASED&lt;/strong&gt;&lt;/h3&gt;
&lt;!-- more --&gt;
&lt;p&gt;Feature： $F$ , Label: $L$ ,  Energy: $E(F,l)$&lt;/p&gt;
&lt;p&gt;能量函数倾向于将已知的类别分类到低熵的分布上，然后我们可以根据特征在能量空间上的划分来区分新类和旧类。然后我们可以根据logits表达的softmax形式，找到输出和Gibbs distribution的相关性：&lt;/p&gt;
&lt;div&gt;
$$ 
p(l \mid \boldsymbol{f})=\frac{\exp \left(\frac{g_{l}(\boldsymbol{f})}{T}\right)}{\sum_{i=1}^{\mathrm{C}} \exp \left(\frac{g_{i}(\boldsymbol{f})}{T}\right)}=\frac{\exp \left(-\frac{E(\boldsymbol{f}, l)}{T}\right)}{\exp \left(-\frac{E(\boldsymbol{f})}{T}\right)}
 $$
&lt;/div&gt;
&lt;p&gt;通过这个相关性，我们对自由能进行一个定义，以logits的形式表达&lt;/p&gt;</description>
    </item>
    <item>
      <title>Attention Mechanism</title>
      <link>https://hugotest-phi.vercel.app/posts/attention/</link>
      <pubDate>Tue, 28 Sep 2021 05:34:22 +0000</pubDate>
      <guid>https://hugotest-phi.vercel.app/posts/attention/</guid>
      <description>&lt;p&gt;@Aiken 2020.9.16&lt;/p&gt;
&lt;p&gt;对基本注意力机制的一些资料和理解做一些简单的汇总，着重分析基本思想原理，应用和实现（即 structure），还有一些Weakness和相应的解决方案。&lt;/p&gt;
&lt;!-- more --&gt;
&lt;p&gt;&lt;strong&gt;1.TODO-List：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;根据Lil’Log的Attention？Attention！进行初步的整理&lt;/li&gt;
&lt;li&gt;各个分类的具体含义分开整理，理解一部分整理一部分，可能结合实际的应用去整理吧。&lt;/li&gt;
&lt;li&gt;其中很重要的一点是数学分析的部分，需要对数学原理进行整理和领会。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;whats-attention-in-deep-learning&#34;&gt;What’s Attention In Deep Learning&lt;/h2&gt;
&lt;p&gt;在某种程度上，注意力是由我么如何关注视觉图像的不同区域或者我们如何关联同一个句子中的不同单词所启发的：针对于问题的不同，我们会对图像的某些具体的区域重视（某些区域在视觉中呈现高分辨率，而另一些则是低分辨率的情况），或者句子中的某些词重视的情况。&lt;/p&gt;
&lt;p&gt;
&lt;div class=&#34;post-img-view&#34;&gt;
  &lt;a data-fancybox=&#34;gallery&#34; href=&#34;https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210911182724.png&#34;&gt;
    &lt;img alt=&#34;image-20210118210915289&#34; loading=&#34;lazy&#34; src=&#34;https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210911182724.png&#34;class=&#34;responsive-image&#34; src=&#34;https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210911182724.png&#34; style=&#34;display: block; margin: 0 auto;&#34;
      alt=&#34;image-20210118210915289&#34;  /&gt;
  &lt;/a&gt;
&lt;/div&gt;


&lt;script&gt;
  document.addEventListener(&#34;DOMContentLoaded&#34;, function() {
      var images = document.querySelectorAll(&#34;.responsive-image&#34;);
      var maxHeight = window.innerHeight / 2.5;
      images.forEach(function(image) {
          image.style.maxHeight = maxHeight + &#34;px&#34;;
      });
  });
&lt;/script&gt;
&lt;/p&gt;
&lt;p&gt;可以解释一个句子中紧密的上下文单词之间的关系，比如我们看到eating就会期待看到food，而color对于我们来说就没有那么重要。&lt;/p&gt;
&lt;p&gt;
&lt;div class=&#34;post-img-view&#34;&gt;
  &lt;a data-fancybox=&#34;gallery&#34; href=&#34;https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210911182729.png&#34;&gt;
    &lt;img alt=&#34;image-20210118210936862&#34; loading=&#34;lazy&#34; src=&#34;https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210911182729.png&#34;class=&#34;responsive-image&#34; src=&#34;https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210911182729.png&#34; style=&#34;display: block; margin: 0 auto;&#34;
      alt=&#34;image-20210118210936862&#34;  /&gt;
  &lt;/a&gt;
&lt;/div&gt;


&lt;script&gt;
  document.addEventListener(&#34;DOMContentLoaded&#34;, function() {
      var images = document.querySelectorAll(&#34;.responsive-image&#34;);
      var maxHeight = window.innerHeight / 2.5;
      images.forEach(function(image) {
          image.style.maxHeight = maxHeight + &#34;px&#34;;
      });
  });
&lt;/script&gt;
&lt;/p&gt;</description>
    </item>
    <item>
      <title>EfficientNet</title>
      <link>https://hugotest-phi.vercel.app/posts/efficientnet/</link>
      <pubDate>Tue, 28 Sep 2021 05:34:22 +0000</pubDate>
      <guid>https://hugotest-phi.vercel.app/posts/efficientnet/</guid>
      <description>&lt;p&gt;Tags: Paper
URL1: &lt;a href=&#34;https://arxiv.org/pdf/1905.11946.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/pdf/1905.11946.pdf&lt;/a&gt;

URL2: &lt;a href=&#34;https://arxiv.org/pdf/2104.00298.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/pdf/2104.00298.pdf&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;提出了一种模型缩放策略，如何更高效的平衡网络的深度、宽度、和图片分辨率
&lt;strong&gt;1. Efficient Net: Rethinking Model Scaling for Convolutional Neural Networks
2. EfficientNetV2: Smaller Models and Faster Training&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;@Aiken H 2021 find detail to code his&lt;/p&gt;
&lt;h1 id=&#34;efficient-net-v1&#34;&gt;Efficient Net V1&lt;/h1&gt;
&lt;p&gt;除了提出了缩放策略以外，还使用神经架构搜索还建立了一个新的baseline network，得到了一系列模型。&lt;/p&gt;
&lt;p&gt;平衡网络宽度、深度、分辨率至关重要，这种平衡可以通过简单的恒定比率缩放维度来实现，于是我们&lt;strong&gt;提出了一种简单有效的复合缩放&lt;/strong&gt;方法。&lt;/p&gt;
&lt;p&gt;
&lt;div class=&#34;post-img-view&#34;&gt;
  &lt;a data-fancybox=&#34;gallery&#34; href=&#34;https://raw.githubusercontent.com/AikenH/md-image/master/img/image-20210610180603496.png&#34;&gt;
    &lt;img alt=&#34;https://raw.githubusercontent.com/AikenH/md-image/master/img/image-20210610180603496.png&#34; loading=&#34;lazy&#34; src=&#34;https://raw.githubusercontent.com/AikenH/md-image/master/img/image-20210610180603496.png&#34;class=&#34;responsive-image&#34; src=&#34;https://raw.githubusercontent.com/AikenH/md-image/master/img/image-20210610180603496.png&#34; style=&#34;display: block; margin: 0 auto;&#34;
      alt=&#34;https://raw.githubusercontent.com/AikenH/md-image/master/img/image-20210610180603496.png&#34;  /&gt;
  &lt;/a&gt;
&lt;/div&gt;


&lt;script&gt;
  document.addEventListener(&#34;DOMContentLoaded&#34;, function() {
      var images = document.querySelectorAll(&#34;.responsive-image&#34;);
      var maxHeight = window.innerHeight / 2.5;
      images.forEach(function(image) {
          image.style.maxHeight = maxHeight + &#34;px&#34;;
      });
  });
&lt;/script&gt;
&lt;/p&gt;
&lt;p&gt;复合缩放的物理意义：输入图像更大的话就需要更多层来增加感受野和更多通道，从而能在更大的图像上捕获更多细粒度的图案，而宽度和深度（对于表达能力来说很重要）之间也存在着一定的关系，“我们”是第一个对此进行了建模的。&lt;/p&gt;
&lt;p&gt;从各个维度单独的进行缩放能发现都存在着增益瓶颈，如何去得到这么一个合适的等比缩放增益&lt;/p&gt;
&lt;!-- more --&gt;
&lt;h2 id=&#34;motivation-and-method&#34;&gt;Motivation and Method&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;一些直观上的motivation，以及假想&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>RL-DouZero</title>
      <link>https://hugotest-phi.vercel.app/posts/rl-douzero/</link>
      <pubDate>Tue, 06 Jul 2021 13:51:48 +0000</pubDate>
      <guid>https://hugotest-phi.vercel.app/posts/rl-douzero/</guid>
      <description>&lt;p&gt;Desc: GAME, RL
Finished?: Yes
Tags: Paper
URL1: &lt;a href=&#34;https://arxiv.org/abs/2106.06135&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/abs/2106.06135&lt;/a&gt;

URL2: &lt;a href=&#34;https://github.com/kwai/DouZero&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/kwai/DouZero&lt;/a&gt;

URL3: &lt;a href=&#34;https://github.com/datamllab/rlcard-showdown&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/datamllab/rlcard-showdown&lt;/a&gt;
）&lt;/p&gt;
&lt;p&gt;使用蒙特卡洛方法进行自我对弈不断更新预测模型的方法，这实际上也是普通人对于强化学习如何在self-play中实现自我更新的最基础的想法把：
自我对弈（记录动作序列）- 用最终的胜负（价值）更新网络。&lt;/p&gt;
&lt;h2 id=&#34;算法的设计和思路&#34;&gt;算法的设计和思路&lt;/h2&gt;
&lt;p&gt;算法的目标是学习一个价值网路。网络的输入是当前状态和一个动作，输出是在当前状态做这个动作的期望收益（比如胜率）。简单来说，价值网络在每一步计算出哪种牌型赢的概率最大，然后选择最有可能赢的牌型。蒙特卡罗方法不断重复以下步骤来优化价值网络：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;用价值网络生成一场对局&lt;/li&gt;
&lt;li&gt;记录下该对局中所有的状态、动作和最后的收益（胜率）&lt;/li&gt;
&lt;li&gt;将每一对状态和动作作为网络输入，收益作为网络输出，用梯度下降对价值网络进行一次更新&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;其实，所谓的蒙特卡罗方法就是一种随机模拟，即通过不断的重复实验来估计真实价值。&lt;/p&gt;
&lt;p&gt;如下图所示，斗零采用一个价值神经网络，其输入是状态和动作，输出是价值。首先，过去的出牌用 LSTM 神经网络进行编码。然后 LSTM 的输出以及其他的表征被送入了 6 层全连接网络，最后输出价值。&lt;/p&gt;
&lt;!-- more --&gt;
&lt;p&gt;
&lt;div class=&#34;post-img-view&#34;&gt;
  &lt;a data-fancybox=&#34;gallery&#34; href=&#34;https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210911211016&#34;&gt;
    &lt;img alt=&#34;img&#34; loading=&#34;lazy&#34; src=&#34;https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210911211016&#34;class=&#34;responsive-image&#34; src=&#34;https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210911211016&#34; style=&#34;display: block; margin: 0 auto;&#34;
      alt=&#34;img&#34;  /&gt;
  &lt;/a&gt;
&lt;/div&gt;


&lt;script&gt;
  document.addEventListener(&#34;DOMContentLoaded&#34;, function() {
      var images = document.querySelectorAll(&#34;.responsive-image&#34;);
      var maxHeight = window.innerHeight / 2.5;
      images.forEach(function(image) {
          image.style.maxHeight = maxHeight + &#34;px&#34;;
      });
  });
&lt;/script&gt;
&lt;/p&gt;
&lt;p&gt;系统训练的主要瓶颈在于模拟数据的生成，因为每一步出牌都要对神经网络做一次前向传播。斗零采用多演员（actor）的架构，在单个 GPU 服务器上，用了 45 个演员同时产生数据，最终数据被汇集到一个中央训练器进行训练。比较有趣的是，斗零并不需要太多的计算资源，仅仅需要一个普通的四卡 GPU 服务器就能达到不错的效果。这可以让大多数实验室轻松基于作者的代码做更多的尝试。&lt;/p&gt;
&lt;p&gt;该方法的设计和实现上听起来都挺简单的，可以找个时间自己测试一下，玩一玩这个东西，对于我来说，看看他们怎么用这个lstm去进行历史编码的，以及在对transformer了解后，看看如何用transformer去替代这样的lstm是我这边的研究重点。&lt;/p&gt;
&lt;h2 id=&#34;蒙特卡洛方法存在的问题&#34;&gt;蒙特卡洛方法存在的问题&lt;/h2&gt;
&lt;p&gt;蒙特卡罗方法在强化学习领域中被大多数研究者忽视。学界普遍认为蒙特卡罗方法存在两个缺点：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;蒙特卡罗方法不能处理不完整的状态序列&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;蒙特卡罗方法有很大的方差，导致采样效率很低。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;但是斗地主中，可以产生转正的状态序列，同时很容易通过并行来采集大量的样本降低方差，主要是实现上简单，但是可能也是需要大量的数据把。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Pooling</title>
      <link>https://hugotest-phi.vercel.app/posts/pooling/</link>
      <pubDate>Wed, 23 Jun 2021 13:48:56 +0000</pubDate>
      <guid>https://hugotest-phi.vercel.app/posts/pooling/</guid>
      <description>&lt;h1 id=&#34;downsamplingpooling的全面调研&#34;&gt;DownSampling：Pooling的全面调研&lt;/h1&gt;
&lt;p&gt;@Aiken 2021 笔记摘录：&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/341820742&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;深度神经网络中的池化方法：全面调研（1989-2020） - 知乎&lt;/a&gt;
 ；&lt;a href=&#34;https://www.sohu.com/a/442710521_823210&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;相同论文的简单中文Version&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;16页综述，共计67篇参考文献。网络千奇百怪，但基础元素却大致相同！本文全面调研了1989至2020年一些著名且有用的池化方法，并主要对20种池化方法进行了详细介绍（这些方法，你都知道么？） 注1：文末附【计算机视…&lt;/p&gt;
&lt;p&gt;来自 &lt;a href=&#34;https://zhuanlan.zhihu.com/p/341820742&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://zhuanlan.zhihu.com/p/341820742&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;原文：《Pooling Methods in Deep Neural Networks, a Review》&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/112216409&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;整合2&lt;/a&gt;
&lt;/p&gt;
&lt;h2 id=&#34;池化的根本目的motivation&#34;&gt;池化的根本目的（Motivation）&lt;/h2&gt;
&lt;p&gt;卷积神经网络是DNN的一种特殊类型，它由几个卷积层组成，每个卷积层后都有一个激活函数和一个池化层。&lt;/p&gt;
&lt;p&gt;池化层是重要的层，它对来自上一层的特征图执行下采样，并生成具有简化分辨率的新feature maps 。该层&lt;strong&gt;极大地减小了输入的空间尺寸&lt;/strong&gt;。 它有两个主要目的。 首先是减少参数或权重的数量，从而减少计算成本。 第二是控制网络的过拟合。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;池化可以增加网络对于平移（旋转，伸缩）的不变性，提升网络的泛化能力。&lt;/li&gt;
&lt;li&gt;增大感受野；&lt;/li&gt;
&lt;li&gt;降低优化难度和参数数目，&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;理想的池化方法应仅提取有用的信息，并丢弃无关的细节。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;特征不变性、特征降维、在一定程度上防止过拟合，更方便优化&lt;/strong&gt;&lt;/p&gt;
&lt;!-- more --&gt;
&lt;h2 id=&#34;主流的池化方法&#34;&gt;主流的池化方法&lt;/h2&gt;
&lt;h3 id=&#34;average-pooling-平均池化&#34;&gt;Average Pooling 平均池化&lt;/h3&gt;
&lt;p&gt;没啥好说的，就是每个block取一个均值。如下图所示：更关注全局特征&lt;/p&gt;
&lt;h3 id=&#34;max-pooling-最大值池化&#34;&gt;Max Pooling 最大值池化&lt;/h3&gt;
&lt;p&gt;更关注重要的局部特征&lt;/p&gt;
&lt;p&gt;
&lt;div class=&#34;post-img-view&#34;&gt;
  &lt;a data-fancybox=&#34;gallery&#34; href=&#34;https://raw.githubusercontent.com/AikenH/md-image/master/img/image-20210219153154458.png&#34;&gt;
    &lt;img alt=&#34;https://raw.githubusercontent.com/AikenH/md-image/master/img/image-20210219153154458.png&#34; loading=&#34;lazy&#34; src=&#34;https://raw.githubusercontent.com/AikenH/md-image/master/img/image-20210219153154458.png&#34;class=&#34;responsive-image&#34; src=&#34;https://raw.githubusercontent.com/AikenH/md-image/master/img/image-20210219153154458.png&#34; style=&#34;display: block; margin: 0 auto;&#34;
      alt=&#34;https://raw.githubusercontent.com/AikenH/md-image/master/img/image-20210219153154458.png&#34;  /&gt;
  &lt;/a&gt;
&lt;/div&gt;


&lt;script&gt;
  document.addEventListener(&#34;DOMContentLoaded&#34;, function() {
      var images = document.querySelectorAll(&#34;.responsive-image&#34;);
      var maxHeight = window.innerHeight / 2.5;
      images.forEach(function(image) {
          image.style.maxHeight = maxHeight + &#34;px&#34;;
      });
  });
&lt;/script&gt;
&lt;/p&gt;</description>
    </item>
    <item>
      <title>OW-openmix</title>
      <link>https://hugotest-phi.vercel.app/posts/ow-openmix/</link>
      <pubDate>Wed, 23 Jun 2021 13:45:50 +0000</pubDate>
      <guid>https://hugotest-phi.vercel.app/posts/ow-openmix/</guid>
      <description>&lt;p&gt;@Aiken 2021 究极万恶的撞车论文&lt;/p&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Motivation&lt;/strong&gt; ：Tackle the problem of 发现无标注数据中与给定（已知）类别不相交的新类。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Related Research：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;现有的方法通常1. 使用标记数据对模型进行预训练； 2. 无监督聚类在未标记的数据中识别新的类&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;作者认为label带来的essential knowledge在第二步中没有被充分学习利用到，这样模型就只能从第一步的现成知识中获益，而不能利用标记数据和未标记数据之间的潜在关系&lt;/p&gt;
&lt;/blockquote&gt;
&lt;!-- more --&gt;
&lt;p&gt;&lt;strong&gt;Hypothesis：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;有标记的类别和无标记的类别之间没有Overlap，这样导致在两个类别之间很难建立学习关系，（为啥我感觉这个说的都是屁话）&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Solution：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Openmix：将标注的数据和未标注的数据同时混合起来得到一个联合标签的分布中，用两种方式来动态合成示例：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;我们混合标记和未标记数据作为Training Img，混合了已知类别的先验生成的伪标签会比无监督情况下生成的伪标签跟家的可靠？防止在错误的伪标签前提下发生过拟合&lt;/li&gt;
&lt;li&gt;在第一步的时候我们鼓励具有高类别置信度的无标记example作为可考虑的类别，然后我们将这些samples作为anchor，并将它们进一步的和无标注的samples整合，这使得我们能够对无标注数据产生更多的组合，并发现更精细的新类关系。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;detail&#34;&gt;Detail&lt;/h2&gt;
&lt;p&gt;果然在混合的方式上和MixUp的策略进行比对了，就是diss了Mixup使用伪标签的情景可能会进一步的引入不确定性，导致算法的效果反向优化，就是再label和unlabeled数据上混用mixup，而不是单纯的对unlabel数据集进行混合。&lt;/p&gt;
&lt;p&gt;首先将没有overlap的标签表现为联合标签分布再进行混合，也就是加长onehot，这样的标签的优越性在？对于unlabelled data引入了确定性，防止标签容易过拟合。也就是给伪标签加入了一个锚定，让他能够变化的更平滑&lt;/p&gt;
&lt;p&gt;
&lt;div class=&#34;post-img-view&#34;&gt;
  &lt;a data-fancybox=&#34;gallery&#34; href=&#34;https://raw.githubusercontent.com/AikenH/md-image/master/img/image-20210414225637547.png&#34;&gt;
    &lt;img alt=&#34;image-20210414225637547&#34; loading=&#34;lazy&#34; src=&#34;https://raw.githubusercontent.com/AikenH/md-image/master/img/image-20210414225637547.png&#34;class=&#34;responsive-image&#34; src=&#34;https://raw.githubusercontent.com/AikenH/md-image/master/img/image-20210414225637547.png&#34; style=&#34;display: block; margin: 0 auto;&#34;
      alt=&#34;image-20210414225637547&#34;  /&gt;
  &lt;/a&gt;
&lt;/div&gt;


&lt;script&gt;
  document.addEventListener(&#34;DOMContentLoaded&#34;, function() {
      var images = document.querySelectorAll(&#34;.responsive-image&#34;);
      var maxHeight = window.innerHeight / 2.5;
      images.forEach(function(image) {
          image.style.maxHeight = maxHeight + &#34;px&#34;;
      });
  });
&lt;/script&gt;
&lt;/p&gt;
&lt;p&gt;这尼玛这张图看了不久完事了，bibi一大堆啥的呢。主要分析一下三个损失函数代表的是什么意思。&lt;/p&gt;
&lt;p&gt;
&lt;div class=&#34;post-img-view&#34;&gt;
  &lt;a data-fancybox=&#34;gallery&#34; href=&#34;https://raw.githubusercontent.com/AikenH/md-image/master/img/image-20210414231455696.png&#34;&gt;
    &lt;img alt=&#34;image-20210414231455696&#34; loading=&#34;lazy&#34; src=&#34;https://raw.githubusercontent.com/AikenH/md-image/master/img/image-20210414231455696.png&#34;class=&#34;responsive-image&#34; src=&#34;https://raw.githubusercontent.com/AikenH/md-image/master/img/image-20210414231455696.png&#34; style=&#34;display: block; margin: 0 auto;&#34;
      alt=&#34;image-20210414231455696&#34;  /&gt;
  &lt;/a&gt;
&lt;/div&gt;


&lt;script&gt;
  document.addEventListener(&#34;DOMContentLoaded&#34;, function() {
      var images = document.querySelectorAll(&#34;.responsive-image&#34;);
      var maxHeight = window.innerHeight / 2.5;
      images.forEach(function(image) {
          image.style.maxHeight = maxHeight + &#34;px&#34;;
      });
  });
&lt;/script&gt;
&lt;/p&gt;</description>
    </item>
    <item>
      <title>Reward is Enough</title>
      <link>https://hugotest-phi.vercel.app/posts/rl-reward_is_enough/</link>
      <pubDate>Sun, 06 Jun 2021 13:53:36 +0000</pubDate>
      <guid>https://hugotest-phi.vercel.app/posts/rl-reward_is_enough/</guid>
      <description>&lt;p&gt;Desc: RL
Finished?: Yes
Tags: Paper&lt;/p&gt;
&lt;p&gt;通用人工智能，是否能通过强化学习的奖励机制就实现&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/XTNyLjZ9KfdtHY4Omb9_4w&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;实现AGI，强化学习就够了？Sutton、Silver师徒联手：奖励机制足够实现各种目标&lt;/a&gt;
&lt;/p&gt;
&lt;h2 id=&#34;对reward构建agi的可行性的分析和探讨&#34;&gt;对reward构建AGI的可行性的分析和探讨&lt;/h2&gt;
&lt;p&gt;这篇文章实际上没有给出一个很好的方案通过reward来实现各种AGI的设计，但是给出了在每一种场景下的AGI的reward设计的设想把。和对用reward进行设计的可行性分析。
同时分析了：感知、社交、语言、泛化、模仿，这几个方面&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;类似地，如果人工智能体的经验流足够丰富，那么单一目标（例如电池寿命或生存）可能隐含地需要实现同样广泛的子目标的能力，因此奖励最大化应该足以产生一种通用人工智能。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;这不久回到了最基础的问题，没有这种长线以及大量数据交互以及全面场景的经验流，来支撑这样一个AGI的学习，所以这不也是在现阶段上纸上谈兵嘛？&lt;/p&gt;
&lt;p&gt;对这篇论文我的总结是，我不推荐详细阅读，我觉得收益有限，太理想化，其实和强化学习本身的假设也没有太多新东西，我们可以假设强化学习能带来一个AGI，但是对应的约束和限制确实是有点多了。&lt;/p&gt;</description>
    </item>
    <item>
      <title>RL-MobaAI</title>
      <link>https://hugotest-phi.vercel.app/posts/rl-mobaai_tencent/</link>
      <pubDate>Sun, 30 May 2021 13:52:42 +0000</pubDate>
      <guid>https://hugotest-phi.vercel.app/posts/rl-mobaai_tencent/</guid>
      <description>&lt;p&gt;Created by: Aiken H
Desc: GAME, RL
Finished?: Yes
Tags: Paper&lt;/p&gt;
&lt;p&gt;《Master Complex Control in MOBA Games with Deep Reinforcement Learning》 论文阅读笔记&lt;/p&gt;
&lt;p&gt;@Aiken H 2021.06&lt;/p&gt;
&lt;h2 id=&#34;introduction-and-related-research&#34;&gt;Introduction and Related Research.&lt;/h2&gt;
&lt;p&gt;MOBA游戏的复杂度和状态空间都远比以前的围棋之类的运动更大，所以难度会更大一些&lt;/p&gt;
&lt;p&gt;早一些的游戏ai使用的是（2015） Deep Q-Network  通过 supervised learning and self-play 结合的训练策略在围棋上击败了专业人类，而最近更多的使用了DRL（Deep Reinforcement Learning）的方法在近几年被进一步的应用。&lt;/p&gt;
&lt;h3 id=&#34;neural-network-architecture-include&#34;&gt;&lt;strong&gt;Neural Network Architecture Include&lt;/strong&gt;&lt;/h3&gt;
&lt;h3 id=&#34;contributions&#34;&gt;&lt;strong&gt;Contributions&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;the &lt;strong&gt;encoding&lt;/strong&gt; of &lt;strong&gt;Multi-modal inputs 多模态输入&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;the &lt;strong&gt;decoupling&lt;/strong&gt; of inter-correlations in controls &lt;strong&gt;控制内关联解码&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;exploration &lt;strong&gt;pruning&lt;/strong&gt; mechanism  &lt;strong&gt;剪枝设置&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Action mask&lt;/strong&gt; for efficient exploration ❓&lt;strong&gt;效率&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;attack &lt;strong&gt;attention&lt;/strong&gt;(for target selection) &lt;strong&gt;Attention机制做目标选择&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;LSTM&lt;/strong&gt; for learning skill combos &lt;strong&gt;LSTM 机制做技能释放和链接&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Optimize&lt;/strong&gt; by multi-label proximal policy algorithm(&lt;strong&gt;improved PPO&lt;/strong&gt;)
&lt;ul&gt;
&lt;li&gt;dual-clip PPO 帮助训练的收敛&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;!-- more --&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;present a systematic and thorough study&lt;/p&gt;</description>
    </item>
    <item>
      <title>RL Notebook 01</title>
      <link>https://hugotest-phi.vercel.app/posts/rl-c1/</link>
      <pubDate>Sun, 23 May 2021 13:50:06 +0000</pubDate>
      <guid>https://hugotest-phi.vercel.app/posts/rl-c1/</guid>
      <description>&lt;p&gt;Created by: Aiken H
Detail: survey
Finished?: No
Tags: Paper
URL1: &lt;a href=&#34;https://www.cnblogs.com/pinard/category/1254674.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.cnblogs.com/pinard/category/1254674.html&lt;/a&gt;

URL2: &lt;a href=&#34;https://github.com/ljpzzz/machinelearning&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/ljpzzz/machinelearning&lt;/a&gt;

URL3: &lt;a href=&#34;https://datawhalechina.github.io/easy-rl/#/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://datawhalechina.github.io/easy-rl/#/&lt;/a&gt;
&lt;/p&gt;
&lt;h1 id=&#34;chapter1-模型基础&#34;&gt;Chapter1 模型基础&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://www.cnblogs.com/pinard/p/9385570.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;强化学习（一）模型基础&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;强化学习是介于监督和无监督学习之间的，强化学习没有输出值，但是有&lt;strong&gt;reward：&lt;/strong&gt; 同时这个reward是事后给出的，而不是及时回馈的。而无监督学习是只有数据特征，同时数据之间是独立的，没有前后依赖的关系。&lt;/p&gt;
&lt;p&gt;
&lt;div class=&#34;post-img-view&#34;&gt;
  &lt;a data-fancybox=&#34;gallery&#34; href=&#34;https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210911210000.png&#34;&gt;
    &lt;img alt=&#34;https://images2018.cnblogs.com/blog/1042406/201807/1042406-20180729163058011-290427357.png&#34; loading=&#34;lazy&#34; src=&#34;https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210911210000.png&#34;class=&#34;responsive-image&#34; src=&#34;https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210911210000.png&#34; style=&#34;display: block; margin: 0 auto;&#34;
      alt=&#34;https://images2018.cnblogs.com/blog/1042406/201807/1042406-20180729163058011-290427357.png&#34;  /&gt;
  &lt;/a&gt;
&lt;/div&gt;


&lt;script&gt;
  document.addEventListener(&#34;DOMContentLoaded&#34;, function() {
      var images = document.querySelectorAll(&#34;.responsive-image&#34;);
      var maxHeight = window.innerHeight / 2.5;
      images.forEach(function(image) {
          image.style.maxHeight = maxHeight + &#34;px&#34;;
      });
  });
&lt;/script&gt;
&lt;/p&gt;
&lt;h2 id=&#34;theory理论基础&#34;&gt;Theory理论基础&lt;/h2&gt;
&lt;!-- more --&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;简化模型介绍：&lt;/p&gt;
&lt;p&gt;上面的大脑代表我们的算法执行个体，我们可以操作个体来做决策，即选择一个合适的动作（Action）At。下面的地球代表我们要研究的环境,它有自己的状态模型，我们选择了动作At后，环境的状态(State)会变，我们会发现环境状态已经变为St+1,同时我们得到了我们采取动作At的延时奖励(Reward)Rt+1。然后个体可以继续选择下一个合适的动作，然后环境的状态又会变，又有新的奖励值。。。这就是强化学习的思路。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;强化学习的模型关键要素：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;环境的状态S：t时刻环境的状态 $S_t$ 是它环境状态集中的某一个状态&lt;/li&gt;
&lt;li&gt;个体的动作A：个体在某个时刻可能做出的动作集合&lt;/li&gt;
&lt;li&gt;环境的奖励R：个体在某个时刻对应状态下做出的动作 $A_t$ 得到的奖励会在t+1时刻得到&lt;/li&gt;
&lt;li&gt;个体的策略 $\pi$ ：个体根据当前的环境选择采取动作的策略分布（函数），一般表示为一个条件概率分布的形式，概率大的动作被个体选择的概率显然更高&lt;/li&gt;
&lt;/ol&gt;
&lt;div&gt;
$$ \pi(a|s)= P(A_t = a | S_t = s) $$
&lt;/div&gt;
&lt;ol start=&#34;5&#34;&gt;
&lt;li&gt;在策略 $\pi$ 和状态s采行动后的价值 $v_\pi(s)$ ：一般是一个期望函数，因为我们不能每次只能选择当前的reward最大的策略，而是需要考虑大局，所以我们要有一个综合的（当前和后续）的延时奖励。&lt;/li&gt;
&lt;/ol&gt;
&lt;div&gt;
$$ v_\pi(s) = \mathbb{E}(R_{t+1} + \gamma R_{t+2} + \gamma ^2 R_{t+3} + ... |S_t = s) $$
&lt;/div&gt;
&lt;ol start=&#34;6&#34;&gt;
&lt;li&gt;奖励衰减因子 $\gamma$ ：也就是上式的权重，极端值考虑贪婪和一致等同，范围在[0,1]&lt;/li&gt;
&lt;li&gt;环境的状态转移模型：也就是环境从s经过a后转化下一个状态的状态机，也可以表示为一个概率模型 $P_{ss^‘}^a$ (s→s&amp;rsquo; , a)&lt;/li&gt;
&lt;li&gt;探索率 $\epsilon$ ：主要用于训练迭代中，我们一般选择当前价值最大的动作，但是为了搜索空间的完备，我们会用 $\epsilon$ 的概率去选择非最大价值的动作，来提升训练的鲁棒性&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;SUMMARY：主要介绍了强化学习模型的workflow以及其中需要考虑的8个主要参数和函数架构。最主要的机制还是Policy和reward设计这一块&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>经典深度学习与机器学习算法</title>
      <link>https://hugotest-phi.vercel.app/posts/intview_%E9%9D%A2%E8%AF%95%E8%A1%A5%E5%85%85/</link>
      <pubDate>Fri, 23 Apr 2021 01:43:43 +0000</pubDate>
      <guid>https://hugotest-phi.vercel.app/posts/intview_%E9%9D%A2%E8%AF%95%E8%A1%A5%E5%85%85/</guid>
      <description>&lt;head&gt;
    
    &lt;script src=&#34;https://mirrors.sustech.edu.cn/cdnjs/ajax/libs/crypto-js/3.1.9-1/crypto-js.js&#34;&gt;&lt;/script&gt;
&lt;/head&gt;





&lt;div class=&#34;hugo-encryptor-container&#34;&gt;
  &lt;div class=&#34;hugo-encryptor-prompt&#34;&gt;
    
      &lt;p&gt;文章的部分内容被密码保护：&lt;/p&gt;
    
  &lt;/div&gt;
  &lt;div class=&#34;hugo-encryptor-form&#34;&gt;
    &lt;input
      class=&#34;hugo-encryptor-input&#34;
      placeholder=&#39;请输入密码&#39;
    /&gt;
    &lt;input
      class=&#34;hugo-encryptor-button&#34;
      type=&#34;button&#34;
      value=&#39;CLICK&#39;
      onclick=&#34;_click_handler(this)&#34;
    /&gt;
  &lt;/div&gt;
  &lt;div
    class=&#34;hugo-encryptor-cipher-text&#34;
    data-password=&#34;aikenhong_blog&#34;
    style=&#34;display: none;&#34;
  &gt;
    &lt;span style=&#34;display: none;&#34;&gt;--- DON&#39;T MODIFY THIS LINE ---&lt;/span&gt;
    &lt;p&gt;&lt;a href=&#34;https://blog.csdn.net/cppjava_/article/details/68921869&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LINK1&lt;/a&gt;
&lt;/p&gt;
&lt;h2 id=&#34;深度学习部分&#34;&gt;深度学习部分&lt;/h2&gt;
&lt;h3 id=&#34;densenet&#34;&gt;DenseNet&lt;/h3&gt;
&lt;p&gt;任意的两层之间都存在直接的链接，每一层的输入都是前面的所有层的输出的并集，而该层学习的特征图也会是后面所有层的输入，DenseBlock中需要Feature_map保持一致；&lt;/p&gt;
&lt;p&gt;Block与Block之间的降采样则使用transition layer，使用BN，1*1的 conv，Pooling来降维&lt;/p&gt;
&lt;p&gt;优缺点：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;省参数，省计算&lt;/li&gt;
&lt;li&gt;抗过拟合，泛化能力强&lt;/li&gt;
&lt;li&gt;需要保存Feature占显存&lt;/li&gt;
&lt;/ol&gt;
&lt;!-- more --&gt;
&lt;h3 id=&#34;efficientnet&#34;&gt;EfficientNet&lt;/h3&gt;
&lt;p&gt;针对卷积神经网络的模型拓展，可以通过：1&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;增加网络规模可以、 增加&lt;strong&gt;模型宽度&lt;/strong&gt;，增加&lt;strong&gt;模型深度&lt;/strong&gt;，增加&lt;strong&gt;输入图像的分辨率&lt;/strong&gt;，但是如何去人工调整这个比例和参数，比较麻烦，&lt;/li&gt;
&lt;li&gt;实验发现ConvNet缩放过程中平衡网络宽度、深度、和分辨率的维度是很重要的&lt;/li&gt;
&lt;li&gt;EfficientNet就是对这些的合理组合，复合型的模型扩展技术集合神经结构搜索技术获得&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;
&lt;div class=&#34;post-img-view&#34;&gt;
  &lt;a data-fancybox=&#34;gallery&#34; href=&#34;https://raw.githubusercontent.com/AikenH/md-image/master/img/v2-fcbff6e21eb9e7f9cce94c3bd935b84a_720w2.jpg&#34;&gt;
    &lt;img alt=&#34;img&#34; loading=&#34;lazy&#34; src=&#34;https://raw.githubusercontent.com/AikenH/md-image/master/img/v2-fcbff6e21eb9e7f9cce94c3bd935b84a_720w2.jpg&#34;class=&#34;responsive-image&#34; src=&#34;https://raw.githubusercontent.com/AikenH/md-image/master/img/v2-fcbff6e21eb9e7f9cce94c3bd935b84a_720w2.jpg&#34; style=&#34;display: block; margin: 0 auto;&#34;
      alt=&#34;img&#34;  /&gt;
  &lt;/a&gt;
&lt;/div&gt;


&lt;script&gt;
  document.addEventListener(&#34;DOMContentLoaded&#34;, function() {
      var images = document.querySelectorAll(&#34;.responsive-image&#34;);
      var maxHeight = window.innerHeight / 2.5;
      images.forEach(function(image) {
          image.style.maxHeight = maxHeight + &#34;px&#34;;
      });
  });
&lt;/script&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;关键技术&lt;/strong&gt;复合扩张技术&lt;/p&gt;
&lt;p&gt;所以本文提出了&lt;strong&gt;复合扩张方法&lt;/strong&gt;，这也是文章核心的地方，&lt;/p&gt;
&lt;div&gt;
$$ \alpha,\beta,\gamma $$
&lt;/div&gt;
 是我们需要求解的一组参数，如下图公式，带约束的最优参数求解。 
&lt;div&gt;
$$ \alpha,\beta,\gamma $$
&lt;/div&gt;
 分别衡量着depth, width和 resolution的比重，其中 
&lt;div&gt;
$$ \beta,\gamma $$
&lt;/div&gt;
 在约束上会有平方，是因为如果增加宽度或分辨率两倍，其计算量是增加四倍，但是增加深度两倍，其计算量只会增加两倍。
&lt;p&gt;
&lt;div class=&#34;post-img-view&#34;&gt;
  &lt;a data-fancybox=&#34;gallery&#34; href=&#34;https://raw.githubusercontent.com/AikenH/md-image/master/img/v2-d158f21d6bae78f05fbcf40fd81255d3_720w.jpg&#34;&gt;
    &lt;img alt=&#34;img&#34; loading=&#34;lazy&#34; src=&#34;https://raw.githubusercontent.com/AikenH/md-image/master/img/v2-d158f21d6bae78f05fbcf40fd81255d3_720w.jpg&#34;class=&#34;responsive-image&#34; src=&#34;https://raw.githubusercontent.com/AikenH/md-image/master/img/v2-d158f21d6bae78f05fbcf40fd81255d3_720w.jpg&#34; style=&#34;display: block; margin: 0 auto;&#34;
      alt=&#34;img&#34;  /&gt;
  &lt;/a&gt;
&lt;/div&gt;


&lt;script&gt;
  document.addEventListener(&#34;DOMContentLoaded&#34;, function() {
      var images = document.querySelectorAll(&#34;.responsive-image&#34;);
      var maxHeight = window.innerHeight / 2.5;
      images.forEach(function(image) {
          image.style.maxHeight = maxHeight + &#34;px&#34;;
      });
  });
&lt;/script&gt;
&lt;/p&gt;</description>
    </item>
    <item>
      <title>Involution</title>
      <link>https://hugotest-phi.vercel.app/posts/involution/</link>
      <pubDate>Thu, 08 Apr 2021 13:12:15 +0000</pubDate>
      <guid>https://hugotest-phi.vercel.app/posts/involution/</guid>
      <description>&lt;p&gt;@Aiken 2021-4-8&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/UmumqhZW7Aqk6s8X1Aj7aA&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ariticle &lt;/a&gt;
；&lt;a href=&#34;https://arxiv.org/abs/2103.06255&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt;
；&lt;a href=&#34;https://github.com/d-li14/involution&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;:star:Code；&lt;/a&gt;
 &lt;a href=&#34;https://zhuanlan.zhihu.com/p/357408252&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ZHIHU&lt;/a&gt;
&lt;/p&gt;
&lt;h2 id=&#34;intro-引子&#34;&gt;Intro 引子&lt;/h2&gt;
&lt;p&gt;提出了一种新的神经网络算子（operator或op）称为involution，它比convolution更轻量更高效，形式上比self-attention更加简洁，可以用在各种视觉任务的模型上取得精度和效率的双重提升。&lt;/p&gt;
&lt;p&gt;通过involution的结构设计，我们能够以统一的视角来理解经典的卷积操作和近来流行的自注意力操作。&lt;/p&gt;
&lt;h2 id=&#34;基本思想&#34;&gt;基本思想&lt;/h2&gt;
&lt;p&gt;将传统Convolution Kernel 的两个基本特性：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;**空间不变性：**在同个通道的HW上共享3*3的卷积系数，参数共享；&lt;/li&gt;
&lt;li&gt;**通道特异性：**在每个通道上有特异的卷积核，最终使用1*1 like的方式来进行通道间的整合&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;反对称的修改成：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;空间特异性：&lt;/strong&gt; 对每个Feature有对应size  $H·W·K·K·G | G&amp;laquo;C$  的kernel，特异性的对不同图像的不同部分进行处理
&lt;ul&gt;
&lt;li&gt;G表示Involution操作的分组数，如果需要下采样，就需要接步长为2的平均池化层，最终可以得到，实际上是一个分组卷积的方式，也就是说，我们K个一组的共享一个Kernel。用G去切分C，最终组合起来&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;**通道不变性：**对每个通道之间共享这样的kernel，然后做简单的线性整合，对每个不同的channel有相同的处理方式。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;传统的卷积基于邻域相关性的思想，同时旨在同一个channel中用单一的角度去分析特征，所以有空间不变性核通道特异性的这两个特征。&lt;/p&gt;
&lt;p&gt;而Involution实际上更像是Self-Attention这种思路，通过Whole-Size的Kernel，执行一个特异性处理？&lt;/p&gt;
&lt;h2 id=&#34;要点分析&#34;&gt;要点分析&lt;/h2&gt;
&lt;!-- more --&gt;
&lt;p&gt;这一部分主要介绍一些实现上的技术/理论要点：&lt;/p&gt;
&lt;h3 id=&#34;生成featuremap对应size的kernel&#34;&gt;&lt;strong&gt;生成FeatureMap对应Size的Kernel&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;通用的公式如下，我们可以自定义对应的Kernel生成Function，这是算法的开放性和潜力所在。&lt;/p&gt;
&lt;div&gt;
$$ 
\mathbf{H}_{i,j} = \phi(\mathbf{X}_{\Psi_{i,j}}) \\
\Psi_{i,j} 是邻域的一个index集合，\mathbf{X}_{\Psi_{i,j}}是包含i,j的邻域的一个patch
 $$
&lt;/div&gt;
&lt;p&gt;其中可能会包含一些线性变换和通道缩减之类的变换，一种简单的实例化可以由下图来理解。&lt;/p&gt;
&lt;p&gt;
&lt;div class=&#34;post-img-view&#34;&gt;
  &lt;a data-fancybox=&#34;gallery&#34; href=&#34;https://raw.githubusercontent.com/AikenH/md-image/master/img/image-20210409110945686.png&#34;&gt;
    &lt;img alt=&#34;image-20210409110945686&#34; loading=&#34;lazy&#34; src=&#34;https://raw.githubusercontent.com/AikenH/md-image/master/img/image-20210409110945686.png&#34;class=&#34;responsive-image&#34; src=&#34;https://raw.githubusercontent.com/AikenH/md-image/master/img/image-20210409110945686.png&#34; style=&#34;display: block; margin: 0 auto;&#34;
      alt=&#34;image-20210409110945686&#34;  /&gt;
  &lt;/a&gt;
&lt;/div&gt;


&lt;script&gt;
  document.addEventListener(&#34;DOMContentLoaded&#34;, function() {
      var images = document.querySelectorAll(&#34;.responsive-image&#34;);
      var maxHeight = window.innerHeight / 2.5;
      images.forEach(function(image) {
          image.style.maxHeight = maxHeight + &#34;px&#34;;
      });
  });
&lt;/script&gt;
&lt;/p&gt;</description>
    </item>
    <item>
      <title>Knowledge Evolution</title>
      <link>https://hugotest-phi.vercel.app/posts/knowledgeevolution/</link>
      <pubDate>Wed, 07 Apr 2021 05:34:22 +0000</pubDate>
      <guid>https://hugotest-phi.vercel.app/posts/knowledgeevolution/</guid>
      <description>&lt;h1 id=&#34;knowledge-evolution-in-neural-networks&#34;&gt;Knowledge Evolution in Neural Networks&lt;/h1&gt;
&lt;p&gt;@Aiken 2021.4.7&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/ZdHhdTrHmdcBF4DYf1HXPQ&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Article：只能当成OverView，技术细节写的很差&lt;/a&gt;
；Mendeley；&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/ahmdtaha/knowledge_evolution&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Code_PyTorch&lt;/a&gt;
&lt;/p&gt;
&lt;h2 id=&#34;intro引子&#34;&gt;&lt;strong&gt;Intro引子&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Problem&lt;/strong&gt;：如何在较小的数据集上训练神经网络，这到底是个小样本的方法还是个类别增量的方法？&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Motivation：&lt;/strong&gt; 考虑生物“基因”进化的方式，有一部分是“祖传”，另一部分是“适应”，通过对“祖传”的假设的不断学习进化，得到一个新的模型。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;基因编码了从祖先到后代的遗传信息（知识），而基因传递将遗传信息从父母传递至其后代。虽然祖先并不一定具有更好的知识，但是遗传信息（知识）在几代人之间的发展将会促进后代更好的学习曲线。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Hypothesis：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;拟合假设 $H^{origin}$ ：&lt;/li&gt;
&lt;li&gt;重置假设： $H^{later}$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;TOBEUPDATE：将神经网络拆分成两个假设(子网络)：通过重新训练多代网络来进化 $H^{origin}$ 中的知识，每一代都会扰乱 $H^{later}$ 的内部权重来鼓励 $H^{origin}$ 学习独立的表达形式。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;将深度神经网络的知识封装在一个名为拟合假设的子网络H中，将拟合假设的知识从&lt;strong&gt;父母网络&lt;/strong&gt;传递至其后代，即下一代神经网络。并反复重复此过程，在后代网络中证明了其性能的显著提升：&lt;/p&gt;
&lt;/blockquote&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/AikenH/md-image/master/img/640&#34; alt=&#34;图片&#34; style=&#34;zoom:67%;&#34; /&gt;
&lt;p&gt;&lt;strong&gt;Contribution&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;提出了KELS（内核级卷积感知拆分），为CNN量身定做。虽然增加了训练时间，但是大大降低了推理成本，也减轻了较小数据集中的过拟合问题。&lt;/p&gt;
&lt;!-- more --&gt;
&lt;ul&gt;
&lt;li&gt;提出了KE，提升网络在较小数据集上的性能&lt;/li&gt;
&lt;li&gt;KELS，训练时自动学习slim网络，支持CNN，降低推理成本&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Related Work&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;与两种不同的训练方法作比较&lt;/p&gt;
&lt;p&gt;DSD：在网络结构上与这种dense-sparse-dense&lt;/p&gt;
&lt;h2 id=&#34;理论与实现细节&#34;&gt;理论与实现细节&lt;/h2&gt;
&lt;p&gt;
&lt;div class=&#34;post-img-view&#34;&gt;
  &lt;a data-fancybox=&#34;gallery&#34; href=&#34;https://raw.githubusercontent.com/AikenH/md-image/master/img/image-20210408113607935.png&#34;&gt;
    &lt;img alt=&#34;image-20210408113607935&#34; loading=&#34;lazy&#34; src=&#34;https://raw.githubusercontent.com/AikenH/md-image/master/img/image-20210408113607935.png&#34;class=&#34;responsive-image&#34; src=&#34;https://raw.githubusercontent.com/AikenH/md-image/master/img/image-20210408113607935.png&#34; style=&#34;display: block; margin: 0 auto;&#34;
      alt=&#34;image-20210408113607935&#34;  /&gt;
  &lt;/a&gt;
&lt;/div&gt;


&lt;script&gt;
  document.addEventListener(&#34;DOMContentLoaded&#34;, function() {
      var images = document.querySelectorAll(&#34;.responsive-image&#34;);
      var maxHeight = window.innerHeight / 2.5;
      images.forEach(function(image) {
          image.style.maxHeight = maxHeight + &#34;px&#34;;
      });
  });
&lt;/script&gt;
&lt;/p&gt;</description>
    </item>
    <item>
      <title>Related Word of Emotion</title>
      <link>https://hugotest-phi.vercel.app/posts/facial_expression_and_emotion/</link>
      <pubDate>Wed, 16 Dec 2020 17:08:26 +0000</pubDate>
      <guid>https://hugotest-phi.vercel.app/posts/facial_expression_and_emotion/</guid>
      <description>&lt;h2 id=&#34;疑似&#34;&gt;疑似&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;M. Suwa, N. Sugie and K. Fujimora, &amp;ldquo;A Preliminary Note on Pattern Recognition of Human Emotional Expression&amp;rdquo;, &lt;em&gt;Proc. Int&amp;rsquo;l Joint Conf. Pattern Recognition&lt;/em&gt;, pp. 408-410, 1978.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;K. Scherer and P. Ekman, &lt;em&gt;Handbook of Methods in Nonverbal Behavior Research.&lt;/em&gt;, 1982.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;J.M. Carroll and J. Russell, &amp;ldquo;Facial Expression in Hollywood&amp;rsquo;s Portrayal of Emotion&amp;rdquo;, &lt;em&gt;J. Personality and Social Psychology&lt;/em&gt;, vol. 72, pp. 164-176, 1997.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Standardization and Assessment of College Students&amp;rsquo; Facial Expression of Emotion.&lt;/p&gt;</description>
    </item>
    <item>
      <title>GANs 01</title>
      <link>https://hugotest-phi.vercel.app/posts/gans/</link>
      <pubDate>Mon, 19 Oct 2020 07:47:36 +0000</pubDate>
      <guid>https://hugotest-phi.vercel.app/posts/gans/</guid>
      <description>&lt;h1 id=&#34;fgan-对gan理论的深度理解&#34;&gt;fGAN 对GAN理论的深度理解&lt;/h1&gt;
&lt;p&gt;@Aiken 2021 onenote部分的拓展编写，到时候拷过去，整合在一起。&lt;/p&gt;
&lt;p&gt;fGAN: 不只是JS-Div散度，我们可以&lt;strong&gt;将所有的散度都应用到GANs的框架&lt;/strong&gt;中。该部分的阅读是对GAN的基本理论最重要的文章之一。&lt;/p&gt;
&lt;h2 id=&#34;基本理论体系和推演&#34;&gt;基本理论体系和推演&lt;/h2&gt;
&lt;p&gt;首先给出fGAN中提出的基本理论：可以将所有的Div放入GANs的框架中，来做那个核心的关键演化判别指标：&lt;/p&gt;
&lt;div&gt;
$$ 
D_{f}(P||Q) = \int_xq(x)f(\frac{p(x)}{q(x)}dx)
 $$
&lt;/div&gt;
&lt;p&gt;上述公式将衡量P和Q两个分布之间的差距，公式中的 $f$ 可以是很多不同的版本，但是要求满足如下的两个条件：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;是一个凸函数； $f(\frac{(x1+x2)}{2})\leq \frac{[f(x1)+f(x2)]}{2}$ ，需要注意国内外的凹凸相反&lt;/li&gt;
&lt;li&gt;$f(1)=0$ 。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;而我们知道 $q(x)$ 是概率密度分布函数，实际上可以看成凸函数性质的推广，所以我们可以证得：&lt;/p&gt;
&lt;div&gt;
$$ 
D_{f}(P||Q) = \int_xq(x)f(\frac{p(x)}{q(x)}dx) \geq
f(\int q(x) \frac{p(x)}{q(x)} dx) = f(1) = 0
 $$
&lt;/div&gt;
&lt;p&gt;显然当我们取得合适的f，KL（ $f(x) = xlog(x)$ ）; ReverseKL( $-log(x)$ )；chi square ( $f(x) = (x-1)^2$ )；&lt;/p&gt;
&lt;h3 id=&#34;fenchel-conjugate共轭&#34;&gt;Fenchel Conjugate共轭&lt;/h3&gt;
&lt;p&gt;补充Fenchel共轭的知识来对后续的fGAN推导进行补充，定理内容如下：&lt;/p&gt;
&lt;!-- more --&gt;
&lt;p&gt;每个凸函数都有一个对应的共轭函数读作 $f^*(x)$&lt;/p&gt;
&lt;div&gt;
$$ 
f^*(x) = \max \limits_{x\in dom(f)} xt - f(x)
 $$
&lt;/div&gt;
&lt;p&gt;t是给定的，对于所有的变量t， $xt-f(x)$ 对应了无数条直线：&lt;/p&gt;</description>
    </item>
    <item>
      <title>Hyper-Resolution</title>
      <link>https://hugotest-phi.vercel.app/posts/hyper_resolution/</link>
      <pubDate>Fri, 06 Mar 2020 14:55:02 +0000</pubDate>
      <guid>https://hugotest-phi.vercel.app/posts/hyper_resolution/</guid>
      <description>&lt;p&gt;说明：重点针对&lt;strong&gt;超分辨率&lt;/strong&gt;技术&lt;/p&gt;
&lt;p&gt;备注：
超分辨率在人脸识别上的多，但是表情识别上的确实不多，不过很多都会引用一波&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;超分辨率在表情识别中的应用&#34;&gt;超分辨率在表情识别中的应用&lt;/h2&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;KEY WORDs ：

1. (&amp;#34;super resolution&amp;#34; OR &amp;#34;image restore&amp;#34;) AND (&amp;#34;facial expression recognition&amp;#34; OR &amp;#34;emotion recognition&amp;#34;)   
2. (&amp;#34;super resolution&amp;#34;) AND  (&amp;#34;expression recognition&amp;#34;)   
&lt;/code&gt;&lt;/pre&gt;&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1709.03126.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;lt; Robust Emotion Recognition from Low Quality and Low Bit Rate Video: A Deep Learning Approach &amp;gt;&lt;/a&gt;
&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;针对于低带宽传输的分辨率不足和比率低的应用场景&lt;/li&gt;
&lt;li&gt;基于facial expression recognition 的 emotion recognition&lt;/li&gt;
&lt;li&gt;在解码器进行视频下采样的时候，&lt;strong&gt;联合SR和识别&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://www.sciencedirect.com/science/article/abs/pii/S0925231219312974&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;lt; Effective image super resolution via hierarchical convolutional neural network &amp;gt;&lt;/a&gt;
&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;通过层次卷积神经网络(HCNN)来实现有校的SR&lt;/li&gt;
&lt;li&gt;在facial expression recognition 中案例研究发现增强后的图像有助于提高识别性能&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://link.springer.com/chapter/10.1007/978-3-319-56687-0_13&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;lt; Spatio-temporal Pain Recognition in CNN-Based Super-Resolved Facial Images &amp;gt;&lt;/a&gt;
&lt;/p&gt;</description>
    </item>
    <item>
      <title>Image Caption Dataset</title>
      <link>https://hugotest-phi.vercel.app/posts/imagecaptionrequirement/</link>
      <pubDate>Tue, 14 Jan 2020 02:13:25 +0000</pubDate>
      <guid>https://hugotest-phi.vercel.app/posts/imagecaptionrequirement/</guid>
      <description>&lt;h2 id=&#34;goals&#34;&gt;Goals：&lt;/h2&gt;
&lt;p&gt;1.数据量要求&lt;br&gt;
2.标注的标准&lt;br&gt;
3.标注的手段&lt;/p&gt;
&lt;h2 id=&#34;microsoft-coco-captions&#34;&gt;Microsoft COCO Captions:&lt;/h2&gt;
&lt;p&gt;使用Amazon的Mechanical Turk(AMT)收集数据，再对数据进行标注。&lt;br&gt;
“Each of our captions are also generated using human subjects on AMT.”&lt;/p&gt;
&lt;h3 id=&#34;一些其他信息caption-evaluation-server&#34;&gt;一些其他信息：(Caption Evaluation Server):&lt;/h3&gt;
&lt;p&gt;好像是可以评价caption的生成质量，但是应该是仅仅针对于使用COCO数据进行的，所以这一部分就不分析了。&lt;br&gt;
文中（section 3）包含了几种不同评价方法的介绍：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;BLEU&lt;br&gt;
ROUGE&lt;br&gt;
METEOR&lt;br&gt;
CIDEr&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;在进行Evaluation之前的 Tokenization and preprocessing中：&lt;br&gt;
使用了工具来添加caption标记：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Stanford PTBTokenizer in Stanford CoreNLP tools (version 3.4.1)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这个工具是模仿的是peen treebank3.   其参考文献和相关链接如下：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“The Stanford CoreNLP natural language processing toolkit,” in Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, 2014, pp. 55–60. &lt;a href=&#34;http://www.aclweb.org/anthology/P/P14/P14-5010&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;related-link&lt;/a&gt;
&lt;/p&gt;</description>
    </item>
    <item>
      <title>表情数据集</title>
      <link>https://hugotest-phi.vercel.app/posts/emotion_dataset/</link>
      <pubDate>Mon, 14 Oct 2019 21:34:54 +0000</pubDate>
      <guid>https://hugotest-phi.vercel.app/posts/emotion_dataset/</guid>
      <description>&lt;p&gt;根据这次需要搜集的表情的数据集，整理一下搜索数据集的网站和思路等&lt;/p&gt;
&lt;p&gt;##PART1 &amp;ldquo;表情数据集&amp;rdquo;
下列是对数据搜集的要求：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;是否开源&lt;/li&gt;
&lt;li&gt;图片的大小和数量&lt;/li&gt;
&lt;li&gt;图片的采集方式&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;eg：&lt;strong&gt;ck+&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;==数据来源及对应的搜索结果如下：==&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://toolbox.google.com/datasetsearch&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;谷歌数据集搜索导航&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.kairos.com/blog/60-facial-recognition-databases&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;60个人脸识别的数据集汇总&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;cv方面的好几百个数据集汇总&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;www.cvpapers.com/datasets.html&#34;&gt;另一个cv方向的数据集汇总&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/ChanChiChoi/awesome-Face_Recognition&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;github-CV汇总帖&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;http://cbcsl.ece.ohio-state.edu/EmotionNetChallenge/index.html#overview&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EmotioNet&lt;/a&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;好像是一个什么挑战赛的数据集要博士后或者相应教员才能申请使用&lt;a href=&#34;http://cbcsl.ece.ohio-state.edu/dbform_compound.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;申请页面&lt;/a&gt;
   &lt;br&gt;
没有具体的用于表情识别的数据子集的信息（好像数据很多，但是不知道在哪下，除了那个博士后申请的）&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;http://www.whdeng.cn/RAF/model1.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RAF&lt;/a&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;real-world Affective Face&lt;br&gt;
&lt;strong&gt;数据量&lt;/strong&gt;29672个图像，7种基本情绪，12种复合情绪，（包含种族年龄范围性别属性，5个准确定位和37个自动生成的定位）&lt;br&gt;
**数据收集方式：**来源网络，大小应该很杂 （由40个人独立标定）&lt;br&gt;
email&lt;/p&gt;
&lt;/blockquote&gt;
&lt;!-- more --&gt;
&lt;p&gt;&lt;strong&gt;onenote中标记的和google 数据集搜索&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;http://www.cs.columbia.edu/CAVE/databases/facetracer/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;FaceTracer Database&lt;/a&gt;

basic info：（有图片的原始url）（wild）(网上收集的)姿势、环境、照明、质量 等等参差不齐，&lt;strong&gt;大小不固定&lt;/strong&gt;
(针对&lt;strong&gt;非商业用途开放&lt;/strong&gt;)   (&lt;strong&gt;表情只有笑容&lt;/strong&gt;)
故而不在详细收集，其他的标注信息，文中有详细讲解。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/Tencent/tencent-ml-images?tdsourcetag=s_pctim_aiomsg#tencent-ml-images&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tencent ML-Images&lt;/a&gt;

可能会有表情吧，是一个很大规模的多标签数据集。。。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cvrl.nd.edu/projects/data/#nd-2006-data-set&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ND-2006 Dataset&lt;/a&gt;
 06年貌似
13450张图片
6种基本情感
888个对象&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://ai.google/tools/datasets/google-facial-expression/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google facial expression comparison dataset&lt;/a&gt;

没有对数据集的基本信息介绍&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;百度/CSDN搜索&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.csdn.net/mathlxj/article/details/87920084&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://blog.csdn.net/mathlxj/article/details/87920084&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.csdn.net/computerme/article/details/49469767&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://blog.csdn.net/computerme/article/details/49469767&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://zenodo.org/record/3451524#.XaQ4vm5uKmQ&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;JAFFE&lt;/a&gt;

只有219张，标签为分散离散值。
划分六种情感指标  256*256&lt;/li&gt;
&lt;li&gt;中科大的&lt;a href=&#34;http://nvie.ustc.edu.cn/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NVIE&lt;/a&gt;

其中&lt;strong&gt;正面光照103人，左侧光照99人，右侧光照103人&lt;/strong&gt;。每种光照下，每人有六种表情（&lt;strong&gt;喜悦、愤怒、哀 伤、恐惧、厌恶、惊奇&lt;/strong&gt;）中的三种以上
平静帧、最大帧都已挑出 &lt;a href=&#34;http://nvie.ustc.edu.cn/contact.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;下载协议然后发给他们，才能下载&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://sites.google.com/site/emotiwchallenge/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AFEW database&lt;/a&gt;

&lt;strong&gt;数据来源&lt;/strong&gt;：电影片段的剪辑。&lt;strong&gt;情绪类型&lt;/strong&gt;：“六类基本表情”+中性
&lt;a href=&#34;https://cs.anu.edu.au/few/emotiw2015.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SFEW database&lt;/a&gt;

&lt;strong&gt;数据来源&lt;/strong&gt;：从AFEW中抽取出来的表情的静态帧。标注都在xml中&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://liris-accede.ec-lyon.fr/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LIRIS-ACCEDE database&lt;/a&gt;

同样也是基于电影抽取的，有三种数据集，包含离散的情感数据和基于维度的情感数据&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.cs.binghamton.edu/~lijun/Research/3DFE/3DFE_Analysis.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BU-3DFE database&lt;/a&gt;

3D的人脸表情数据集 **数据来源：**找人来做实验采集，按照要求的情绪做出表情
**数据量：**2500个3d面部模型（来自100个人）
还有同类的一些包含序列的等等的数据集，估计差别不大。
同样需要email获取&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.cse.oulu.fi/CMV/Downloads/Oulu-CASIA&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Oulu-CASIA database&lt;/a&gt;

&lt;strong&gt;数据来源&lt;/strong&gt;：让80个受试者做出相应的表情并用不同相机采集（红外可见光正常光和弱光）
&lt;strong&gt;情绪类型&lt;/strong&gt;：快乐、悲伤、惊讶、愤怒、恐惧、厌恶
email&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.socsci.ru.nl:8180/RaFD2/RaFD&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RAFD&lt;/a&gt;

&lt;strong&gt;数据来源&lt;/strong&gt;：让67个受试者做出相应的表情在不同注视点和不同角度采集
&lt;strong&gt;情绪类型&lt;/strong&gt;：8种情感类型
email&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.emotionlab.se/kdef/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;KDEF database&lt;/a&gt;

&lt;strong&gt;数据来源&lt;/strong&gt;：柔和、均匀的光线，多角度拍摄表情，使用统一的T恤颜色，在拍摄过程中使用网格将参与者面部居中，以及在扫描过程中将眼睛和嘴巴定位在固定的图像坐标中。
&lt;strong&gt;数据量&lt;/strong&gt;：4900张 (70个人，一个7个情感)
页面底端超链接（没进去成功。。）&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://mmlab.ie.cuhk.edu.hk/projects/socialrelation/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ExpW&lt;/a&gt;

9w张左右，图片差不多8G&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://mohammadmahoor.com/affectnet/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AffectNet&lt;/a&gt;

百万量级数据（Emotion Net好像也是）
&lt;strong&gt;获取方式&lt;/strong&gt;：从互联网获取
&lt;strong&gt;7类情感&lt;/strong&gt;，首页有各种情感的数据量，最少的也有4k张
填写申请表email下载&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.flintbox.com/public/project/4742/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Multi-PIE Face Database&lt;/a&gt;

收钱给数据集
&lt;strong&gt;获取方式&lt;/strong&gt;：记录会话
&lt;strong&gt;数据量&lt;/strong&gt;：75w图片&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;一些视频数据集(具体的在CSDN站上)（这些我就没有去详细看了）&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
