<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Machine Learning | aiken's blog</title>
<meta name=keywords content><meta name=description content="Let's learn and innovate together!"><meta name=author content="aikenhong"><link rel=canonical href=https://hugotest-phi.vercel.app/categories/machine-learning/><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><link rel=icon href=https://hugotest-phi.vercel.app/favicon/ghost.ico><link rel=icon type=image/png sizes=16x16 href=https://hugotest-phi.vercel.app/favicon/ghost-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://hugotest-phi.vercel.app/favicon/ghost-32x32.png><link rel=apple-touch-icon href=https://hugotest-phi.vercel.app/favicon/ghost-apple-touch-icon.png><link rel=mask-icon href=https://hugotest-phi.vercel.app/favicon/ghost-192x192.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://hugotest-phi.vercel.app/categories/machine-learning/index.xml><link rel=alternate hreflang=en href=https://hugotest-phi.vercel.app/categories/machine-learning/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script src=https://cdn.jsdmirror.com/npm/jquery@3.5.1/dist/jquery.min.js></script><link rel=stylesheet href=https://cdn.jsdmirror.com/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css><script src=https://cdn.jsdmirror.com/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js></script><link rel=stylesheet href=https://cdn.jsdmirror.com/npm/katex@0.16.11/dist/katex.min.css><script defer src=https://cdn.jsdmirror.com/npm/katex@0.16.11/dist/katex.min.js></script><script defer src=https://cdn.jsdmirror.com/npm/katex@0.16.11/dist/contrib/auto-render.min.js onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><link rel=stylesheet href=https://cdn.jsdmirror.com/npm/lxgw-wenkai-webfont@1.1.0/style.css><link rel=stylesheet href=https://cdn.jsdmirror.com/npm/lxgw-wenkai-lite-webfont@1.1.0/style.css><link rel=stylesheet href=https://cdn.jsdmirror.com/npm/lxgw-wenkai-tc-webfont@1.0.0/style.css><link rel=stylesheet href=https://cdn.jsdmirror.com/npm/lxgw-wenkai-screen-webfont@1.1.0/style.css><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Open+Sans:ital,wght@0,300..800;1,300..800&family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&family=Ubuntu+Mono:ital,wght@0,400;0,700;1,400;1,700&family=Ubuntu:ital,wght@0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><meta property="og:url" content="https://hugotest-phi.vercel.app/categories/machine-learning/"><meta property="og:site_name" content="aiken's blog"><meta property="og:title" content="Machine Learning"><meta property="og:description" content="Let's learn and innovate together!"><meta property="og:locale" content="en-us"><meta property="og:type" content="website"><meta name=twitter:card content="summary"><meta name=twitter:title content="Machine Learning"><meta name=twitter:description content="Let's learn and innovate together!"></head><body class=list id=top><script type=module src=https://cdn.jsdmirror.com/npm/ionicons@7.1.0/dist/ionicons/ionicons.esm.js defer></script><script nomodule src=https://cdn.jsdmirror.com/npm/ionicons@7.1.0/dist/ionicons/ionicons.js defer></script><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://hugotest-phi.vercel.app/ accesskey=h title="aiken's blog (Alt + H)">aiken's blog</a><div class=logo-switches><button id=theme-toggle-nav accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://hugotest-phi.vercel.app/ title=home><span>home</span></a></li><li><a href=https://hugotest-phi.vercel.app/archives/ title=archives><span>archives</span></a></li><li><a href=https://hugotest-phi.vercel.app/search title="search (Alt + /)" accesskey=/><span>search</span></a></li></ul></nav></header><div class=sidebar><ul><li class=logo style=--bg:#333><a href=#><div class=logo-icon><img src=/logo/logo.png></div><div class=logo-text>Aiken's Blog</div></a></li><div class=menulist><li style=--bg:#f44336><a href=https://hugotest-phi.vercel.app/ title=home><div class=logo-icon><ion-icon name=home-outline></ion-icon></div><div class=logo-text>home</div></a></li><li style=--bg:#b145e9><a href=https://hugotest-phi.vercel.app/posts/ title=posts><div class=logo-icon><ion-icon name=newspaper-outline></ion-icon></div><div class=logo-text>posts</div></a></li><li style=--bg:#0f93c7><a href=https://hugotest-phi.vercel.app/tags/ title=tags><div class=logo-icon><ion-icon name=pricetags-outline></ion-icon></div><div class=logo-text>tags</div></a></li><li style=--bg:#ffa117><a href=https://hugotest-phi.vercel.app/categories/ title=categories><div class=logo-icon><ion-icon name=grid-outline></ion-icon></div><div class=logo-text>categories</div></a></li><li style=--bg:#0fc70f><a href=https://hugotest-phi.vercel.app/archives/ title=archives><div class=logo-icon><ion-icon name=folder-outline></ion-icon></div><div class=logo-text>archives</div></a></li><li style=--bg:#d16111><a href=https://hugotest-phi.vercel.app/about/ title=about><div class=logo-icon><ion-icon name=person></ion-icon></div><div class=logo-text>about</div></a></li><li style=--bg:#15c095><a href=https://hugotest-phi.vercel.app/search title="search (Alt + /)" accesskey=/><div class=logo-icon><ion-icon name=search></ion-icon></div><div class=logo-text>search</div></a></li></div><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt +T)"><li><div class=logo-icon id=moon><ion-icon name=moon-outline></ion-icon></div><div class=logo-icon id=sun><ion-icon name=sunny-outline></ion-icon></div></li></button></div></ul></div><link rel=stylesheet href=https://cdn.jsdmirror.com/npm/sakana-widget@2.7.0/lib/sakana.min.css><div id=sakana-widget></div><script>function initSakanaWidget(){const e=SakanaWidget.getCharacter("chisato");e.initialState={...e.initialState,controls:!1,t:.8,i:.002,s:1,d:.999,t:.5,w:.05},e.image="https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/mac/%E5%B7%B2%E7%A7%BB%E9%99%A4%E8%83%8C%E6%99%AF%E7%9A%84Xnip2024-11-30_00-47-03.png",SakanaWidget.registerCharacter("ronSang",e),new SakanaWidget({character:"ronSang"}).mount("#sakana-widget");const t=SakanaWidget.getCharacter("chisato");t.initialState={...t.initialState,controls:!1,t:.8,i:.002,s:1,d:.999,t:.5,w:.05},t.image="https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/mac/%E5%B7%B2%E7%A7%BB%E9%99%A4%E8%83%8C%E6%99%AF%E7%9A%84Xnip2024-11-30_00-36-37.png",SakanaWidget.registerCharacter("xinxin",t),new SakanaWidget({character:"xinxin"}).mount("#sakana-widget")}</script><script async onload=initSakanaWidget() src=https://cdn.jsdmirror.com/npm/sakana-widget@2.7.0/lib/sakana.min.js></script><main class=main><header class=page-header><div class=breadcrumbs><a href=https://hugotest-phi.vercel.app/>Home</a>&nbsp;»&nbsp;<a href=https://hugotest-phi.vercel.app/categories/>Categories</a></div><h1>Machine Learning
<a href=/categories/machine-learning/index.xml title=RSS aria-label=RSS><svg viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" height="23"><path d="M4 11a9 9 0 019 9"/><path d="M4 4a16 16 0 0116 16"/><circle cx="5" cy="19" r="1"/></svg></a></h1></header><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://hugotest-phi.vercel.app/cover/cover11.jpeg alt></figure><div class=post-info><header class=entry-header><h2 class=entry-hint-parent>MLFlow</h2></header><section class=entry-content><p>MLFlow 机器学习系统的使用 @Aiken 2020
基于Python开发的DAG数据工作流系统，面向机器学习，支持Spark并行环境和K8S容器集群；
MLFlow主要解决了三个问题，也就是三个我们可能会需要使用的功能：
Tracking：跟踪实验训练结果，记录算法参数，模型结果和运行效果等等； Projects：对所有的算法项目有一套标准的projects概念，记录下代码版本，参数和运行环境这些东西，并且projects是可以拟合所有的算法框架的； Models：解决的是打包和部署模型的这样一个行为，提供json接口给后续的flsk框架等等进行使用 基本部署 INSTALL：
DEPLOY：
Tracking 实验版本跟踪 Tracking为本次描述的重点，来做一个训练过程中的版本管理，记录每一次训练的参数和变量信息等等，这样便于后续的恢复和实验信息的整理。便于统计和管理。使用的时候好像也是需要代码嵌入的部分，就是需要在代码中调用MLFlow的API。
但是在Tracking的时候有一个比较重要的点在于，这个方法和Tensorboard对原模型的参数的嵌入和Logging记录中会不会产生冲突，同时两个方法之间是不是有什么overlap；关键的问题：
这两个API能不能进行混合使用 怎么统一和区分两个方法的应用情景 Reference https://mlflow.org/docs/latest/tracking.html https://mlflow.org/docs/latest/projects.html https://github.com/mlflow/mlflow https://blog.csdn.net/chenhuipin1173/article/details/100913909 https://my.oschina.net/u/2306127/blog/1825638 https://www.zhihu.com/question/280162556</p></section><footer class=entry-footer><span title='2021-11-28 06:24:19 +0000 UTC'>November 28, 2021</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;25 words&nbsp;·&nbsp;aikenhong&nbsp;·&nbsp;<a href=/tags/mlflow> MLFlow</a>&nbsp;·&nbsp;<a href=/tags/machine-learning> Machine Learning</a></footer><div class=tags style=padding:2px><div style=display:flex;flex-wrap:wrap;justify-content:flex-end;margin-top:5px><a href=/tags/mlflow style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#MLFlow
</a><a href=/tags/machine-learning style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Machine Learning</a></div></div></div><a class=entry-link aria-label="post link to MLFlow" href=https://hugotest-phi.vercel.app/posts/mlflow/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://hugotest-phi.vercel.app/cover/cover12.jpeg alt></figure><div class=post-info><header class=entry-header><h2 class=entry-hint-parent>Hard Task Sampling</h2></header><section class=entry-content><p>Trick：Hard Task 思路来源于Meta-Tranfer-Learning，基本思路是在Meta-Learning的每一次Meta-Test的时候，会从预训练错误率比较高的Task中再次采样，增加那些task的训练次数。也就是难题多做的策略。
基本思路 对比Adaboost 这样的思路其实和AdaBoost的想法是有一定的异曲同工之妙的，或者说其实就是AdaBoost的思路：
Adaboost
参考笔记 ，从该笔记中我们可以看到，AdaBoost的基本思路如下：
Boosting算法的工作机制是首先从训练集用初始权重训练出一个弱学习器1，根据弱学习的学习误差率表现来更新训练样本的权重，使得之前弱学习器1学习误差率高的训练样本点的权重变高，使得这些误差率高的点在后面的弱学习器2中得到更多的重视。然后基于调整权重后的训练集来训练弱学习器2.，如此重复进行，直到弱学习器数达到事先指定的数目T，最终将这T个弱学习器通过集合策略进行整合，得到最终的强学习器.
和Meta-Transfer-Learning对比一下，我们可以发现，这个方法实际上就是讲Transfer Learning的与训练网络当成弱学习器1，然后通过弱学习器1的训练样本权重，来增大Hard-Task的配比（也就是增加任务的权重）完全一致。
具体实现 实现上主要是，样本sample的过程，就是如何在进行参数选择后和原本的Dataloader，结合起来。在这里我们主要参考MTL中的方法，进行网络的构建处理。
第一部分：sampler构建，为了后续Dataloader中进行数据的采样，需要构建一个这样的sampler，关键在于index的对应关系，以及最后输出的是index的集合。
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 import torch import numpy as np # 注意的点，我们需要确定我们batch数目，cls数目和每次每个cls选出多少个数据per # 紧接着定义一个sample，sample输出的是对应原dataset中的数据的index， class CatagoriesSampler(): def __init__(self, label, n_batch, n_cls, n_per): self.n_batch = n_batch self.n_cls = n_cls self.n_per = n_per label = np.array(label) # 根据不同的label输入情况，我们可可能需要找到每个label对应的样本的index，将其整合在一起。如下（option） self.m_idx = [] for i in range(max(label)+1): idx = np.argwhere(label==i).reshape(-1) idx = torch.from_numpy(idx) self.m_idx.append(idx) def __len__(self): # 要注意一下这里数据的长度是根据我们要输出的batch数目决定的 return self.n_batch def __iter__(self): # 直接定义每次采样的时候的batch输出 for i_batch in range(self.n_batch): batch = [] classes = torch.randperm(len(self.m_idx))[:self.n_cls] for c in classes: # 随机选择出的类标签 l = self.m_idx[c] # 随机选择样本 random_pos = torch.randperm(len(l))[:self.n_per] batch.append(l[random_pos]) # stack t and reshape的作用👇 # stack 变成n_cls * n_per , t转置，reshape（-1）变成行向量 batch = torch.stack(batch).t().reshape(-1) yield batch 第二部分：直接调用部分
...</p></section><footer class=entry-footer><span title='2021-11-28 06:24:17 +0000 UTC'>November 28, 2021</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;182 words&nbsp;·&nbsp;aikenhong&nbsp;·&nbsp;<a href=/tags/algorithm> Algorithm</a>&nbsp;·&nbsp;<a href=/tags/sampling> Sampling</a></footer><div class=tags style=padding:2px><div style=display:flex;flex-wrap:wrap;justify-content:flex-end;margin-top:5px><a href=/tags/algorithm style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Algorithm
</a><a href=/tags/sampling style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Sampling</a></div></div></div><a class=entry-link aria-label="post link to Hard Task Sampling" href=https://hugotest-phi.vercel.app/posts/hardtask/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://hugotest-phi.vercel.app/cover/cover7.jpeg alt></figure><div class=post-info><header class=entry-header><h2 class=entry-hint-parent>并行训练</h2></header><section class=entry-content><p>How to Train Really Large Models on Many GPUs? (lilianweng.github.io) 对于浮点运算，模型参数的存储和中间计算输出（梯度和优化器状态）的存储的在 GPU 内存上的大量需求使得我们需要并行化，下面我们参考一些常用的并行化范式：
数据并行策略： 在Multi-GPUs上Duplicate模型，然后分别feed数据，进行运算，每个batch同步或者异步的进行多卡间的梯度传递和模型同步。
同步可能会导致每个batch需要停止训练，异步则是可能会使用陈旧的梯度进行一段时间的训练，增加了计算时间。
而在PT1.5以来，使用一种中间的方式：每隔x次迭代，进行多卡间的全局同步梯度一次，使用这种梯度积累的策略，根据计算图来进行计算和通信调度的优化，提高吞吐量。
模型并行范式： 是为了解决单模型过大无法存储在单一的Node上的问题，但是这样会有GPU间的顺序依赖，虽然减少了内存的占用和计算量，但是这种IO的需求导致计算资源的利用率严重不足。
在这种pipeline中，就存在利用率的bubble，也就是空白的部分
Pipeline并行策略： 混合模型和数据并行的策略，来减少低效时间的泡沫，也就是，将一个batch切分成多个小batch，然后分发到每个node上，减少相应的等待时间，只要我们对计算量和运行速度有合理的把握，就能极大的降低这个inefficient time bubbles. 多个mini-batch的梯度聚集起来最后同步更新. 最有情况下甚至可以忽略气泡的开销
$$ 1- \frac{2md}{(2m+2(d-1))d} = \frac{d-1}{m+d-1}
$$
m个mini-batch和d个分布, bubble的比例将如上述所示
...</p></section><footer class=entry-footer><span title='2021-11-28 02:20:31 +0000 UTC'>November 28, 2021</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;72 words&nbsp;·&nbsp;aikenhong&nbsp;·&nbsp;<a href=/tags/pytorch> Pytorch</a></footer><div class=tags style=padding:2px><div style=display:flex;flex-wrap:wrap;justify-content:flex-end;margin-top:5px><a href=/tags/pytorch style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Pytorch</a></div></div></div><a class=entry-link aria-label="post link to 并行训练" href=https://hugotest-phi.vercel.app/posts/%E5%B9%B6%E8%A1%8C%E8%AE%AD%E7%BB%83%E7%9A%84%E7%90%86%E8%A7%A3/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://hugotest-phi.vercel.app/cover/cover2.jpeg alt></figure><div class=post-info><header class=entry-header><h2 class=entry-hint-parent>MIM-V-simMIM</h2></header><section class=entry-content><p>@Author： MSRA Zhenda Xie @Source：Arxiv ， Code TBP ，Blog_CVer @Read：AikenHong 2021.11.22
“What I cannot create, I do not understand.” — Richard Feynman
Intro & Simple Conclusion Conclusion 继MAE和iBoT之后，MSRA也提出了一个图像掩码建模的新框架，SimMIM，该方法简化了最近这些提出的方法，不需要特殊设计，作者也验证了不需要那些特殊设计就已经能让模型展现出优秀的学习能力
采用中等大小的掩码块（32），对输入图像进行随机掩码，能使其成为强大的代理任务（pretext task） 直接回归预测原始像素的RGB值的效果并不比复杂设计的Patch分类方法差 Projector Head可以是轻量的Linear Layer，效果并不一定比MLP（多层）的差 Motivation 通过这种MIM方法可以实现在大量无标注的数据上得到一个表征能力up的通用特征模型，这种方式的backbone可以广泛的应用到图像上的各种子任务中（按照NLP）的经验来说，而为了类似的方式在图像上的大放异彩，我们首先需要分析Vision和Language的不同
图像有更强的局部关系：相互靠近的像素是高度相关和近似的，我们可以通过简单的copy padding复制一部分缺失 视觉信号是原始，低层次的，而文本分词是高级概念：对低层次信号的预测是否对高层次的视觉识别任务有用呢？ 视觉信号是连续的，而文本的分词是离散的： 如何基于分类的掩码语言建模方法来处理连续的视觉信号 Theoretical Design 掩码选择：同样的掩码的策略还是基于Patch进行的，对于掩码的设计来说，太大的掩码快或者太密集的掩码快，可能会导致找不到附近的像素来预测，实验证明32是一个具有竞争力的size，和文本任务的信息冗余程度不同也带来了覆盖比的选择，NLP通常是0.15，而在V中，32size可以支持0.1-0.7的覆盖率。
任务选择：使用原始像素的回归任务，因为回归任务和具有有序性的视觉信号的连续性很好的吻合。
预测头选择：使用轻量的预测头如（linear），迁移性能与繁琐的预测头相似或者略好，同时训练上更加的块。虽然较大的头或更高的分辨率通常会导致更强的生成能力，但这种更强的能力不一定有利于下游的微调任务。
...</p></section><footer class=entry-footer><span title='2021-11-23 06:38:19 +0000 UTC'>November 23, 2021</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;101 words&nbsp;·&nbsp;aikenhong&nbsp;·&nbsp;<a href=/tags/machine-learning> Machine Learning</a>&nbsp;·&nbsp;<a href=/tags/cv> CV</a></footer><div class=tags style=padding:2px><div style=display:flex;flex-wrap:wrap;justify-content:flex-end;margin-top:5px><a href=/tags/machine-learning style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Machine Learning
</a><a href=/tags/cv style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#CV</a></div></div></div><a class=entry-link aria-label="post link to MIM-V-simMIM" href=https://hugotest-phi.vercel.app/posts/mim-v-simmim/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://hugotest-phi.vercel.app/cover/cover10.jpeg alt></figure><div class=post-info><header class=entry-header><h2 class=entry-hint-parent>Transformer</h2></header><section class=entry-content><p>@aikenhong 2021
References For Transformer:
NLP The Transformer Family (lilianweng.github.io) VIT Transformer眼中世界 VS CNN眼中世界 李沐 NLP Transformer论文精读 Suveys cver1 ， cver2 ，cver3 This blog will divided into several part : lil’s blog, the survey for ViT, we using those article to help us understand the transformer.
综述我们以最新的一篇为准进行阅读，其他的可能后续进行查缺补漏把，如无必要，勿增烦恼。
Intro导言 主要参考文章2来进行我们简单的导入
基本问题 Transformer原本是NLP中的重要模型, 作为LSTM的后继者, 用于处理Seq2Seq的数据类型和情景, 若是要将Transformer运用到Vision的领域中, 首要的问题就是如何:
将Image作为序列化的Token输入Transform中 , 而达成这个目的主要有三种典型的方法:
像素点作为token, 使用VAE离散化图片作为token再输入 ViT: 将图片切为一个个Patch在经过线性的projector之后组成一个embedding表示进行交互 ...</p></section><footer class=entry-footer><span title='2021-11-23 06:38:19 +0000 UTC'>November 23, 2021</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;103 words&nbsp;·&nbsp;aikenhong&nbsp;·&nbsp;<a href=/tags/machine-learning> Machine Learning</a></footer><div class=tags style=padding:2px><div style=display:flex;flex-wrap:wrap;justify-content:flex-end;margin-top:5px><a href=/tags/machine-learning style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Machine Learning</a></div></div></div><a class=entry-link aria-label="post link to Transformer" href=https://hugotest-phi.vercel.app/posts/transformer/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://hugotest-phi.vercel.app/cover/cover15.jpeg alt></figure><div class=post-info><header class=entry-header><h2 class=entry-hint-parent>MIM-V-iBOT</h2></header><section class=entry-content><p>@Read: AikenHong 2021
@Author: https://arxiv.org/abs/2111.07832 @解读：Machine Heart 基本思想 基于NLP中的MLM(Masked Language Model)的核心训练目标: 也就是遮住文本的一部分, 然后通过模型去预测和补全, 这一过程是模型学到泛化的特征, 使用这种方法来进行大规模的与训练范式.
在基本的思想上和MAE采用的是一样的设计, 但是本文中坐着认为visual tokenizer的设计才是其中的关键.
不同于 NLP 中 tokenization 通过离线的词频分析即可将语料编码为含高语义的分词，图像 patch 是连续分布的且存在大量冗余的底层细节信息。而作者认为一个能够提取图像 patch 中高层语义的 tokenizer 可帮助模型避免学习到冗余的这些细节信息。作者认为视觉的 tokenizer 应该具备两个属性：（a）具备完整表征连续图像内容的能力；(b) 像 NLP 中的 tokenizer 一样具备高层语义。
文中对tokenizer的设计为一个知识蒸馏的过程:
文中使用这种在线tokenizer同时来监督这样的MIM过程, 也就是两部分协同学习, 能够较好的保证语义的同时并将图像内容转化为连续的特征分布, 具体的, tokenizer和目标网络狗狗想网络结构, 有移动平均来得到实际的tokenizer.
...</p></section><footer class=entry-footer><span title='2021-11-18 06:35:52 +0000 UTC'>November 18, 2021</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;133 words&nbsp;·&nbsp;aikenhong&nbsp;·&nbsp;<a href=/tags/machine-learning> Machine Learning</a>&nbsp;·&nbsp;<a href=/tags/cv> CV</a></footer><div class=tags style=padding:2px><div style=display:flex;flex-wrap:wrap;justify-content:flex-end;margin-top:5px><a href=/tags/machine-learning style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Machine Learning
</a><a href=/tags/cv style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#CV</a></div></div></div><a class=entry-link aria-label="post link to MIM-V-iBOT" href=https://hugotest-phi.vercel.app/posts/mim-v-ibot/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://hugotest-phi.vercel.app/cover/cover16.jpeg alt></figure><div class=post-info><header class=entry-header><h2 class=entry-hint-parent>MIM-V-MAE</h2></header><section class=entry-content><p>@Author：Facebook AI Research-Kaiming He Kaiming-MAE Conclusion 总而言之这是一种大模型的训练方法, 通过在少量数据的基础上实现大模型的训练.
整体的架构上是参考了NLP中的AutoEncoder机制，将原图切分patch，用mask掩盖原图，通过少量可见的Patch进行Encoder后和Mask融合，再通过非对称的Decoder进行pixel的还原。
这种设计的有点在于mask的scala是可变的，同时这种mask能减少我们训练过程中对显存和计算复杂度的损耗，同时问题本身是一个比较复杂的问题，得以训练复杂的大模型，这种方式最终呈现的效果就是训练的效率高且效益好。
体现了自监督学习在这方面的优越性，同时这种方法得以实现也是由于ViT模型对于CNN模型的取代，才使得这种序列化切块的方式容易实现和验证。
这种方式在最终体现了自监督学习对于有监督与训练的优越性，使用这种方式能够更好的得到一个模型的通用表征。
在这里论文中也说明了和NLP的不同点以及这样的模型对于decoder的要求实际上是比NLP更高的
experiment Masking：对于输入的图像进行均匀的切分并均匀的随机采样
MAE encoder: 简单的ViT模型，对输入图像进行编码后和Mask进行混合得到一个完整的令牌集合，从而确保Decode能够得到对应的位置信息。
MAE decoder：轻量级的架构，可以独立于编码器进行设计，我们使用更窄更浅的网络，计算量比编码器10%更小，这样能够更快的进行训练。解码器的最后一层是先行投影，输出的数量==补丁中像素值的数量，最后会resize层原图的维度。</p></section><footer class=entry-footer><span title='2021-11-15 12:25:45 +0000 UTC'>November 15, 2021</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;20 words&nbsp;·&nbsp;aikenhong&nbsp;·&nbsp;<a href=/tags/machine-learning> Machine Learning</a>&nbsp;·&nbsp;<a href=/tags/cv> CV</a></footer><div class=tags style=padding:2px><div style=display:flex;flex-wrap:wrap;justify-content:flex-end;margin-top:5px><a href=/tags/machine-learning style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Machine Learning
</a><a href=/tags/cv style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#CV</a></div></div></div><a class=entry-link aria-label="post link to MIM-V-MAE" href=https://hugotest-phi.vercel.app/posts/mim-v-mae/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://hugotest-phi.vercel.app/cover/cover15.jpeg alt></figure><div class=post-info><header class=entry-header><h2 class=entry-hint-parent>OWL-survey</h2></header><section class=entry-content><p>@AikenHong2021 OWL
分析现有的OWL特点，和当前自己的研究做一个区分，也汲取一下别人的研究的要点，
Reference arxiv @ self-supervised feature improve open-world learning arxiv @ open-world semi-supervised learning arxiv @ open-world learning without labels arxiv @ unseen class discovery in open-world classification arxiv @ Open-World Active Learning with Stacking Ensemble for Self-Driving Cars www @ open-world learning and application to product classification cvpr @ open world composition zero-shot learning cvpr @ Towards Open World Object Detection [cvpr](Large-Scale Long-Tailed Recognition in an Open World (thecvf.com) ) @ Large-Scale Long-Tailed Recognition in an Open World Conclusion Papers Mulit Open world Learning Definition
...</p></section><footer class=entry-footer><span title='2021-11-12 09:40:46 +0000 UTC'>November 12, 2021</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;180 words&nbsp;·&nbsp;aikenhong&nbsp;·&nbsp;<a href=/tags/machine-learning> Machine Learning</a>&nbsp;·&nbsp;<a href=/tags/open-world-learning> Open World Learning</a>&nbsp;·&nbsp;<a href=/tags/survey> Survey</a>&nbsp;·&nbsp;<a href=/tags/cv> CV</a></footer><div class=tags style=padding:2px><div style=display:flex;flex-wrap:wrap;justify-content:flex-end;margin-top:5px><a href=/tags/machine-learning style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Machine Learning
</a><a href=/tags/open-world-learning style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Open World Learning
</a><a href=/tags/survey style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Survey
</a><a href=/tags/cv style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#CV</a></div></div></div><a class=entry-link aria-label="post link to OWL-survey" href=https://hugotest-phi.vercel.app/posts/owl-survey/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://hugotest-phi.vercel.app/cover/cover24.jpeg alt></figure><div class=post-info><header class=entry-header><h2 class=entry-hint-parent>SS_OD_SoftTeacher</h2></header><section class=entry-content><p>@ Article: ICML from Microsoft & Huazhong Keda @ Code: Github @ Noteby: Aikenhong @ Time: 20210914
Abstrast and Intro in the session we will using describe the main idea of this article.
这篇文章的重点在于Soft Teacher，也就是用pseudo label做为弱标注，逐步提高伪标签的可靠性。
不同于多阶段的方法，端到端的方法再训练中逐步的提升伪标签的质量从而再去benifit目标检测的质量。 这样E2E的框架主要依赖于两部分技术:
soft teacher: 每个未标记边界框的分类损失由教师网络产生的分类分数进行加权 box jitter 窗口抖动: 选择可靠的伪框来学习框回归 在目标检测上获得SOTA的效果;
Multi-Stage 在半监督的情况下，关注的主要是基于伪标签的方法，是目前的SOTA，以往的方法采用多阶段的方式。
使用标记数据训练初始检测器 未标记数据的伪标记，同时基于伪标签进行重新训练 局限：初始少量标注的局限，初始的检测器的伪标签质量
End to End Soft Teacher基本思路：对未标记的图像进行标记，然后通过标记的几个伪标签训练检测器.
具体而言：
采样标注和未标注图片形成Batch 双模型：检测（student）、标记（teacher） EMA：T模型是S模型的EMA 这种方式避免了多阶段方案实现上的复杂，同时实现了飞轮效应==S、T相互加强;
此外Soft Teacher直接对学生模型生成的所有候选框进行评估，而不是使用伪框来为这些候选框进行分类回归。 这样能使用更多的直接监督信息
具体而言：
使用高阈值来分割前景，确保不会错误的将背景分类成前景，确保正伪标签的高精度； 使用可靠性度量来加权背景候选的损失； 教师模型产生的检测分数可以很好的作为可靠性度量 Box Jitter为了更可靠的训练学生网络的本地分支，指的是：
...</p></section><footer class=entry-footer><span title='2021-10-09 02:30:08 +0000 UTC'>October 9, 2021</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;211 words&nbsp;·&nbsp;aikenhong&nbsp;·&nbsp;<a href=/tags/machine-learning> Machine Learning</a>&nbsp;·&nbsp;<a href=/tags/semi-supervised-learning> Semi-Supervised Learning</a></footer><div class=tags style=padding:2px><div style=display:flex;flex-wrap:wrap;justify-content:flex-end;margin-top:5px><a href=/tags/machine-learning style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Machine Learning
</a><a href=/tags/semi-supervised-learning style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Semi-Supervised Learning</a></div></div></div><a class=entry-link aria-label="post link to SS_OD_SoftTeacher" href=https://hugotest-phi.vercel.app/posts/ss_od_softteacher/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://hugotest-phi.vercel.app/cover/cover18.jpeg alt></figure><div class=post-info><header class=entry-header><h2 class=entry-hint-parent>StyleGAN</h2></header><section class=entry-content><p>StyleGAN V1 @AikenHong 2020 10.8
《A Style-Based Generator Architecture for Generative Adversarial Networks》
Related Work： 继承的文献工作： ProGAN 参考解读：
《其中子链接值得一看》 （包括源码解析啥的）（甚至还有GAN的笔记） 《StyleGan源码解析和拓展应用》 《秃头生成器1》 《秃头生成器2》 NO.3 Contribution（Problem）：
解纠缠：Mapping Network Noise Generator AdaIN before all conv Structure： ...</p></section><footer class=entry-footer><span title='2021-10-03 13:16:40 +0000 UTC'>October 3, 2021</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;93 words&nbsp;·&nbsp;aikenhong&nbsp;·&nbsp;<a href=/tags/gan> GAN</a>&nbsp;·&nbsp;<a href=/tags/machine-learning> Machine Learning</a></footer><div class=tags style=padding:2px><div style=display:flex;flex-wrap:wrap;justify-content:flex-end;margin-top:5px><a href=/tags/gan style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#GAN
</a><a href=/tags/machine-learning style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Machine Learning</a></div></div></div><a class=entry-link aria-label="post link to StyleGAN" href=https://hugotest-phi.vercel.app/posts/stylegan/></a></article><footer class=page-footer><nav class=pagination><a class=last-icon href=https://hugotest-phi.vercel.app/categories/machine-learning/><span>&lt;&lt;</span>
</a><a class=prev href=https://hugotest-phi.vercel.app/categories/machine-learning/page/2/>«&nbsp;Prev&nbsp;2/5
</a><a class=next href=https://hugotest-phi.vercel.app/categories/machine-learning/page/4/>Next&nbsp;4/5&nbsp;»
</a><a class=last-icon href=https://hugotest-phi.vercel.app/categories/machine-learning/page/5/><span>>></span></a></nav></footer></main><footer class=footer><span>&copy; 2024 <a href=https://hugotest-phi.vercel.app/>aiken's blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a>
</span><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><span id=busuanzi_container>Visitors: <span id=busuanzi_value_site_uv></span>
Views: <span id=busuanzi_value_site_pv></span></span></footer><script>document.addEventListener("DOMContentLoaded",function(){const e=document.getElementById("busuanzi_value_site_uv"),t=document.getElementById("busuanzi_value_site_pv"),o=13863,i=16993;if(!e||!t){console.error("Busuanzi elements not found.");return}const n=new MutationObserver(e=>{for(let t of e)if(t.type==="childList"){n.disconnect(),t.target.innerHTML=parseInt(t.target.innerHTML||0)+o;break}}),s=new MutationObserver(e=>{for(let t of e)if(t.type==="childList"){s.disconnect(),t.target.innerHTML=parseInt(t.target.innerHTML||0)+i;break}});n.observe(e,{childList:!0}),s.observe(t,{childList:!0})})</script><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><span class=topInner><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
<span id=read_progress></span>
</span></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))}),document.getElementById("theme-toggle-nav").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.addEventListener("scroll",function(){const t=document.getElementById("read_progress"),n=document.documentElement.scrollHeight,s=document.documentElement.clientHeight,o=document.documentElement.scrollTop||document.body.scrollTop;t.innerText=((o/(n-s)).toFixed(2)*100).toFixed(0)})</script><script>(function(e,t){var s=document,o="script",n=s.createElement(o),i=s.getElementsByTagName(o)[0];n.src=e,t&&n.addEventListener("load",function(e){t(e)}),i.parentNode.insertBefore(n,i)})("/js/pangu.js",function(){pangu.spacingPage()})</script></body></html>