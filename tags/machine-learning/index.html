<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Machine Learning | aiken's blog</title>
<meta name=keywords content><meta name=description content="Let's learn and innovate together!"><meta name=author content="aikenhong"><link rel=canonical href=https://hugotest-phi.vercel.app/tags/machine-learning/><link crossorigin=anonymous href=/assets/css/stylesheet.2f85ca17c12c62fa86b1e474b8a51aca4856f0d645debfe4922a4d5ddc6aa978.css integrity="sha256-L4XKF8EsYvqGseR0uKUaykhW8NZF3r/kkipNXdxqqXg=" rel="preload stylesheet" as=style><link rel=icon href=https://hugotest-phi.vercel.app/favicon/ghost.ico><link rel=icon type=image/png sizes=16x16 href=https://hugotest-phi.vercel.app/favicon/ghost-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://hugotest-phi.vercel.app/favicon/ghost-32x32.png><link rel=apple-touch-icon href=https://hugotest-phi.vercel.app/favicon/ghost-apple-touch-icon.png><link rel=mask-icon href=https://hugotest-phi.vercel.app/favicon/ghost-192x192.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://hugotest-phi.vercel.app/tags/machine-learning/index.xml><link rel=alternate hreflang=en href=https://hugotest-phi.vercel.app/tags/machine-learning/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script src=https://cdn.jsdmirror.com/npm/jquery@3.5.1/dist/jquery.min.js></script><link rel=stylesheet href=https://cdn.jsdmirror.com/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css><script src=https://cdn.jsdmirror.com/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js></script><link rel=stylesheet href=https://cdn.jsdmirror.com/npm/katex@0.16.11/dist/katex.min.css><script defer src=https://cdn.jsdmirror.com/npm/katex@0.16.11/dist/katex.min.js></script><script defer src=https://cdn.jsdmirror.com/npm/katex@0.16.11/dist/contrib/auto-render.min.js onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><link rel=stylesheet href=https://cdn.jsdmirror.com/npm/lxgw-wenkai-webfont@1.1.0/style.css><link rel=stylesheet href=https://cdn.jsdmirror.com/npm/lxgw-wenkai-lite-webfont@1.1.0/style.css><link rel=stylesheet href=https://cdn.jsdmirror.com/npm/lxgw-wenkai-tc-webfont@1.0.0/style.css><link rel=stylesheet href=https://cdn.jsdmirror.com/npm/lxgw-wenkai-screen-webfont@1.1.0/style.css><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Open+Sans:ital,wght@0,300..800;1,300..800&family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&family=Ubuntu+Mono:ital,wght@0,400;0,700;1,400;1,700&family=Ubuntu:ital,wght@0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><meta property="og:url" content="https://hugotest-phi.vercel.app/tags/machine-learning/"><meta property="og:site_name" content="aiken's blog"><meta property="og:title" content="Machine Learning"><meta property="og:description" content="Let's learn and innovate together!"><meta property="og:locale" content="en-us"><meta property="og:type" content="website"><meta name=twitter:card content="summary"><meta name=twitter:title content="Machine Learning"><meta name=twitter:description content="Let's learn and innovate together!"></head><body class=list id=top><script type=module src=https://cdn.jsdmirror.com/npm/ionicons@7.1.0/dist/ionicons/ionicons.esm.js defer></script><script nomodule src=https://cdn.jsdmirror.com/npm/ionicons@7.1.0/dist/ionicons/ionicons.js defer></script><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://hugotest-phi.vercel.app/ accesskey=h title="aiken's blog (Alt + H)">aiken's blog</a><div class=logo-switches><button id=theme-toggle-nav accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://hugotest-phi.vercel.app/ title=home><span>home</span></a></li><li><a href=https://hugotest-phi.vercel.app/archives/ title=archives><span>archives</span></a></li><li><a href=https://hugotest-phi.vercel.app/search title="search (Alt + /)" accesskey=/><span>search</span></a></li></ul></nav></header><div class=sidebar><ul><li class=logo style=--bg:#333><a href=#><div class=logo-icon><img src=/logo/logo.png></div><div class=logo-text>Aiken's Blog</div></a></li><div class=menulist><li style=--bg:#f44336><a href=https://hugotest-phi.vercel.app/ title=home><div class=logo-icon><ion-icon name=home-outline></ion-icon></div><div class=logo-text>home</div></a></li><li style=--bg:#b145e9><a href=https://hugotest-phi.vercel.app/posts/ title=posts><div class=logo-icon><ion-icon name=newspaper-outline></ion-icon></div><div class=logo-text>posts</div></a></li><li style=--bg:#0f93c7><a href=https://hugotest-phi.vercel.app/tags/ title=tags><div class=logo-icon><ion-icon name=pricetags-outline></ion-icon></div><div class=logo-text>tags</div></a></li><li style=--bg:#ffa117><a href=https://hugotest-phi.vercel.app/categories/ title=categories><div class=logo-icon><ion-icon name=grid-outline></ion-icon></div><div class=logo-text>categories</div></a></li><li style=--bg:#0fc70f><a href=https://hugotest-phi.vercel.app/archives/ title=archives><div class=logo-icon><ion-icon name=folder-outline></ion-icon></div><div class=logo-text>archives</div></a></li><li style=--bg:#d16111><a href=https://hugotest-phi.vercel.app/about/ title=about><div class=logo-icon><ion-icon name=person></ion-icon></div><div class=logo-text>about</div></a></li><li style=--bg:#15c095><a href=https://hugotest-phi.vercel.app/search title="search (Alt + /)" accesskey=/><div class=logo-icon><ion-icon name=search></ion-icon></div><div class=logo-text>search</div></a></li></div><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt +T)"><li><div class=logo-icon id=moon><ion-icon name=moon-outline></ion-icon></div><div class=logo-icon id=sun><ion-icon name=sunny-outline></ion-icon></div></li></button></div></ul></div><link rel=stylesheet href=https://cdn.jsdmirror.com/npm/sakana-widget@2.7.0/lib/sakana.min.css><div id=sakana-widget></div><script>function initSakanaWidget(){const e=SakanaWidget.getCharacter("chisato");e.initialState={...e.initialState,controls:!1,t:.8,i:.002,s:1,d:.999,t:.5,w:.05},e.image="https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/mac/%E5%B7%B2%E7%A7%BB%E9%99%A4%E8%83%8C%E6%99%AF%E7%9A%84Xnip2024-11-30_00-47-03.png",SakanaWidget.registerCharacter("ronSang",e),new SakanaWidget({character:"ronSang"}).mount("#sakana-widget");const t=SakanaWidget.getCharacter("chisato");t.initialState={...t.initialState,controls:!1,t:.8,i:.002,s:1,d:.999,t:.5,w:.05},t.image="https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/mac/%E5%B7%B2%E7%A7%BB%E9%99%A4%E8%83%8C%E6%99%AF%E7%9A%84Xnip2024-11-30_00-36-37.png",SakanaWidget.registerCharacter("xinxin",t),new SakanaWidget({character:"xinxin"}).mount("#sakana-widget")}</script><script async onload=initSakanaWidget() src=https://cdn.jsdmirror.com/npm/sakana-widget@2.7.0/lib/sakana.min.js></script><main class=main><header class=page-header><div class=breadcrumbs><a href=https://hugotest-phi.vercel.app/>Home</a>&nbsp;»&nbsp;<a href=https://hugotest-phi.vercel.app/tags/>Tags</a></div><h1>Machine Learning
<a href=/tags/machine-learning/index.xml title=RSS aria-label=RSS><svg viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" height="23"><path d="M4 11a9 9 0 019 9"/><path d="M4 4a16 16 0 0116 16"/><circle cx="5" cy="19" r="1"/></svg></a></h1></header><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://hugotest-phi.vercel.app/cover/cover10.jpeg alt></figure><div class=post-info><header class=entry-header><h2 class=entry-hint-parent>Fine Tuning</h2></header><section class=entry-content><p>@Langs: python, torch @reference: d2l-pytorch，transfer_torch This Note focus on the code part. 模型微调和模型预训练，在Pytorch中的使用方式对比汇总。
How to Design the Fine Tune 这一部分主要集中于我们对于微调任务的拆解，有几种不同的预训练和微调的方式，在不同的情景下，对应的参数应该怎么设置和调整是问题的重点。
基于这种Transfer的策略，我们能够学习到一个更通用，泛化能力更强，有助于识别边缘，色彩，等等有助于下游任务的通用特征提取。
在Transfer任务中，有几种不同的调整方式：
固定Bakcbone，只训练Classifier 同步微调网络 区分学习率，微调Backbone，训练Classifirer 为了实现这几种不同的Transfer方式，需要用到以下的几种方式：梯度截断，lr区分设置等。
Code Part 不同lr设置 微调Backbone，训练Classifier作为最经典的Transfer设定，在Code上也较为复杂，所以我们首先举个这种例子。
相关的文档可以参考：torch.optim 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # get dataset train_img = torchvision.datasets.ImageFolder(os.path.join(data_dir, 'train')) # get new model pretrained_new = model.expand_dim(dim=out_dim,re_init=True) # pre train it 定义一个用于微调的函数 # pytorch可以通过字典的形式来区分对设置lr def train_fine_tuning(net, learning_rate, batch_size=128, num_epoch=5, diff_lr=True): # set dataloader train_iter = torch.utils.Dataloader(train_img, batch_size=batch_size, shuffle=True) test_iter = ... # set loss loss = nn.CrossEntropyLoss(reduction='none') # set diff lr for diff part of it if diff_lr: params_1x = [param for name, param in net.name_parameters() if name not in ["fc.weight", "fc.bias"]] trainer = torch.optim.SGD([{'params': params_1x}, {'params': net.fc.parameters(), 'lr': learning_rate *10}], lr=learning_rate, weight_decay=0.001 ) else: trainer = torch.optim.SGD(net.parameters(), lr=learning_rate, weight_decay=0.001) 同时不用担心，scheduler可以将我们的两组lr同时进行更新，可以基于下面的代码进行测试
...</p></section><footer class=entry-footer><span title='2022-02-08 14:31:37 +0000 UTC'>February 8, 2022</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;281 words&nbsp;·&nbsp;aikenhong&nbsp;·&nbsp;<a href=/tags/pytorch> Pytorch</a>&nbsp;·&nbsp;<a href=/tags/fine-tune> Fine-Tune</a>&nbsp;·&nbsp;<a href=/tags/machine-learning> Machine Learning</a></footer><div class=tags style=padding:2px><div style=display:flex;flex-wrap:wrap;justify-content:flex-end;margin-top:5px><a href=/tags/pytorch style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Pytorch
</a><a href=/tags/fine-tune style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Fine-Tune
</a><a href=/tags/machine-learning style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Machine Learning</a></div></div></div><a class=entry-link aria-label="post link to Fine Tuning" href=https://hugotest-phi.vercel.app/posts/finetune/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://hugotest-phi.vercel.app/cover/cover9.jpeg alt></figure><div class=post-info><header class=entry-header><h2 class=entry-hint-parent>IL Collection</h2></header><section class=entry-content><p>@AikenHong 2022
[[Draft/IL 总结]]: Thx 2 wyz to provide some clus for learnning Incremental Learning.
In this Doc, we may add some related knowledge distill works which is used to design our Incremental Structure. 在这个文档中，我们可能还会添加一些知识蒸馏的相关工作的文献，这些实际上对于我的增量学习架构有一个比较大的启发
DER SPPR 没有 get 到方法到底是怎么做的 Introduction 👿 在很多视觉应用中，需要在保留旧知识的基础上学习新知识，==举个例子==，理想的情况是，我们可以保留之前学习的参数，而不发生==灾难性遗忘==，或者我们基于之前的数据进行协同训练，灾难性遗忘是 IL 中最核心的问题。
Incremental 的基本过程可以表示如下[4]： ...</p></section><footer class=entry-footer><span title='2022-01-04 01:38:04 +0000 UTC'>January 4, 2022</span>&nbsp;·&nbsp;5 min&nbsp;·&nbsp;984 words&nbsp;·&nbsp;aikenhong&nbsp;·&nbsp;<a href=/tags/incremental-learning> Incremental Learning</a>&nbsp;·&nbsp;<a href=/tags/machine-learning> Machine Learning</a>&nbsp;·&nbsp;<a href=/tags/survey> Survey</a></footer><div class=tags style=padding:2px><div style=display:flex;flex-wrap:wrap;justify-content:flex-end;margin-top:5px><a href=/tags/incremental-learning style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Incremental Learning
</a><a href=/tags/machine-learning style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Machine Learning
</a><a href=/tags/survey style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Survey</a></div></div></div><a class=entry-link aria-label="post link to IL Collection" href=https://hugotest-phi.vercel.app/posts/il-collection/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://hugotest-phi.vercel.app/cover/cover2.jpeg alt></figure><div class=post-info><header class=entry-header><h2 class=entry-hint-parent>WYZ-IL-Collection</h2></header><section class=entry-content><p>: hammer: 王耀智
Regularization 系列方法 这类方法旨在添加一些正则化损失来解决 catastrophic forgetting 的问题。
Weight Regularization 这类方法一般是对网络中每个参数的重要性进行评估，根据每个参数的重要性和梯度信息更新参数。
典型的文章为 EWC .
PS: 这类文章我也没有读过。
Data Regularization 这类方法专注于记住特征表示，通常是结合 Hinton 的知识蒸馏损失函数使得模型记住旧类别的知识，解决 catastrophic forgetting。
推荐以下几篇文章：
LwF(Learning without forgetting)，这篇文章在我看来是增量学习的开山之作，第一次给增量学习找到了一个比较好的方向，也是第一次将知识蒸馏应用到增量学习上； PODNet CVPR2020 ，这篇文章最大的贡献在我看来是设计了一个全新的蒸馏损失函数，最终结果也是达到了当时的sota，甚至目前也是几个榜单的sota。 Rehearsal 系列方法 这类方法主要的想法是使用一些旧类别的数据，在新类别到来时使用新旧数据一起训练模型，根据旧类别数据的真假分为以下两种方法。
Pseudo rehearsal 这类方法通常是在学习旧类别的同时，训练一个生成模型，可以生成旧的类别数据，在新类别数据到来时，生成相当数量的旧类别数据，一起训练新模型。
这里推荐一篇文章：Continual learning with deep generative replay。
PS：这个小类别的论文我也没有太关注，个人不是很推荐这类方法。
Save real data 这类方法是开辟一个内存空间，空间中保存旧类别的少部分训练数据，在新类别到来时，使用内存空间的数据与新数据共同学习，按照对空间的使用方法不同可分为：
Exemplar Rehearsal 这类方法是将新旧数据混合，共同作为训练数据，一起训练模型，使得模型能够保持旧类别的知识。
但是在训练过程中新旧数据的类别数量是不均衡的，这也催生了我下面会说到的一大类解决方法。
这种方法推荐的论文是 iCaRL，这篇论文是 exemplar rehearsal 的开山之作，第一次提出了内存空间这个概念，也提出了一个非常有效的内存选择策略(herb)，并且也是第一个使用特征作为分类依据的方法，我个人认为是继 LwF 之后又一个将 IL 推到一个新的高度的方法。
Gradient Rectification 这类方法我称之为 Gradient Rectification，其主要思路是模型每次更新的梯度由 shared gradient 和 task-specific gradient 组成。分别代表所有类别的共性信息和某一个类别的特性信息，在新类别学习时借助内存空间中的数据获得旧类别的两项梯度，在更新时对梯度进行修正，力求做到不增加共享梯度代表的损失，尽量减少类别特定梯度代表的损失。
...</p></section><footer class=entry-footer><span title='2022-01-03 10:41:56 +0000 UTC'>January 3, 2022</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;154 words&nbsp;·&nbsp;aikenhong&nbsp;·&nbsp;<a href=/tags/machine-learning> Machine Learning</a>&nbsp;·&nbsp;<a href=/tags/incremental-learning> Incremental Learning</a>&nbsp;·&nbsp;<a href=/tags/survey> Survey</a></footer><div class=tags style=padding:2px><div style=display:flex;flex-wrap:wrap;justify-content:flex-end;margin-top:5px><a href=/tags/machine-learning style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Machine Learning
</a><a href=/tags/incremental-learning style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Incremental Learning
</a><a href=/tags/survey style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Survey</a></div></div></div><a class=entry-link aria-label="post link to WYZ-IL-Collection" href=https://hugotest-phi.vercel.app/posts/il-wyz/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://hugotest-phi.vercel.app/cover/cover0.jpeg alt></figure><div class=post-info><header class=entry-header><h2 class=entry-hint-parent>LT Collection</h2></header><section class=entry-content><p>LT-Collections @AikenHong 2021
Code of must of those methods We will analysis those tricks on LT situation, and Analysis why it works. 在进行LT矫正的任务中，有几种常见的trick在各种模型中被使用，我们会对这几种不同的trick进行介绍和分析。
其实在数据量少这一方面LT和Few-Shot是有一定的OverLap的,可以参考以下那边的思路perhaps
Introduction 通常情况下这种严重的类别不平衡问题会使得模型严重过拟合于头部，而在尾部欠拟合
首先介绍 bag of tricks 这篇论文中总结了一些常用的Trick，并组合出了最佳的一套trick
经过该文实验总结，Trick组合应该是[1]`：
在前几个epoch应用input mixup数据增强，然后后面fine-tuning; (基于CAM的)重采样来重新训练分类器; 实际上就是MixUp + Two-Stage的策略，后续对Mix-up这个策略带来的作用要进行补充了解一下
Rebalance 对于ReBalance的方法，实际上就是从 data和 update两个角度来缓解Unbalance本身，通过从数据量上达到重新均衡，或者基于Loss使得bp过程中赋予Tail更高的权重来达到优化过程的平衡。
...</p></section><footer class=entry-footer><span title='2021-12-22 14:36:16 +0000 UTC'>December 22, 2021</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;690 words&nbsp;·&nbsp;aikenhong&nbsp;·&nbsp;<a href=/tags/machine-learning> Machine Learning</a>&nbsp;·&nbsp;<a href=/tags/survey> Survey</a>&nbsp;·&nbsp;<a href=/tags/long-tailed-learning> Long-Tailed Learning</a></footer><div class=tags style=padding:2px><div style=display:flex;flex-wrap:wrap;justify-content:flex-end;margin-top:5px><a href=/tags/machine-learning style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Machine Learning
</a><a href=/tags/survey style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Survey
</a><a href=/tags/long-tailed-learning style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Long-Tailed Learning</a></div></div></div><a class=entry-link aria-label="post link to LT Collection" href=https://hugotest-phi.vercel.app/posts/lt-collection/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://hugotest-phi.vercel.app/cover/cover8.jpeg alt></figure><div class=post-info><header class=entry-header><h2 class=entry-hint-parent>Loss-NCE</h2></header><section class=entry-content><p>@AikenHong 2021
Noise Contrastive Estimation Loss = NCE Loss 噪声对比估计损失，这里的Noise实际上就是Negative Samples. 该损失被广泛的用于对比学习的任务，而对比学习广泛的作为自监督学习的无监督子任务用来训练一个良好的特征提取器，于是对于对比学习的目标和效用的理解十分关键。
What’s NCE Loss 在介绍NCE之前我们可以将其和CE进行一个简单的对比，虽然名称上不是同一个CE，但是在数学表达上却有很相近的地方（softmax-kind of loss）
首先softmax，他保证所有的值加起来为一，结合onehot的ce，实际上j==gt的情况下外层+log也就是ceLoss，也就是 $logSoftmax$
$$ S_j = \frac{e^{a_j}}{\sum_{k=1}^N e^{a_k}}
$$
然后看infoNCE，基础的对比学习损失可以写成：
$$ L_{contrast} = \mathbb{E}[-\log\frac{e^{f_x^T f_y/T}}{e^{f_x^T f_y/T} + \sum_i e^{f_x^T f_{y_-^i}/T}}]
$$
其中 $f_x^T f_y^T$ 为 $sim(x,y)$ 时即转化为带 $T$ 的NCE，即InforNCE.
分子是正例对的相似度，分母是正例对+所有负例对的相似度，最小化infoNCE loss，就是去最大化分子的同时最小化分母，也就是最大化正例对的相似度，最小化负例对的相似度。
从该形式上看，和soft的CE形式上是统一的，当我们把分母看作概率和自身以及和其他的相似性，这样和NCE在形式上和简化后的CE实现了统一。
但是我不认为这和label smooth 后的CE有相关性，而是和原始的CE经由One-hot简化后结构上有相似性。
How it Works NCE的思想是拉近相似的样本，推开不相近的样本，从而学习到一个好的语义表示空间，这一点上实际上和度量学习的思想是一样的，只是对比学习通常作用在无监督或者自监督的语境中，度量学习这是有监督的。
考虑之前人脸匹配的研究，使用 “Alignment and Uniformity on the Hypersphere"中的Alignment and Uniformity，就是一个更好理解他的角度
$$ \begin{gathered}
L_{\text {contrast }}=\mathbb{E}\left[-\log \frac{e^{f_{x}^{T} f_{y} / \tau}}{e^{f_{x}^{T} f_{y} / \tau}+\sum_{i} e^{T_{x}^{T} f_{y_{i}}^{-} / \tau}}\right] \\
=\mathbb{E}\left[-f_{x}^{T} f_{y} / \tau\right]+\mathbb{E}\left[\log \left(e^{f_{x}^{T} f_{y} / \tau}+\sum_{i} e^{f_{x}^{T} f_{y_{i}^{-} / \tau}}\right)\right] \\
\mathbb{P}\left[\left(f_{x}=f_{y}\right)\right]=1 \underbrace{\mathbb{E}\left[-f_{x}^{T} f_{y} / \tau\right]}_{\text {positive alignment }}+\underbrace{\mathbb{E}\left[\log \left(e^{1 / \tau}+\sum_{i} e^{f_{x}^{T} f_{y_{i}}-/ \tau}\right)\right]}_{\text {uniformity }}
\end{gathered}
$$
公式经过上面的推导就可以看成下的两个部分，其中alignment只与positive pair有关，相反Uniformity只与negative pair相关，希望所有的点都能尽可能的分布在uni hypersphere上。
...</p></section><footer class=entry-footer><span title='2021-12-22 13:39:55 +0000 UTC'>December 22, 2021</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;655 words&nbsp;·&nbsp;aikenhong&nbsp;·&nbsp;<a href=/tags/machine-learning> Machine Learning</a>&nbsp;·&nbsp;<a href=/tags/loss> Loss</a></footer><div class=tags style=padding:2px><div style=display:flex;flex-wrap:wrap;justify-content:flex-end;margin-top:5px><a href=/tags/machine-learning style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Machine Learning
</a><a href=/tags/loss style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Loss</a></div></div></div><a class=entry-link aria-label="post link to Loss-NCE" href=https://hugotest-phi.vercel.app/posts/loss-nce/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://hugotest-phi.vercel.app/cover/cover10.jpeg alt></figure><div class=post-info><header class=entry-header><h2 class=entry-hint-parent>Loss-Smooth(Sharpen)</h2></header><section class=entry-content><p>@AikenHong 2021 @topic
smooth label (inception v2) when does label smoothing help (nips 2019) sharpen in semi-supervised in the future offical code github 不是一个通用的方法，在很多的任务上反而会导致掉点的现象，可以简单分析一下，汲取一下思想和Sharpen做对比，在这篇文章中，我们可以结合之前的人脸对比损失来进行分析。
What’s the smooth label 首先介绍在图像分类任务中对logits和Hard label做ce得到我们的损失，可以表现为如下的形式：
$$ Loss = -\sum^{K}_{i=1}p_i \log(q_i)
$$
由于我们的标签是一个hard label，实际上可以转化成一个one-hot，即
$$ \begin{equation}
p_i = \left\{
\begin{array}{c1}
1 & i==gt \\
0 & i!=gt \\
\end{array} \right.
\end{equation}
$$
而soft label实际上做的是将 1的位置变为 $1-\alpha$ ，其他位置设置为 $\alpha/(K-1)$ ，然后再去求CE，
Hinton论文中给出该损失对特征分布的作用测试图： ...</p></section><footer class=entry-footer><span title='2021-12-17 03:35:27 +0000 UTC'>December 17, 2021</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;144 words&nbsp;·&nbsp;aikenhong&nbsp;·&nbsp;<a href=/tags/machine-learning> Machine Learning</a>&nbsp;·&nbsp;<a href=/tags/loss> Loss</a></footer><div class=tags style=padding:2px><div style=display:flex;flex-wrap:wrap;justify-content:flex-end;margin-top:5px><a href=/tags/machine-learning style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Machine Learning
</a><a href=/tags/loss style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Loss</a></div></div></div><a class=entry-link aria-label="post link to Loss-Smooth(Sharpen)" href=https://hugotest-phi.vercel.app/posts/loss-smoothsharpen/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://hugotest-phi.vercel.app/cover/cover10.jpeg alt></figure><div class=post-info><header class=entry-header><h2 class=entry-hint-parent>Training Strategy</h2></header><section class=entry-content><p>@Aiken 2020，
主要针对神经网络的训练过程中的一些基础策略的调整，比如当训练的曲线出现一定的问题的时候，我们应该怎么去调整我们训练过程中的策略。
参数调整过程中最重要的就是优化器（优化或者说是下降算法）和学习率（优化算法的核心参数），此外像是数据增强策略还是Normalization策略，都能极大的影响一个模型的好坏。
优化器 Some Material 实际上虽然有很多的优化算法，但是到最后最常用的还是 SGD+Mon 和 Adam两种：
Adam主要的有事在于自适应学习率，他对我们设计的学习率实际上没有那么敏感，但是在具体实验中往往不会有调的好的SGD那么好，只是在SGD的参数调整中会比较费劲。
但是有了根据patient调整lr的scheduler后，我们基本上可以使用SGD做一个较为简单的调整，只要设计好初始的lr的实验以及用来调整学习率的参数值。
学习率 $\omega^{n} \leftarrow \omega^{n}-\eta \frac{\partial L}{\partial \omega^{n}}$ 其中的权重就是学习率lr，
==Basic==
学习率大 学习率小 学习速度 快 慢 使用情景 刚开始训练时 一定的次数过后 副作用 1. Loss爆炸 2.振荡 1.过拟合 2.收敛速度慢 学习率的基本设置 在训练过程中，一般根据训练轮数设置动态变化的学习率。
刚开始训练时：学习率以 0.01 ~ 0.001 为宜。 一定轮数过后：逐渐减缓。 接近训练结束：学习速率的衰减应该在100倍以上。 Note： 如果是 迁移学习 ，由于模型已在原始数据上收敛，此时应设置较小学习率 (≤10−4) 在新数据上进行 微调 。
学习率变化方法 ==warm up==
warm up为什么有用 warm up衰减策略与上述的策略有些不同，它是先从一个极低的学习率开始增加，增加到某一个值后再逐渐减少, 这点上倒是和Cosine Anneal LR有一定的相似之处，将这两种结合起来是一种常见的训练策略：
这样训练模型更加稳定，因为在刚开始时模型的参数都是随机初始化的，此时如果学习率应该取小一点，这样就不会使模型一下子跑偏。
而这样的跑偏对于大模型而言，可能是导致很严重的影响，后面收敛了也可能不会达到最佳的效果，一开始的跑偏，可能会造成准确率在后面的严重结果。 ...</p></section><footer class=entry-footer><span title='2021-12-16 08:34:44 +0000 UTC'>December 16, 2021</span>&nbsp;·&nbsp;5 min&nbsp;·&nbsp;1045 words&nbsp;·&nbsp;aikenhong&nbsp;·&nbsp;<a href=/tags/machine-learning> Machine Learning</a>&nbsp;·&nbsp;<a href=/tags/pytorch> Pytorch</a>&nbsp;·&nbsp;<a href=/tags/acceleration> Acceleration</a></footer><div class=tags style=padding:2px><div style=display:flex;flex-wrap:wrap;justify-content:flex-end;margin-top:5px><a href=/tags/machine-learning style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Machine Learning
</a><a href=/tags/pytorch style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Pytorch
</a><a href=/tags/acceleration style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Acceleration</a></div></div></div><a class=entry-link aria-label="post link to Training Strategy" href=https://hugotest-phi.vercel.app/posts/nerualnetworktraining/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://hugotest-phi.vercel.app/cover/cover0.jpeg alt></figure><div class=post-info><header class=entry-header><h2 class=entry-hint-parent>Loss-WhyZero</h2></header><section class=entry-content><p>Loss :Why Zero Loss？ @Comments: ICML2020 《Do We Need Zero Training Loss After Achieving Zero Training Error》
@Noteby：AikenHong2021
如何解决训练损失下降，但是验证损失上升的问题（过拟合like）的问题，该文章实际上可以作为我们损失设计中的一个trick，只需要简单的一行代码，来提升代码的泛化能力；
这张图体现了本文的灵魂（思路），主要体现在我们在算法趋于稳定后继续训练可能验证损失会反而上升；
所以本文提出了一种flooding方法，当我们training loss 大于阈值的时候我们使其正常下降，当低于阈值的时候，flooding的设计会反过来使得梯度上升，让训练损失保持在flooding附近，让模型持续进行random walk，希望模型最终能优化到一个平坦的损失区域，这样发现test loss进一步的进行下降。
理解：
当我们的训练损失低到一定的程度，然后随着lr的下降，模型会很难跳出当前的极小值，这种情况下我们的泛化能力也会被限制住，采用这种方法在牺牲测试精度的同时能提升算法的泛化能力。
损失公式表示如下
$$ \widetilde{J}(\theta) = |J(\theta) - b| +b
$$
...</p></section><footer class=entry-footer><span title='2021-12-10 08:24:46 +0000 UTC'>December 10, 2021</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;66 words&nbsp;·&nbsp;aikenhong&nbsp;·&nbsp;<a href=/tags/machine-learning> Machine Learning</a>&nbsp;·&nbsp;<a href=/tags/loss> Loss</a></footer><div class=tags style=padding:2px><div style=display:flex;flex-wrap:wrap;justify-content:flex-end;margin-top:5px><a href=/tags/machine-learning style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Machine Learning
</a><a href=/tags/loss style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Loss</a></div></div></div><a class=entry-link aria-label="post link to Loss-WhyZero" href=https://hugotest-phi.vercel.app/posts/loss-whyzero/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://hugotest-phi.vercel.app/cover/cover16.jpeg alt></figure><div class=post-info><header class=entry-header><h2 class=entry-hint-parent>UniFramework 01</h2></header><section class=entry-content><p>文章的部分内容被密码保护：
--- DON'T MODIFY THIS LINE ---
@aiken 2021 Framework
Abstract Try To make structure universal，编写一个自己的通用的架构，框架化，满足通过不同的model文件和特殊配置文件就能实现不同的模型的一个架构。
只是一个初步的框架集成，还有很多没有完善的地方，目前测试了ResNet18 跑Cifar10，没有什么问题，如果有什么可以改进的地方，或者你实现了一些Feature，*欢迎进行交流*！（私下联系我最好啦！）
感谢帮助
还有一些可以参数化或者可视化的地方，由于时间关系目前还没有修改，有兴趣的可以自己先添加一下
暂时只集成了分类的模块，后续可能会随缘扩展
本框架主要希望实现的是：易读性，可拓展性，以及简洁；
希望将重要的，可变的参数都尽量的分离出来，通过配置文件和命令行参数去定义和运行我们的网络，在这种情况下实现一个较好的工作流程。
Final Project Design PURPOSE：新类发现和模型自主更新；同时希望能够解决长尾分布的数据情景；
**ANALYSIS：**为了实现这种模型的自主更新过程，将整体的流程分成两个部分
启动（start）： self supervissed 等方法无监督的学习特征提取网络（这种方式是否会对Unbalance产生增益）
初始化预测模型： 基于Unbalance的数据训练一个基础的分类模型，在输出分类结果的同时需要输出对应的预测置信度，这两个其实都是一些简单的Trick，而最重要的是Backbone的分类效果需要得到保证，同时Backbone需要支撑后续的模型蒸馏更新。 模型的自主更新和迭代： Online：在线运行推断模型，通过置信度输出筛选出新类样本，将样本在样本池中收集 Offline：基于样本池的规模和评估触发离线更新：伪标签生成模型；模型蒸馏和更新 创新点：自主新类发现和学习
...</p></section><footer class=entry-footer><span title='2021-12-04 01:43:30 +0000 UTC'>December 4, 2021</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;526 words&nbsp;·&nbsp;aikenhong&nbsp;·&nbsp;<a href=/tags/cv> CV</a>&nbsp;·&nbsp;<a href=/tags/machine-learning> Machine Learning</a></footer><div class=tags style=padding:2px><div style=display:flex;flex-wrap:wrap;justify-content:flex-end;margin-top:5px><a href=/tags/cv style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#CV
</a><a href=/tags/machine-learning style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Machine Learning</a></div></div></div><a class=entry-link aria-label="post link to UniFramework 01" href=https://hugotest-phi.vercel.app/posts/uniframework/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://hugotest-phi.vercel.app/cover/cover11.jpeg alt></figure><div class=post-info><header class=entry-header><h2 class=entry-hint-parent>FSL前期调研</h2></header><section class=entry-content><p>文章的部分内容被密码保护：
--- DON'T MODIFY THIS LINE ---
主要是limited labels & Few Samples & Data programing Weakly supervised learning
semi-supervised in video field
if we can recoding this work?
多指标下降（LOSS的耦合或者循环的选择）、相关的CV最新论文等等会在后续关注
元学习、浅层神经网络的概念等等 semi-supervised
PART1 Limited Labels （base on LiFeiFei‘s reference） in this part we may list the paper which is useful for my recoding.
还有一些其他重要的可能在对论文进行重新精读的时候要记得注意reference：就比如说在loss变换和决策树生成那一块。
distant supervision(it’s kind of early) can be another baseline for our method, we need to understand how this method work for that situation
distant supervisor到底是什么机制可以去CSDN什么的看一下
...</p></section><footer class=entry-footer><span title='2021-11-29 13:12:05 +0000 UTC'>November 29, 2021</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;549 words&nbsp;·&nbsp;aikenhong&nbsp;·&nbsp;<a href=/tags/machine-learning> Machine Learning</a>&nbsp;·&nbsp;<a href=/tags/survey> Survey</a>&nbsp;·&nbsp;<a href=/tags/fsl> FSL</a></footer><div class=tags style=padding:2px><div style=display:flex;flex-wrap:wrap;justify-content:flex-end;margin-top:5px><a href=/tags/machine-learning style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Machine Learning
</a><a href=/tags/survey style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Survey
</a><a href=/tags/fsl style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#FSL</a></div></div></div><a class=entry-link aria-label="post link to FSL前期调研" href=https://hugotest-phi.vercel.app/posts/fsl%E5%89%8D%E6%9C%9F%E8%B0%83%E7%A0%94/></a></article><footer class=page-footer><nav class=pagination><a class=next href=https://hugotest-phi.vercel.app/tags/machine-learning/page/2/>Next&nbsp;2/4&nbsp;»
</a><a class=last-icon href=https://hugotest-phi.vercel.app/tags/machine-learning/page/4/><span>>></span></a></nav></footer></main><footer class=footer><span>&copy; 2024 <a href=https://hugotest-phi.vercel.app/>aiken's blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a>
</span><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><span id=busuanzi_container>Visitors: <span id=busuanzi_value_site_uv></span>
Views: <span id=busuanzi_value_site_pv></span></span></footer><script>document.addEventListener("DOMContentLoaded",function(){const e=document.getElementById("busuanzi_value_site_uv"),t=document.getElementById("busuanzi_value_site_pv"),o=13863,i=16993;if(!e||!t){console.error("Busuanzi elements not found.");return}const n=new MutationObserver(e=>{for(let t of e)if(t.type==="childList"){n.disconnect(),t.target.innerHTML=parseInt(t.target.innerHTML||0)+o;break}}),s=new MutationObserver(e=>{for(let t of e)if(t.type==="childList"){s.disconnect(),t.target.innerHTML=parseInt(t.target.innerHTML||0)+i;break}});n.observe(e,{childList:!0}),s.observe(t,{childList:!0})})</script><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><span class=topInner><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
<span id=read_progress></span>
</span></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))}),document.getElementById("theme-toggle-nav").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.addEventListener("scroll",function(){const t=document.getElementById("read_progress"),n=document.documentElement.scrollHeight,s=document.documentElement.clientHeight,o=document.documentElement.scrollTop||document.body.scrollTop;t.innerText=((o/(n-s)).toFixed(2)*100).toFixed(0)})</script><script>(function(e,t){var s=document,o="script",n=s.createElement(o),i=s.getElementsByTagName(o)[0];n.src=e,t&&n.addEventListener("load",function(e){t(e)}),i.parentNode.insertBefore(n,i)})("/js/pangu.js",function(){pangu.spacingPage()})</script></body></html>