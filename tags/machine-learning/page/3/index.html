<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Machine Learning | aiken's blog</title>
<meta name=keywords content><meta name=description content="Let's learn and innovate together!"><meta name=author content="aikenhong"><link rel=canonical href=https://hugotest-phi.vercel.app/tags/machine-learning/><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><link rel=icon href=https://hugotest-phi.vercel.app/favicon/ghost.ico><link rel=icon type=image/png sizes=16x16 href=https://hugotest-phi.vercel.app/favicon/ghost-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://hugotest-phi.vercel.app/favicon/ghost-32x32.png><link rel=apple-touch-icon href=https://hugotest-phi.vercel.app/favicon/ghost-apple-touch-icon.png><link rel=mask-icon href=https://hugotest-phi.vercel.app/favicon/ghost-192x192.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://hugotest-phi.vercel.app/tags/machine-learning/index.xml><link rel=alternate hreflang=en href=https://hugotest-phi.vercel.app/tags/machine-learning/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script src=https://cdn.jsdmirror.com/npm/jquery@3.5.1/dist/jquery.min.js></script><link rel=stylesheet href=https://cdn.jsdmirror.com/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css><script src=https://cdn.jsdmirror.com/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js></script><link rel=stylesheet href=https://cdn.jsdmirror.com/npm/katex@0.16.11/dist/katex.min.css><script defer src=https://cdn.jsdmirror.com/npm/katex@0.16.11/dist/katex.min.js></script><script defer src=https://cdn.jsdmirror.com/npm/katex@0.16.11/dist/contrib/auto-render.min.js onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><link rel=stylesheet href=https://cdn.jsdmirror.com/npm/lxgw-wenkai-webfont@1.1.0/style.css><link rel=stylesheet href=https://cdn.jsdmirror.com/npm/lxgw-wenkai-lite-webfont@1.1.0/style.css><link rel=stylesheet href=https://cdn.jsdmirror.com/npm/lxgw-wenkai-tc-webfont@1.0.0/style.css><link rel=stylesheet href=https://cdn.jsdmirror.com/npm/lxgw-wenkai-screen-webfont@1.1.0/style.css><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Open+Sans:ital,wght@0,300..800;1,300..800&family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&family=Ubuntu+Mono:ital,wght@0,400;0,700;1,400;1,700&family=Ubuntu:ital,wght@0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><meta property="og:url" content="https://hugotest-phi.vercel.app/tags/machine-learning/"><meta property="og:site_name" content="aiken's blog"><meta property="og:title" content="Machine Learning"><meta property="og:description" content="Let's learn and innovate together!"><meta property="og:locale" content="en-us"><meta property="og:type" content="website"><meta name=twitter:card content="summary"><meta name=twitter:title content="Machine Learning"><meta name=twitter:description content="Let's learn and innovate together!"></head><body class=list id=top><script type=module src=https://cdn.jsdmirror.com/npm/ionicons@7.1.0/dist/ionicons/ionicons.esm.js defer></script><script nomodule src=https://cdn.jsdmirror.com/npm/ionicons@7.1.0/dist/ionicons/ionicons.js defer></script><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://hugotest-phi.vercel.app/ accesskey=h title="aiken's blog (Alt + H)">aiken's blog</a><div class=logo-switches><button id=theme-toggle-nav accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://hugotest-phi.vercel.app/ title=home><span>home</span></a></li><li><a href=https://hugotest-phi.vercel.app/archives/ title=archives><span>archives</span></a></li><li><a href=https://hugotest-phi.vercel.app/search title="search (Alt + /)" accesskey=/><span>search</span></a></li></ul></nav></header><div class=sidebar><ul><li class=logo style=--bg:#333><a href=#><div class=logo-icon><img src=/logo/logo.png></div><div class=logo-text>Aiken's Blog</div></a></li><div class=menulist><li style=--bg:#f44336><a href=https://hugotest-phi.vercel.app/ title=home><div class=logo-icon><ion-icon name=home-outline></ion-icon></div><div class=logo-text>home</div></a></li><li style=--bg:#b145e9><a href=https://hugotest-phi.vercel.app/posts/ title=posts><div class=logo-icon><ion-icon name=newspaper-outline></ion-icon></div><div class=logo-text>posts</div></a></li><li style=--bg:#0f93c7><a href=https://hugotest-phi.vercel.app/tags/ title=tags><div class=logo-icon><ion-icon name=pricetags-outline></ion-icon></div><div class=logo-text>tags</div></a></li><li style=--bg:#ffa117><a href=https://hugotest-phi.vercel.app/categories/ title=categories><div class=logo-icon><ion-icon name=grid-outline></ion-icon></div><div class=logo-text>categories</div></a></li><li style=--bg:#0fc70f><a href=https://hugotest-phi.vercel.app/archives/ title=archives><div class=logo-icon><ion-icon name=folder-outline></ion-icon></div><div class=logo-text>archives</div></a></li><li style=--bg:#d16111><a href=https://hugotest-phi.vercel.app/about/ title=about><div class=logo-icon><ion-icon name=person></ion-icon></div><div class=logo-text>about</div></a></li><li style=--bg:#15c095><a href=https://hugotest-phi.vercel.app/search title="search (Alt + /)" accesskey=/><div class=logo-icon><ion-icon name=search></ion-icon></div><div class=logo-text>search</div></a></li></div><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt +T)"><li><div class=logo-icon id=moon><ion-icon name=moon-outline></ion-icon></div><div class=logo-icon id=sun><ion-icon name=sunny-outline></ion-icon></div></li></button></div></ul></div><link rel=stylesheet href=https://cdn.jsdmirror.com/npm/sakana-widget@2.7.0/lib/sakana.min.css><div id=sakana-widget></div><script>function initSakanaWidget(){const e=SakanaWidget.getCharacter("chisato");e.initialState={...e.initialState,controls:!1,t:.8,i:.002,s:1,d:.999,t:.5,w:.05},e.image="https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/mac/%E5%B7%B2%E7%A7%BB%E9%99%A4%E8%83%8C%E6%99%AF%E7%9A%84Xnip2024-11-30_00-47-03.png",SakanaWidget.registerCharacter("ronSang",e),new SakanaWidget({character:"ronSang"}).mount("#sakana-widget");const t=SakanaWidget.getCharacter("chisato");t.initialState={...t.initialState,controls:!1,t:.8,i:.002,s:1,d:.999,t:.5,w:.05},t.image="https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/mac/%E5%B7%B2%E7%A7%BB%E9%99%A4%E8%83%8C%E6%99%AF%E7%9A%84Xnip2024-11-30_00-36-37.png",SakanaWidget.registerCharacter("xinxin",t),new SakanaWidget({character:"xinxin"}).mount("#sakana-widget")}</script><script async onload=initSakanaWidget() src=https://cdn.jsdmirror.com/npm/sakana-widget@2.7.0/lib/sakana.min.js></script><main class=main><header class=page-header><div class=breadcrumbs><a href=https://hugotest-phi.vercel.app/>Home</a>&nbsp;»&nbsp;<a href=https://hugotest-phi.vercel.app/tags/>Tags</a></div><h1>Machine Learning
<a href=/tags/machine-learning/index.xml title=RSS aria-label=RSS><svg viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" height="23"><path d="M4 11a9 9 0 019 9"/><path d="M4 4a16 16 0 0116 16"/><circle cx="5" cy="19" r="1"/></svg></a></h1></header><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://hugotest-phi.vercel.app/cover/cover24.jpeg alt></figure><div class=post-info><header class=entry-header><h2 class=entry-hint-parent>SS_OD_SoftTeacher</h2></header><section class=entry-content><p>@ Article: ICML from Microsoft & Huazhong Keda @ Code: Github @ Noteby: Aikenhong @ Time: 20210914
Abstrast and Intro in the session we will using describe the main idea of this article.
这篇文章的重点在于Soft Teacher，也就是用pseudo label做为弱标注，逐步提高伪标签的可靠性。
不同于多阶段的方法，端到端的方法再训练中逐步的提升伪标签的质量从而再去benifit目标检测的质量。 这样E2E的框架主要依赖于两部分技术:
soft teacher: 每个未标记边界框的分类损失由教师网络产生的分类分数进行加权 box jitter 窗口抖动: 选择可靠的伪框来学习框回归 在目标检测上获得SOTA的效果;
Multi-Stage 在半监督的情况下，关注的主要是基于伪标签的方法，是目前的SOTA，以往的方法采用多阶段的方式。
使用标记数据训练初始检测器 未标记数据的伪标记，同时基于伪标签进行重新训练 局限：初始少量标注的局限，初始的检测器的伪标签质量
End to End Soft Teacher基本思路：对未标记的图像进行标记，然后通过标记的几个伪标签训练检测器.
具体而言：
采样标注和未标注图片形成Batch 双模型：检测（student）、标记（teacher） EMA：T模型是S模型的EMA 这种方式避免了多阶段方案实现上的复杂，同时实现了飞轮效应==S、T相互加强;
此外Soft Teacher直接对学生模型生成的所有候选框进行评估，而不是使用伪框来为这些候选框进行分类回归。 这样能使用更多的直接监督信息
具体而言：
使用高阈值来分割前景，确保不会错误的将背景分类成前景，确保正伪标签的高精度； 使用可靠性度量来加权背景候选的损失； 教师模型产生的检测分数可以很好的作为可靠性度量 Box Jitter为了更可靠的训练学生网络的本地分支，指的是：
...</p></section><footer class=entry-footer><span title='2021-10-09 02:30:08 +0000 UTC'>October 9, 2021</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;211 words&nbsp;·&nbsp;aikenhong&nbsp;·&nbsp;<a href=/tags/machine-learning> Machine Learning</a>&nbsp;·&nbsp;<a href=/tags/semi-supervised-learning> Semi-Supervised Learning</a></footer><div class=tags style=padding:2px><div style=display:flex;flex-wrap:wrap;justify-content:flex-end;margin-top:5px><a href=/tags/machine-learning style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Machine Learning
</a><a href=/tags/semi-supervised-learning style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Semi-Supervised Learning</a></div></div></div><a class=entry-link aria-label="post link to SS_OD_SoftTeacher" href=https://hugotest-phi.vercel.app/posts/ss_od_softteacher/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://hugotest-phi.vercel.app/cover/cover18.jpeg alt></figure><div class=post-info><header class=entry-header><h2 class=entry-hint-parent>StyleGAN</h2></header><section class=entry-content><p>StyleGAN V1 @AikenHong 2020 10.8
《A Style-Based Generator Architecture for Generative Adversarial Networks》
Related Work： 继承的文献工作： ProGAN 参考解读：
《其中子链接值得一看》 （包括源码解析啥的）（甚至还有GAN的笔记） 《StyleGan源码解析和拓展应用》 《秃头生成器1》 《秃头生成器2》 NO.3 Contribution（Problem）：
解纠缠：Mapping Network Noise Generator AdaIN before all conv Structure： ...</p></section><footer class=entry-footer><span title='2021-10-03 13:16:40 +0000 UTC'>October 3, 2021</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;93 words&nbsp;·&nbsp;aikenhong&nbsp;·&nbsp;<a href=/tags/gan> GAN</a>&nbsp;·&nbsp;<a href=/tags/machine-learning> Machine Learning</a></footer><div class=tags style=padding:2px><div style=display:flex;flex-wrap:wrap;justify-content:flex-end;margin-top:5px><a href=/tags/gan style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#GAN
</a><a href=/tags/machine-learning style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Machine Learning</a></div></div></div><a class=entry-link aria-label="post link to StyleGAN" href=https://hugotest-phi.vercel.app/posts/stylegan/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://hugotest-phi.vercel.app/cover/cover16.jpeg alt></figure><div class=post-info><header class=entry-header><h2 class=entry-hint-parent>OW Object Detector</h2></header><section class=entry-content><p>@Aiken 2021
框架撞车系列，主要看看这一篇论文中怎么解决如下的问题👇，并从中借鉴和优化的我框架设计
思路分析 Motivation 模型实现的主要的两个TASK：
Open Set Learning ： 在没有明确监督的时候，将尚未引入的目标类别识别为未知 Incremental Learning：类别增量学习 实现这两个问题的主要思路：
自动标注：借鉴RPN的class-agnostic，以及检测和分类的显著性指标的差异，找到并自动标注NewClass **对比聚类：**使用prototype feature来进行聚类，同时计算Distance损失 it seems like contain a unknown prototype. **energy based：**亥姆霍兹自由能公式？ ENERGY BASED Feature： $F$ , Label: $L$ , Energy: $E(F,l)$
能量函数倾向于将已知的类别分类到低熵的分布上，然后我们可以根据特征在能量空间上的划分来区分新类和旧类。然后我们可以根据logits表达的softmax形式，找到输出和Gibbs distribution的相关性：
$$ p(l \mid \boldsymbol{f})=\frac{\exp \left(\frac{g_{l}(\boldsymbol{f})}{T}\right)}{\sum_{i=1}^{\mathrm{C}} \exp \left(\frac{g_{i}(\boldsymbol{f})}{T}\right)}=\frac{\exp \left(-\frac{E(\boldsymbol{f}, l)}{T}\right)}{\exp \left(-\frac{E(\boldsymbol{f})}{T}\right)}
$$
通过这个相关性，我们对自由能进行一个定义，以logits的形式表达
...</p></section><footer class=entry-footer><span title='2021-09-28 13:44:20 +0000 UTC'>September 28, 2021</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;71 words&nbsp;·&nbsp;aikenhong&nbsp;·&nbsp;<a href=/tags/machine-learning> Machine Learning</a>&nbsp;·&nbsp;<a href=/tags/open-world-learning> Open World Learning</a>&nbsp;·&nbsp;<a href=/tags/cv> CV</a></footer><div class=tags style=padding:2px><div style=display:flex;flex-wrap:wrap;justify-content:flex-end;margin-top:5px><a href=/tags/machine-learning style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Machine Learning
</a><a href=/tags/open-world-learning style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Open World Learning
</a><a href=/tags/cv style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#CV</a></div></div></div><a class=entry-link aria-label="post link to OW Object Detector" href=https://hugotest-phi.vercel.app/posts/ow-od/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://hugotest-phi.vercel.app/cover/cover5.jpeg alt></figure><div class=post-info><header class=entry-header><h2 class=entry-hint-parent>Attention Mechanism</h2></header><section class=entry-content><p>@Aiken 2020.9.16
对基本注意力机制的一些资料和理解做一些简单的汇总，着重分析基本思想原理，应用和实现（即 structure），还有一些Weakness和相应的解决方案。
1.TODO-List：
根据Lil’Log的Attention？Attention！进行初步的整理 各个分类的具体含义分开整理，理解一部分整理一部分，可能结合实际的应用去整理吧。 其中很重要的一点是数学分析的部分，需要对数学原理进行整理和领会。 What’s Attention In Deep Learning 在某种程度上，注意力是由我么如何关注视觉图像的不同区域或者我们如何关联同一个句子中的不同单词所启发的：针对于问题的不同，我们会对图像的某些具体的区域重视（某些区域在视觉中呈现高分辨率，而另一些则是低分辨率的情况），或者句子中的某些词重视的情况。
可以解释一个句子中紧密的上下文单词之间的关系，比如我们看到eating就会期待看到food，而color对于我们来说就没有那么重要。
...</p></section><footer class=entry-footer><span title='2021-09-28 05:34:22 +0000 UTC'>September 28, 2021</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;332 words&nbsp;·&nbsp;aikenhong&nbsp;·&nbsp;<a href=/tags/machine-learning> Machine Learning</a>&nbsp;·&nbsp;<a href=/tags/attention> Attention</a></footer><div class=tags style=padding:2px><div style=display:flex;flex-wrap:wrap;justify-content:flex-end;margin-top:5px><a href=/tags/machine-learning style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Machine Learning
</a><a href=/tags/attention style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Attention</a></div></div></div><a class=entry-link aria-label="post link to Attention Mechanism" href=https://hugotest-phi.vercel.app/posts/attention/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://hugotest-phi.vercel.app/cover/cover15.jpeg alt></figure><div class=post-info><header class=entry-header><h2 class=entry-hint-parent>EfficientNet</h2></header><section class=entry-content><p>Tags: Paper URL1: https://arxiv.org/pdf/1905.11946.pdf URL2: https://arxiv.org/pdf/2104.00298.pdf 提出了一种模型缩放策略，如何更高效的平衡网络的深度、宽度、和图片分辨率 1. Efficient Net: Rethinking Model Scaling for Convolutional Neural Networks 2. EfficientNetV2: Smaller Models and Faster Training
@Aiken H 2021 find detail to code his
Efficient Net V1 除了提出了缩放策略以外，还使用神经架构搜索还建立了一个新的baseline network，得到了一系列模型。
平衡网络宽度、深度、分辨率至关重要，这种平衡可以通过简单的恒定比率缩放维度来实现，于是我们提出了一种简单有效的复合缩放方法。
复合缩放的物理意义：输入图像更大的话就需要更多层来增加感受野和更多通道，从而能在更大的图像上捕获更多细粒度的图案，而宽度和深度（对于表达能力来说很重要）之间也存在着一定的关系，“我们”是第一个对此进行了建模的。
从各个维度单独的进行缩放能发现都存在着增益瓶颈，如何去得到这么一个合适的等比缩放增益
Motivation and Method 一些直观上的motivation，以及假想
...</p></section><footer class=entry-footer><span title='2021-09-28 05:34:22 +0000 UTC'>September 28, 2021</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;109 words&nbsp;·&nbsp;aikenhong&nbsp;·&nbsp;<a href=/tags/machine-learning> Machine Learning</a>&nbsp;·&nbsp;<a href=/tags/cv> CV</a></footer><div class=tags style=padding:2px><div style=display:flex;flex-wrap:wrap;justify-content:flex-end;margin-top:5px><a href=/tags/machine-learning style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Machine Learning
</a><a href=/tags/cv style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#CV</a></div></div></div><a class=entry-link aria-label="post link to EfficientNet" href=https://hugotest-phi.vercel.app/posts/efficientnet/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://hugotest-phi.vercel.app/cover/cover23.jpeg alt></figure><div class=post-info><header class=entry-header><h2 class=entry-hint-parent>RL-DouZero</h2></header><section class=entry-content><p>Desc: GAME, RL Finished?: Yes Tags: Paper URL1: https://arxiv.org/abs/2106.06135 URL2: https://github.com/kwai/DouZero URL3: https://github.com/datamllab/rlcard-showdown ）
使用蒙特卡洛方法进行自我对弈不断更新预测模型的方法，这实际上也是普通人对于强化学习如何在self-play中实现自我更新的最基础的想法把： 自我对弈（记录动作序列）- 用最终的胜负（价值）更新网络。
算法的设计和思路 算法的目标是学习一个价值网路。网络的输入是当前状态和一个动作，输出是在当前状态做这个动作的期望收益（比如胜率）。简单来说，价值网络在每一步计算出哪种牌型赢的概率最大，然后选择最有可能赢的牌型。蒙特卡罗方法不断重复以下步骤来优化价值网络：
用价值网络生成一场对局 记录下该对局中所有的状态、动作和最后的收益（胜率） 将每一对状态和动作作为网络输入，收益作为网络输出，用梯度下降对价值网络进行一次更新 其实，所谓的蒙特卡罗方法就是一种随机模拟，即通过不断的重复实验来估计真实价值。
如下图所示，斗零采用一个价值神经网络，其输入是状态和动作，输出是价值。首先，过去的出牌用 LSTM 神经网络进行编码。然后 LSTM 的输出以及其他的表征被送入了 6 层全连接网络，最后输出价值。
系统训练的主要瓶颈在于模拟数据的生成，因为每一步出牌都要对神经网络做一次前向传播。斗零采用多演员（actor）的架构，在单个 GPU 服务器上，用了 45 个演员同时产生数据，最终数据被汇集到一个中央训练器进行训练。比较有趣的是，斗零并不需要太多的计算资源，仅仅需要一个普通的四卡 GPU 服务器就能达到不错的效果。这可以让大多数实验室轻松基于作者的代码做更多的尝试。
该方法的设计和实现上听起来都挺简单的，可以找个时间自己测试一下，玩一玩这个东西，对于我来说，看看他们怎么用这个lstm去进行历史编码的，以及在对transformer了解后，看看如何用transformer去替代这样的lstm是我这边的研究重点。
蒙特卡洛方法存在的问题 蒙特卡罗方法在强化学习领域中被大多数研究者忽视。学界普遍认为蒙特卡罗方法存在两个缺点：
蒙特卡罗方法不能处理不完整的状态序列
蒙特卡罗方法有很大的方差，导致采样效率很低。
但是斗地主中，可以产生转正的状态序列，同时很容易通过并行来采集大量的样本降低方差，主要是实现上简单，但是可能也是需要大量的数据把。
...</p></section><footer class=entry-footer><span title='2021-07-06 13:51:48 +0000 UTC'>July 6, 2021</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;86 words&nbsp;·&nbsp;aikenhong&nbsp;·&nbsp;<a href=/tags/machine-learning> Machine Learning</a>&nbsp;·&nbsp;<a href=/tags/reinforcement-learning> Reinforcement learning</a></footer><div class=tags style=padding:2px><div style=display:flex;flex-wrap:wrap;justify-content:flex-end;margin-top:5px><a href=/tags/machine-learning style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Machine Learning
</a><a href=/tags/reinforcement-learning style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Reinforcement learning</a></div></div></div><a class=entry-link aria-label="post link to RL-DouZero" href=https://hugotest-phi.vercel.app/posts/rl-douzero/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://hugotest-phi.vercel.app/cover/cover14.jpeg alt></figure><div class=post-info><header class=entry-header><h2 class=entry-hint-parent>Pooling</h2></header><section class=entry-content><p>DownSampling：Pooling的全面调研 @Aiken 2021 笔记摘录：
深度神经网络中的池化方法：全面调研（1989-2020） - 知乎 ；相同论文的简单中文Version 16页综述，共计67篇参考文献。网络千奇百怪，但基础元素却大致相同！本文全面调研了1989至2020年一些著名且有用的池化方法，并主要对20种池化方法进行了详细介绍（这些方法，你都知道么？） 注1：文末附【计算机视…
来自 https://zhuanlan.zhihu.com/p/341820742 原文：《Pooling Methods in Deep Neural Networks, a Review》
整合2 池化的根本目的（Motivation） 卷积神经网络是DNN的一种特殊类型，它由几个卷积层组成，每个卷积层后都有一个激活函数和一个池化层。
池化层是重要的层，它对来自上一层的特征图执行下采样，并生成具有简化分辨率的新feature maps 。该层极大地减小了输入的空间尺寸。 它有两个主要目的。 首先是减少参数或权重的数量，从而减少计算成本。 第二是控制网络的过拟合。
池化可以增加网络对于平移（旋转，伸缩）的不变性，提升网络的泛化能力。 增大感受野； 降低优化难度和参数数目， 理想的池化方法应仅提取有用的信息，并丢弃无关的细节。
特征不变性、特征降维、在一定程度上防止过拟合，更方便优化
主流的池化方法 Average Pooling 平均池化 没啥好说的，就是每个block取一个均值。如下图所示：更关注全局特征
Max Pooling 最大值池化 更关注重要的局部特征
...</p></section><footer class=entry-footer><span title='2021-06-23 13:48:56 +0000 UTC'>June 23, 2021</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;232 words&nbsp;·&nbsp;aikenhong&nbsp;·&nbsp;<a href=/tags/pooling> Pooling</a>&nbsp;·&nbsp;<a href=/tags/survey> Survey</a>&nbsp;·&nbsp;<a href=/tags/machine-learning> Machine Learning</a></footer><div class=tags style=padding:2px><div style=display:flex;flex-wrap:wrap;justify-content:flex-end;margin-top:5px><a href=/tags/pooling style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Pooling
</a><a href=/tags/survey style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Survey
</a><a href=/tags/machine-learning style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Machine Learning</a></div></div></div><a class=entry-link aria-label="post link to Pooling" href=https://hugotest-phi.vercel.app/posts/pooling/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://hugotest-phi.vercel.app/cover/cover9.jpeg alt></figure><div class=post-info><header class=entry-header><h2 class=entry-hint-parent>OW-openmix</h2></header><section class=entry-content><p>@Aiken 2021 究极万恶的撞车论文
Intro Motivation ：Tackle the problem of 发现无标注数据中与给定（已知）类别不相交的新类。
Related Research：
现有的方法通常1. 使用标记数据对模型进行预训练； 2. 无监督聚类在未标记的数据中识别新的类
作者认为label带来的essential knowledge在第二步中没有被充分学习利用到，这样模型就只能从第一步的现成知识中获益，而不能利用标记数据和未标记数据之间的潜在关系
Hypothesis：
有标记的类别和无标记的类别之间没有Overlap，这样导致在两个类别之间很难建立学习关系，（为啥我感觉这个说的都是屁话）
Solution：
Openmix：将标注的数据和未标注的数据同时混合起来得到一个联合标签的分布中，用两种方式来动态合成示例：
我们混合标记和未标记数据作为Training Img，混合了已知类别的先验生成的伪标签会比无监督情况下生成的伪标签跟家的可靠？防止在错误的伪标签前提下发生过拟合 在第一步的时候我们鼓励具有高类别置信度的无标记example作为可考虑的类别，然后我们将这些samples作为anchor，并将它们进一步的和无标注的samples整合，这使得我们能够对无标注数据产生更多的组合，并发现更精细的新类关系。 Detail 果然在混合的方式上和MixUp的策略进行比对了，就是diss了Mixup使用伪标签的情景可能会进一步的引入不确定性，导致算法的效果反向优化，就是再label和unlabeled数据上混用mixup，而不是单纯的对unlabel数据集进行混合。
首先将没有overlap的标签表现为联合标签分布再进行混合，也就是加长onehot，这样的标签的优越性在？对于unlabelled data引入了确定性，防止标签容易过拟合。也就是给伪标签加入了一个锚定，让他能够变化的更平滑
这尼玛这张图看了不久完事了，bibi一大堆啥的呢。主要分析一下三个损失函数代表的是什么意思。
...</p></section><footer class=entry-footer><span title='2021-06-23 13:45:50 +0000 UTC'>June 23, 2021</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;43 words&nbsp;·&nbsp;aikenhong&nbsp;·&nbsp;<a href=/tags/machine-learning> Machine Learning</a>&nbsp;·&nbsp;<a href=/tags/cv> CV</a></footer><div class=tags style=padding:2px><div style=display:flex;flex-wrap:wrap;justify-content:flex-end;margin-top:5px><a href=/tags/machine-learning style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Machine Learning
</a><a href=/tags/cv style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#CV</a></div></div></div><a class=entry-link aria-label="post link to OW-openmix" href=https://hugotest-phi.vercel.app/posts/ow-openmix/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://hugotest-phi.vercel.app/cover/cover5.jpeg alt></figure><div class=post-info><header class=entry-header><h2 class=entry-hint-parent>Reward is Enough</h2></header><section class=entry-content><p>Desc: RL Finished?: Yes Tags: Paper
通用人工智能，是否能通过强化学习的奖励机制就实现
实现AGI，强化学习就够了？Sutton、Silver师徒联手：奖励机制足够实现各种目标 对reward构建AGI的可行性的分析和探讨 这篇文章实际上没有给出一个很好的方案通过reward来实现各种AGI的设计，但是给出了在每一种场景下的AGI的reward设计的设想把。和对用reward进行设计的可行性分析。 同时分析了：感知、社交、语言、泛化、模仿，这几个方面
类似地，如果人工智能体的经验流足够丰富，那么单一目标（例如电池寿命或生存）可能隐含地需要实现同样广泛的子目标的能力，因此奖励最大化应该足以产生一种通用人工智能。
这不久回到了最基础的问题，没有这种长线以及大量数据交互以及全面场景的经验流，来支撑这样一个AGI的学习，所以这不也是在现阶段上纸上谈兵嘛？
对这篇论文我的总结是，我不推荐详细阅读，我觉得收益有限，太理想化，其实和强化学习本身的假设也没有太多新东西，我们可以假设强化学习能带来一个AGI，但是对应的约束和限制确实是有点多了。</p></section><footer class=entry-footer><span title='2021-06-06 13:53:36 +0000 UTC'>June 6, 2021</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;14 words&nbsp;·&nbsp;aikenhong&nbsp;·&nbsp;<a href=/tags/machine-learning> Machine Learning</a>&nbsp;·&nbsp;<a href=/tags/reinforcement-learning> Reinforcement learning</a></footer><div class=tags style=padding:2px><div style=display:flex;flex-wrap:wrap;justify-content:flex-end;margin-top:5px><a href=/tags/machine-learning style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Machine Learning
</a><a href=/tags/reinforcement-learning style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Reinforcement learning</a></div></div></div><a class=entry-link aria-label="post link to Reward is Enough" href=https://hugotest-phi.vercel.app/posts/rl-reward_is_enough/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://hugotest-phi.vercel.app/cover/cover21.jpeg alt></figure><div class=post-info><header class=entry-header><h2 class=entry-hint-parent>RL-MobaAI</h2></header><section class=entry-content><p>Created by: Aiken H Desc: GAME, RL Finished?: Yes Tags: Paper
《Master Complex Control in MOBA Games with Deep Reinforcement Learning》 论文阅读笔记
@Aiken H 2021.06
Introduction and Related Research. MOBA游戏的复杂度和状态空间都远比以前的围棋之类的运动更大，所以难度会更大一些
早一些的游戏ai使用的是（2015） Deep Q-Network 通过 supervised learning and self-play 结合的训练策略在围棋上击败了专业人类，而最近更多的使用了DRL（Deep Reinforcement Learning）的方法在近几年被进一步的应用。
Neural Network Architecture Include Contributions the encoding of Multi-modal inputs 多模态输入 the decoupling of inter-correlations in controls 控制内关联解码 exploration pruning mechanism 剪枝设置 Action mask for efficient exploration ❓效率 attack attention(for target selection) Attention机制做目标选择 LSTM for learning skill combos LSTM 机制做技能释放和链接 Optimize by multi-label proximal policy algorithm(improved PPO) dual-clip PPO 帮助训练的收敛 present a systematic and thorough study
...</p></section><footer class=entry-footer><span title='2021-05-30 13:52:42 +0000 UTC'>May 30, 2021</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;279 words&nbsp;·&nbsp;aikenhong&nbsp;·&nbsp;<a href=/tags/machine-learning> Machine Learning</a>&nbsp;·&nbsp;<a href=/tags/reinforcement-learning> Reinforcement learning</a></footer><div class=tags style=padding:2px><div style=display:flex;flex-wrap:wrap;justify-content:flex-end;margin-top:5px><a href=/tags/machine-learning style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Machine Learning
</a><a href=/tags/reinforcement-learning style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Reinforcement learning</a></div></div></div><a class=entry-link aria-label="post link to RL-MobaAI" href=https://hugotest-phi.vercel.app/posts/rl-mobaai_tencent/></a></article><footer class=page-footer><nav class=pagination><a class=last-icon href=https://hugotest-phi.vercel.app/tags/machine-learning/><span>&lt;&lt;</span>
</a><a class=prev href=https://hugotest-phi.vercel.app/tags/machine-learning/page/2/>«&nbsp;Prev&nbsp;2/4
</a><a class=next href=https://hugotest-phi.vercel.app/tags/machine-learning/page/4/>Next&nbsp;4/4&nbsp;»
</a><a class=last-icon href=https://hugotest-phi.vercel.app/tags/machine-learning/page/4/><span>>></span></a></nav></footer></main><footer class=footer><span>&copy; 2024 <a href=https://hugotest-phi.vercel.app/>aiken's blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a>
</span><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><span id=busuanzi_container>Visitors: <span id=busuanzi_value_site_uv></span>
Views: <span id=busuanzi_value_site_pv></span></span></footer><script>document.addEventListener("DOMContentLoaded",function(){const e=document.getElementById("busuanzi_value_site_uv"),t=document.getElementById("busuanzi_value_site_pv"),o=13863,i=16993;if(!e||!t){console.error("Busuanzi elements not found.");return}const n=new MutationObserver(e=>{for(let t of e)if(t.type==="childList"){n.disconnect(),t.target.innerHTML=parseInt(t.target.innerHTML||0)+o;break}}),s=new MutationObserver(e=>{for(let t of e)if(t.type==="childList"){s.disconnect(),t.target.innerHTML=parseInt(t.target.innerHTML||0)+i;break}});n.observe(e,{childList:!0}),s.observe(t,{childList:!0})})</script><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><span class=topInner><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
<span id=read_progress></span>
</span></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))}),document.getElementById("theme-toggle-nav").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.addEventListener("scroll",function(){const t=document.getElementById("read_progress"),n=document.documentElement.scrollHeight,s=document.documentElement.clientHeight,o=document.documentElement.scrollTop||document.body.scrollTop;t.innerText=((o/(n-s)).toFixed(2)*100).toFixed(0)})</script><script>(function(e,t){var s=document,o="script",n=s.createElement(o),i=s.getElementsByTagName(o)[0];n.src=e,t&&n.addEventListener("load",function(e){t(e)}),i.parentNode.insertBefore(n,i)})("/js/pangu.js",function(){pangu.spacingPage()})</script></body></html>