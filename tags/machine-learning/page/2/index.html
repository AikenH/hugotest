<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Machine Learning | aiken's blog</title>
<meta name=keywords content><meta name=description content="Let's learn and innovate together!"><meta name=author content="aikenhong"><link rel=canonical href=https://hugotest-phi.vercel.app/tags/machine-learning/><link crossorigin=anonymous href=/assets/css/stylesheet.2f85ca17c12c62fa86b1e474b8a51aca4856f0d645debfe4922a4d5ddc6aa978.css integrity="sha256-L4XKF8EsYvqGseR0uKUaykhW8NZF3r/kkipNXdxqqXg=" rel="preload stylesheet" as=style><link rel=icon href=https://hugotest-phi.vercel.app/favicon/ghost.ico><link rel=icon type=image/png sizes=16x16 href=https://hugotest-phi.vercel.app/favicon/ghost-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://hugotest-phi.vercel.app/favicon/ghost-32x32.png><link rel=apple-touch-icon href=https://hugotest-phi.vercel.app/favicon/ghost-apple-touch-icon.png><link rel=mask-icon href=https://hugotest-phi.vercel.app/favicon/ghost-192x192.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://hugotest-phi.vercel.app/tags/machine-learning/index.xml><link rel=alternate hreflang=en href=https://hugotest-phi.vercel.app/tags/machine-learning/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script src=https://cdn.jsdmirror.com/npm/jquery@3.5.1/dist/jquery.min.js></script><link rel=stylesheet href=https://cdn.jsdmirror.com/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css><script src=https://cdn.jsdmirror.com/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js></script><link rel=stylesheet href=https://cdn.jsdmirror.com/npm/katex@0.16.11/dist/katex.min.css><script defer src=https://cdn.jsdmirror.com/npm/katex@0.16.11/dist/katex.min.js></script><script defer src=https://cdn.jsdmirror.com/npm/katex@0.16.11/dist/contrib/auto-render.min.js onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><link rel=stylesheet href=https://cdn.jsdmirror.com/npm/lxgw-wenkai-webfont@1.1.0/style.css><link rel=stylesheet href=https://cdn.jsdmirror.com/npm/lxgw-wenkai-lite-webfont@1.1.0/style.css><link rel=stylesheet href=https://cdn.jsdmirror.com/npm/lxgw-wenkai-tc-webfont@1.0.0/style.css><link rel=stylesheet href=https://cdn.jsdmirror.com/npm/lxgw-wenkai-screen-webfont@1.1.0/style.css><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Open+Sans:ital,wght@0,300..800;1,300..800&family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&family=Ubuntu+Mono:ital,wght@0,400;0,700;1,400;1,700&family=Ubuntu:ital,wght@0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><meta property="og:url" content="https://hugotest-phi.vercel.app/tags/machine-learning/"><meta property="og:site_name" content="aiken's blog"><meta property="og:title" content="Machine Learning"><meta property="og:description" content="Let's learn and innovate together!"><meta property="og:locale" content="en-us"><meta property="og:type" content="website"><meta name=twitter:card content="summary"><meta name=twitter:title content="Machine Learning"><meta name=twitter:description content="Let's learn and innovate together!"></head><body class=list id=top><script type=module src=https://cdn.jsdmirror.com/npm/ionicons@7.1.0/dist/ionicons/ionicons.esm.js defer></script><script nomodule src=https://cdn.jsdmirror.com/npm/ionicons@7.1.0/dist/ionicons/ionicons.js defer></script><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://hugotest-phi.vercel.app/ accesskey=h title="aiken's blog (Alt + H)">aiken's blog</a><div class=logo-switches><button id=theme-toggle-nav accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://hugotest-phi.vercel.app/ title=home><span>home</span></a></li><li><a href=https://hugotest-phi.vercel.app/archives/ title=archives><span>archives</span></a></li><li><a href=https://hugotest-phi.vercel.app/search title="search (Alt + /)" accesskey=/><span>search</span></a></li></ul></nav></header><div class=sidebar><ul><li class=logo style=--bg:#333><a href=#><div class=logo-icon><img src=/logo/logo.png></div><div class=logo-text>Aiken's Blog</div></a></li><div class=menulist><li style=--bg:#f44336><a href=https://hugotest-phi.vercel.app/ title=home><div class=logo-icon><ion-icon name=home-outline></ion-icon></div><div class=logo-text>home</div></a></li><li style=--bg:#b145e9><a href=https://hugotest-phi.vercel.app/posts/ title=posts><div class=logo-icon><ion-icon name=newspaper-outline></ion-icon></div><div class=logo-text>posts</div></a></li><li style=--bg:#0f93c7><a href=https://hugotest-phi.vercel.app/tags/ title=tags><div class=logo-icon><ion-icon name=pricetags-outline></ion-icon></div><div class=logo-text>tags</div></a></li><li style=--bg:#ffa117><a href=https://hugotest-phi.vercel.app/categories/ title=categories><div class=logo-icon><ion-icon name=grid-outline></ion-icon></div><div class=logo-text>categories</div></a></li><li style=--bg:#0fc70f><a href=https://hugotest-phi.vercel.app/archives/ title=archives><div class=logo-icon><ion-icon name=folder-outline></ion-icon></div><div class=logo-text>archives</div></a></li><li style=--bg:#d16111><a href=https://hugotest-phi.vercel.app/about/ title=about><div class=logo-icon><ion-icon name=person></ion-icon></div><div class=logo-text>about</div></a></li><li style=--bg:#15c095><a href=https://hugotest-phi.vercel.app/search title="search (Alt + /)" accesskey=/><div class=logo-icon><ion-icon name=search></ion-icon></div><div class=logo-text>search</div></a></li></div><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt +T)"><li><div class=logo-icon id=moon><ion-icon name=moon-outline></ion-icon></div><div class=logo-icon id=sun><ion-icon name=sunny-outline></ion-icon></div></li></button></div></ul></div><link rel=stylesheet href=https://cdn.jsdmirror.com/npm/sakana-widget@2.7.0/lib/sakana.min.css><div id=sakana-widget></div><script>function initSakanaWidget(){const e=SakanaWidget.getCharacter("chisato");e.initialState={...e.initialState,controls:!1,t:.8,i:.002,s:1,d:.999,t:.5,w:.05},e.image="https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/mac/%E5%B7%B2%E7%A7%BB%E9%99%A4%E8%83%8C%E6%99%AF%E7%9A%84Xnip2024-11-30_00-47-03.png",SakanaWidget.registerCharacter("ronSang",e),new SakanaWidget({character:"ronSang"}).mount("#sakana-widget");const t=SakanaWidget.getCharacter("chisato");t.initialState={...t.initialState,controls:!1,t:.8,i:.002,s:1,d:.999,t:.5,w:.05},t.image="https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/mac/%E5%B7%B2%E7%A7%BB%E9%99%A4%E8%83%8C%E6%99%AF%E7%9A%84Xnip2024-11-30_00-36-37.png",SakanaWidget.registerCharacter("xinxin",t),new SakanaWidget({character:"xinxin"}).mount("#sakana-widget")}</script><script async onload=initSakanaWidget() src=https://cdn.jsdmirror.com/npm/sakana-widget@2.7.0/lib/sakana.min.js></script><main class=main><header class=page-header><div class=breadcrumbs><a href=https://hugotest-phi.vercel.app/>Home</a>&nbsp;»&nbsp;<a href=https://hugotest-phi.vercel.app/tags/>Tags</a></div><h1>Machine Learning
<a href=/tags/machine-learning/index.xml title=RSS aria-label=RSS><svg viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" height="23"><path d="M4 11a9 9 0 019 9"/><path d="M4 4a16 16 0 0116 16"/><circle cx="5" cy="19" r="1"/></svg></a></h1></header><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://hugotest-phi.vercel.app/cover/cover3.jpeg alt></figure><div class=post-info><header class=entry-header><h2 class=entry-hint-parent>IL-MgSvF</h2></header><section class=entry-content><p>@Author & Paper：Arxiv @Note：Aikenhong 2021/11/12
Other’s Note 1 Intro 旧知识的缓慢忘记和新知识的快速适应的困境：主要探讨Incremental中的Old和New的相互牵制和适应的问题，
旧知识的缓慢遗忘会导致对新任务的欠拟合，而快速适应会导致灾难性的遗忘，如何对这两种策略之间进行权衡，是一个重要的问题。
多尺度混合的解决这个问题：
Intra-space： 新类别的特征在同一个特征空间中 inter-saoce：新旧类别的特征在不同的特征空间中 本文提出的多粒度策略：
提出了一种频率感知的正则化操作，加速空间内的增量学习能力 新的特征空间组合操作，提升空间间的学习性能 实际上新类和旧类的特征最好是通过自监督或者无监督的特征学习方法归化到同一个特征空间中，在这种情况下对Classifier进行调整可能是一种更好的策略。通过混合特征空间来得到一个泛化能力更高的特征表示器。
传统的策略：无论是累加还是进行数据混合进行梯度计算，这种方式应该是将类别之间的梯度进行直接的叠加。
是否可以自行混合不同类别之间的学习梯度？通过对梯度的下降方程求解来得到一个旧类和新类之间的更好的下降方法。 具体的操作上就是对step进行处理，通过mixdataset对梯度进行分开计算 在混合策略上可以考虑梯度的下降方向，对不同的维度进行加权计算？ 上述的策略难以实施的点在于框架中的梯度是自动计算的，我们可以对损失进行加权，但是很难重新计算不同节点之间的梯度值 退而求其次的方法就是对新旧类的损失进行加权处理, 或者直接的混合数据 如果我们能获取梯度的方向, 或许我们能在每次迭代的过程中获得一个更好的加权值 首先可以尝试对梯度进行获取,Grad 我们在蒸馏的过程中通过MLP对不同的类别进行聚类划分, 这种方式的聚类和传统机器学习聚类的优劣又如何对比解释呢. 能不能用PCA方法或者multi-head策略来对特征进行处理, 这种类似因果的方式来分析特征中的冗余维度 上述的分析基于Mix Guide make error 的想法, 实际上还有一个问题就是Feature’s capabliity 不足的问题 New Key Word： Few-Shot class-incremental Learning
...</p></section><footer class=entry-footer><span title='2021-11-29 13:12:05 +0000 UTC'>November 29, 2021</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;72 words&nbsp;·&nbsp;aikenhong&nbsp;·&nbsp;<a href=/tags/machine-learning> Machine Learning</a>&nbsp;·&nbsp;<a href=/tags/incremental-learning> Incremental Learning</a></footer><div class=tags style=padding:2px><div style=display:flex;flex-wrap:wrap;justify-content:flex-end;margin-top:5px><a href=/tags/machine-learning style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Machine Learning
</a><a href=/tags/incremental-learning style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Incremental Learning</a></div></div></div><a class=entry-link aria-label="post link to IL-MgSvF" href=https://hugotest-phi.vercel.app/posts/il-mgsvf/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://hugotest-phi.vercel.app/cover/cover4.jpeg alt></figure><div class=post-info><header class=entry-header><h2 class=entry-hint-parent>SSL-MoCov3</h2></header><section class=entry-content><p>@Aiken 2021
恺明大神对自监督学习+transformer的实证研究，针对Transformer再自监督学习学习框架中的训练不稳定问题提出了Random Patch Projection的解决方案。
Article ；Paper ；
Motivation ViT的方法在自监督学习的任务中，精度下降的主要原因是由于算法的不稳定性，容易陷入局部的最优值，本文主要聚焦于采用视觉领域的自监督框架进行Transformer的训练，CNN的训练方法已经是一个比较明确约定俗称的方法，而Transformer的训练架构实际上还没有被完全的构建。</p></section><footer class=entry-footer><span title='2021-11-29 13:12:05 +0000 UTC'>November 29, 2021</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;10 words&nbsp;·&nbsp;aikenhong&nbsp;·&nbsp;<a href=/tags/machine-learning> Machine Learning</a></footer><div class=tags style=padding:2px><div style=display:flex;flex-wrap:wrap;justify-content:flex-end;margin-top:5px><a href=/tags/machine-learning style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Machine Learning</a></div></div></div><a class=entry-link aria-label="post link to SSL-MoCov3" href=https://hugotest-phi.vercel.app/posts/ssl-mocov3/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://hugotest-phi.vercel.app/cover/cover1.jpeg alt></figure><div class=post-info><header class=entry-header><h2 class=entry-hint-parent>Survey for Few-Shot Learning</h2></header><section class=entry-content><p>@aikenhong 2020 @h.aiken.970@gmail.com
另一个综述文章：https://zhuanlan.zhihu.com/p/61215293 对该文中一些内容有一些补充，可以看看
FSL简介：https://blog.csdn.net/xhw205/article/details/79491649
GCN用于FSL：https://blog.csdn.net/qq_36022260/article/details/93753532
Abstract FSL的根本目的就是弥合人工智能和人类之间的鸿沟，从少量带有监督信息的示例中学习。像人类一样有很高的泛化能力。这也能解决在实际应用场景中，数据难以收集或者大型数据难以建立的情景。
FSL的核心问题是：经验风险最小化器不可靠；那么如何使用先验知识去解决这个问题？
三个主要的角度：
数据：使用先验知识增强数据的监督经验 模型：使用先验知识来降低假设空间 算法：使用先验知识来改变搜索最佳假设（来进行搜索？) 现阶段针对FSL提出的一些相关的机器学习方法： meta-learning; embedding learning; generative modeling etc.
本文的主要工作：
基于FSL的原有设定，在现阶段的FSL发展上给出正式定义，同时阐明具体目标以及解决方式
通过具体示例列举和FSL的相关学习问题，比较了相关性和差异性，更好的区分问题
指出核心问题：经验风险最小化器不可靠，这提供了更系统有组织的改进FSL的方向。 经验风险最小化器👉：基于ML中的错误分解来分析的
整理，更好的理解
未来方向
Notation and Terminology 一般基于参数方法（因为非参数方法需要大量数据），在假设空间中搜索最优假设，并基于基于标签的Loss Function 来衡量效果。
Main Body Overview 2.1：具体定义&示例 2.2：相关问题和FSL的相关性和差异 2.3：核心问题 2.4:现有的方法如何处理这个问题
...</p></section><footer class=entry-footer><span title='2021-11-29 13:12:05 +0000 UTC'>November 29, 2021</span>&nbsp;·&nbsp;5 min&nbsp;·&nbsp;912 words&nbsp;·&nbsp;aikenhong&nbsp;·&nbsp;<a href=/tags/machine-learning> Machine Learning</a>&nbsp;·&nbsp;<a href=/tags/survey> Survey</a>&nbsp;·&nbsp;<a href=/tags/fsl> FSL</a></footer><div class=tags style=padding:2px><div style=display:flex;flex-wrap:wrap;justify-content:flex-end;margin-top:5px><a href=/tags/machine-learning style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Machine Learning
</a><a href=/tags/survey style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Survey
</a><a href=/tags/fsl style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#FSL</a></div></div></div><a class=entry-link aria-label="post link to Survey for Few-Shot Learning" href=https://hugotest-phi.vercel.app/posts/fsl-collection/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://hugotest-phi.vercel.app/cover/cover12.jpeg alt></figure><div class=post-info><header class=entry-header><h2 class=entry-hint-parent>Data Augmentation</h2></header><section class=entry-content><p>intergrate with those augmentation method.
this doc will
Record those theory and the effect after transformation Show the codes for ez use And the complete .py will be intergrate in my classification pipeline
reference below:arrow_down_small:, if use them,start it for respect for his work.
aleju/imgaug :star:albumentations-team/albumentations: torchvision PIL/ImageEnhance CCBS opencv Principle Principle 1 of coding: Don’t reinvent the wheel unless it’s needed
具体而言，仅在函数的拓展性较差，无法对其定制化，满足我们的日常需求的时候，我们会自行编写函数从而满足我们的需求，否则我们直接引用已知的库，提升我们的实现效率。 Principle 2 of coding 图像增强的两种使用方式：
...</p></section><footer class=entry-footer><span title='2021-11-28 06:24:20 +0000 UTC'>November 28, 2021</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;624 words&nbsp;·&nbsp;aikenhong&nbsp;·&nbsp;<a href=/tags/augmentation> Augmentation</a>&nbsp;·&nbsp;<a href=/tags/pytorch> Pytorch</a>&nbsp;·&nbsp;<a href=/tags/machine-learning> Machine Learning</a></footer><div class=tags style=padding:2px><div style=display:flex;flex-wrap:wrap;justify-content:flex-end;margin-top:5px><a href=/tags/augmentation style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Augmentation
</a><a href=/tags/pytorch style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Pytorch
</a><a href=/tags/machine-learning style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Machine Learning</a></div></div></div><a class=entry-link aria-label="post link to Data Augmentation" href=https://hugotest-phi.vercel.app/posts/dataaugmentation/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://hugotest-phi.vercel.app/cover/cover11.jpeg alt></figure><div class=post-info><header class=entry-header><h2 class=entry-hint-parent>MLFlow</h2></header><section class=entry-content><p>MLFlow 机器学习系统的使用 @Aiken 2020
基于Python开发的DAG数据工作流系统，面向机器学习，支持Spark并行环境和K8S容器集群；
MLFlow主要解决了三个问题，也就是三个我们可能会需要使用的功能：
Tracking：跟踪实验训练结果，记录算法参数，模型结果和运行效果等等； Projects：对所有的算法项目有一套标准的projects概念，记录下代码版本，参数和运行环境这些东西，并且projects是可以拟合所有的算法框架的； Models：解决的是打包和部署模型的这样一个行为，提供json接口给后续的flsk框架等等进行使用 基本部署 INSTALL：
DEPLOY：
Tracking 实验版本跟踪 Tracking为本次描述的重点，来做一个训练过程中的版本管理，记录每一次训练的参数和变量信息等等，这样便于后续的恢复和实验信息的整理。便于统计和管理。使用的时候好像也是需要代码嵌入的部分，就是需要在代码中调用MLFlow的API。
但是在Tracking的时候有一个比较重要的点在于，这个方法和Tensorboard对原模型的参数的嵌入和Logging记录中会不会产生冲突，同时两个方法之间是不是有什么overlap；关键的问题：
这两个API能不能进行混合使用 怎么统一和区分两个方法的应用情景 Reference https://mlflow.org/docs/latest/tracking.html https://mlflow.org/docs/latest/projects.html https://github.com/mlflow/mlflow https://blog.csdn.net/chenhuipin1173/article/details/100913909 https://my.oschina.net/u/2306127/blog/1825638 https://www.zhihu.com/question/280162556</p></section><footer class=entry-footer><span title='2021-11-28 06:24:19 +0000 UTC'>November 28, 2021</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;25 words&nbsp;·&nbsp;aikenhong&nbsp;·&nbsp;<a href=/tags/mlflow> MLFlow</a>&nbsp;·&nbsp;<a href=/tags/machine-learning> Machine Learning</a></footer><div class=tags style=padding:2px><div style=display:flex;flex-wrap:wrap;justify-content:flex-end;margin-top:5px><a href=/tags/mlflow style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#MLFlow
</a><a href=/tags/machine-learning style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Machine Learning</a></div></div></div><a class=entry-link aria-label="post link to MLFlow" href=https://hugotest-phi.vercel.app/posts/mlflow/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://hugotest-phi.vercel.app/cover/cover2.jpeg alt></figure><div class=post-info><header class=entry-header><h2 class=entry-hint-parent>MIM-V-simMIM</h2></header><section class=entry-content><p>@Author： MSRA Zhenda Xie @Source：Arxiv ， Code TBP ，Blog_CVer @Read：AikenHong 2021.11.22
“What I cannot create, I do not understand.” — Richard Feynman
Intro & Simple Conclusion Conclusion 继MAE和iBoT之后，MSRA也提出了一个图像掩码建模的新框架，SimMIM，该方法简化了最近这些提出的方法，不需要特殊设计，作者也验证了不需要那些特殊设计就已经能让模型展现出优秀的学习能力
采用中等大小的掩码块（32），对输入图像进行随机掩码，能使其成为强大的代理任务（pretext task） 直接回归预测原始像素的RGB值的效果并不比复杂设计的Patch分类方法差 Projector Head可以是轻量的Linear Layer，效果并不一定比MLP（多层）的差 Motivation 通过这种MIM方法可以实现在大量无标注的数据上得到一个表征能力up的通用特征模型，这种方式的backbone可以广泛的应用到图像上的各种子任务中（按照NLP）的经验来说，而为了类似的方式在图像上的大放异彩，我们首先需要分析Vision和Language的不同
图像有更强的局部关系：相互靠近的像素是高度相关和近似的，我们可以通过简单的copy padding复制一部分缺失 视觉信号是原始，低层次的，而文本分词是高级概念：对低层次信号的预测是否对高层次的视觉识别任务有用呢？ 视觉信号是连续的，而文本的分词是离散的： 如何基于分类的掩码语言建模方法来处理连续的视觉信号 Theoretical Design 掩码选择：同样的掩码的策略还是基于Patch进行的，对于掩码的设计来说，太大的掩码快或者太密集的掩码快，可能会导致找不到附近的像素来预测，实验证明32是一个具有竞争力的size，和文本任务的信息冗余程度不同也带来了覆盖比的选择，NLP通常是0.15，而在V中，32size可以支持0.1-0.7的覆盖率。
任务选择：使用原始像素的回归任务，因为回归任务和具有有序性的视觉信号的连续性很好的吻合。
预测头选择：使用轻量的预测头如（linear），迁移性能与繁琐的预测头相似或者略好，同时训练上更加的块。虽然较大的头或更高的分辨率通常会导致更强的生成能力，但这种更强的能力不一定有利于下游的微调任务。
...</p></section><footer class=entry-footer><span title='2021-11-23 06:38:19 +0000 UTC'>November 23, 2021</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;101 words&nbsp;·&nbsp;aikenhong&nbsp;·&nbsp;<a href=/tags/machine-learning> Machine Learning</a>&nbsp;·&nbsp;<a href=/tags/cv> CV</a></footer><div class=tags style=padding:2px><div style=display:flex;flex-wrap:wrap;justify-content:flex-end;margin-top:5px><a href=/tags/machine-learning style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Machine Learning
</a><a href=/tags/cv style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#CV</a></div></div></div><a class=entry-link aria-label="post link to MIM-V-simMIM" href=https://hugotest-phi.vercel.app/posts/mim-v-simmim/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://hugotest-phi.vercel.app/cover/cover10.jpeg alt></figure><div class=post-info><header class=entry-header><h2 class=entry-hint-parent>Transformer</h2></header><section class=entry-content><p>@aikenhong 2021
References For Transformer:
NLP The Transformer Family (lilianweng.github.io) VIT Transformer眼中世界 VS CNN眼中世界 李沐 NLP Transformer论文精读 Suveys cver1 ， cver2 ，cver3 This blog will divided into several part : lil’s blog, the survey for ViT, we using those article to help us understand the transformer.
综述我们以最新的一篇为准进行阅读，其他的可能后续进行查缺补漏把，如无必要，勿增烦恼。
Intro导言 主要参考文章2来进行我们简单的导入
基本问题 Transformer原本是NLP中的重要模型, 作为LSTM的后继者, 用于处理Seq2Seq的数据类型和情景, 若是要将Transformer运用到Vision的领域中, 首要的问题就是如何:
将Image作为序列化的Token输入Transform中 , 而达成这个目的主要有三种典型的方法:
像素点作为token, 使用VAE离散化图片作为token再输入 ViT: 将图片切为一个个Patch在经过线性的projector之后组成一个embedding表示进行交互 ...</p></section><footer class=entry-footer><span title='2021-11-23 06:38:19 +0000 UTC'>November 23, 2021</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;103 words&nbsp;·&nbsp;aikenhong&nbsp;·&nbsp;<a href=/tags/machine-learning> Machine Learning</a></footer><div class=tags style=padding:2px><div style=display:flex;flex-wrap:wrap;justify-content:flex-end;margin-top:5px><a href=/tags/machine-learning style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Machine Learning</a></div></div></div><a class=entry-link aria-label="post link to Transformer" href=https://hugotest-phi.vercel.app/posts/transformer/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://hugotest-phi.vercel.app/cover/cover15.jpeg alt></figure><div class=post-info><header class=entry-header><h2 class=entry-hint-parent>MIM-V-iBOT</h2></header><section class=entry-content><p>@Read: AikenHong 2021
@Author: https://arxiv.org/abs/2111.07832 @解读：Machine Heart 基本思想 基于NLP中的MLM(Masked Language Model)的核心训练目标: 也就是遮住文本的一部分, 然后通过模型去预测和补全, 这一过程是模型学到泛化的特征, 使用这种方法来进行大规模的与训练范式.
在基本的思想上和MAE采用的是一样的设计, 但是本文中坐着认为visual tokenizer的设计才是其中的关键.
不同于 NLP 中 tokenization 通过离线的词频分析即可将语料编码为含高语义的分词，图像 patch 是连续分布的且存在大量冗余的底层细节信息。而作者认为一个能够提取图像 patch 中高层语义的 tokenizer 可帮助模型避免学习到冗余的这些细节信息。作者认为视觉的 tokenizer 应该具备两个属性：（a）具备完整表征连续图像内容的能力；(b) 像 NLP 中的 tokenizer 一样具备高层语义。
文中对tokenizer的设计为一个知识蒸馏的过程:
文中使用这种在线tokenizer同时来监督这样的MIM过程, 也就是两部分协同学习, 能够较好的保证语义的同时并将图像内容转化为连续的特征分布, 具体的, tokenizer和目标网络狗狗想网络结构, 有移动平均来得到实际的tokenizer.
...</p></section><footer class=entry-footer><span title='2021-11-18 06:35:52 +0000 UTC'>November 18, 2021</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;133 words&nbsp;·&nbsp;aikenhong&nbsp;·&nbsp;<a href=/tags/machine-learning> Machine Learning</a>&nbsp;·&nbsp;<a href=/tags/cv> CV</a></footer><div class=tags style=padding:2px><div style=display:flex;flex-wrap:wrap;justify-content:flex-end;margin-top:5px><a href=/tags/machine-learning style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Machine Learning
</a><a href=/tags/cv style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#CV</a></div></div></div><a class=entry-link aria-label="post link to MIM-V-iBOT" href=https://hugotest-phi.vercel.app/posts/mim-v-ibot/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://hugotest-phi.vercel.app/cover/cover16.jpeg alt></figure><div class=post-info><header class=entry-header><h2 class=entry-hint-parent>MIM-V-MAE</h2></header><section class=entry-content><p>@Author：Facebook AI Research-Kaiming He Kaiming-MAE Conclusion 总而言之这是一种大模型的训练方法, 通过在少量数据的基础上实现大模型的训练.
整体的架构上是参考了NLP中的AutoEncoder机制，将原图切分patch，用mask掩盖原图，通过少量可见的Patch进行Encoder后和Mask融合，再通过非对称的Decoder进行pixel的还原。
这种设计的有点在于mask的scala是可变的，同时这种mask能减少我们训练过程中对显存和计算复杂度的损耗，同时问题本身是一个比较复杂的问题，得以训练复杂的大模型，这种方式最终呈现的效果就是训练的效率高且效益好。
体现了自监督学习在这方面的优越性，同时这种方法得以实现也是由于ViT模型对于CNN模型的取代，才使得这种序列化切块的方式容易实现和验证。
这种方式在最终体现了自监督学习对于有监督与训练的优越性，使用这种方式能够更好的得到一个模型的通用表征。
在这里论文中也说明了和NLP的不同点以及这样的模型对于decoder的要求实际上是比NLP更高的
experiment Masking：对于输入的图像进行均匀的切分并均匀的随机采样
MAE encoder: 简单的ViT模型，对输入图像进行编码后和Mask进行混合得到一个完整的令牌集合，从而确保Decode能够得到对应的位置信息。
MAE decoder：轻量级的架构，可以独立于编码器进行设计，我们使用更窄更浅的网络，计算量比编码器10%更小，这样能够更快的进行训练。解码器的最后一层是先行投影，输出的数量==补丁中像素值的数量，最后会resize层原图的维度。</p></section><footer class=entry-footer><span title='2021-11-15 12:25:45 +0000 UTC'>November 15, 2021</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;20 words&nbsp;·&nbsp;aikenhong&nbsp;·&nbsp;<a href=/tags/machine-learning> Machine Learning</a>&nbsp;·&nbsp;<a href=/tags/cv> CV</a></footer><div class=tags style=padding:2px><div style=display:flex;flex-wrap:wrap;justify-content:flex-end;margin-top:5px><a href=/tags/machine-learning style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Machine Learning
</a><a href=/tags/cv style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#CV</a></div></div></div><a class=entry-link aria-label="post link to MIM-V-MAE" href=https://hugotest-phi.vercel.app/posts/mim-v-mae/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://hugotest-phi.vercel.app/cover/cover15.jpeg alt></figure><div class=post-info><header class=entry-header><h2 class=entry-hint-parent>OWL-survey</h2></header><section class=entry-content><p>@AikenHong2021 OWL
分析现有的OWL特点，和当前自己的研究做一个区分，也汲取一下别人的研究的要点，
Reference arxiv @ self-supervised feature improve open-world learning arxiv @ open-world semi-supervised learning arxiv @ open-world learning without labels arxiv @ unseen class discovery in open-world classification arxiv @ Open-World Active Learning with Stacking Ensemble for Self-Driving Cars www @ open-world learning and application to product classification cvpr @ open world composition zero-shot learning cvpr @ Towards Open World Object Detection [cvpr](Large-Scale Long-Tailed Recognition in an Open World (thecvf.com) ) @ Large-Scale Long-Tailed Recognition in an Open World Conclusion Papers Mulit Open world Learning Definition
...</p></section><footer class=entry-footer><span title='2021-11-12 09:40:46 +0000 UTC'>November 12, 2021</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;180 words&nbsp;·&nbsp;aikenhong&nbsp;·&nbsp;<a href=/tags/machine-learning> Machine Learning</a>&nbsp;·&nbsp;<a href=/tags/open-world-learning> Open World Learning</a>&nbsp;·&nbsp;<a href=/tags/survey> Survey</a>&nbsp;·&nbsp;<a href=/tags/cv> CV</a></footer><div class=tags style=padding:2px><div style=display:flex;flex-wrap:wrap;justify-content:flex-end;margin-top:5px><a href=/tags/machine-learning style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Machine Learning
</a><a href=/tags/open-world-learning style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Open World Learning
</a><a href=/tags/survey style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Survey
</a><a href=/tags/cv style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#CV</a></div></div></div><a class=entry-link aria-label="post link to OWL-survey" href=https://hugotest-phi.vercel.app/posts/owl-survey/></a></article><footer class=page-footer><nav class=pagination><a class=last-icon href=https://hugotest-phi.vercel.app/tags/machine-learning/><span>&lt;&lt;</span>
</a><a class=prev href=https://hugotest-phi.vercel.app/tags/machine-learning/>«&nbsp;Prev&nbsp;1/4
</a><a class=next href=https://hugotest-phi.vercel.app/tags/machine-learning/page/3/>Next&nbsp;3/4&nbsp;»
</a><a class=last-icon href=https://hugotest-phi.vercel.app/tags/machine-learning/page/4/><span>>></span></a></nav></footer></main><footer class=footer><span>&copy; 2024 <a href=https://hugotest-phi.vercel.app/>aiken's blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a>
</span><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><span id=busuanzi_container>Visitors: <span id=busuanzi_value_site_uv></span>
Views: <span id=busuanzi_value_site_pv></span></span></footer><script>document.addEventListener("DOMContentLoaded",function(){const e=document.getElementById("busuanzi_value_site_uv"),t=document.getElementById("busuanzi_value_site_pv"),o=13863,i=16993;if(!e||!t){console.error("Busuanzi elements not found.");return}const n=new MutationObserver(e=>{for(let t of e)if(t.type==="childList"){n.disconnect(),t.target.innerHTML=parseInt(t.target.innerHTML||0)+o;break}}),s=new MutationObserver(e=>{for(let t of e)if(t.type==="childList"){s.disconnect(),t.target.innerHTML=parseInt(t.target.innerHTML||0)+i;break}});n.observe(e,{childList:!0}),s.observe(t,{childList:!0})})</script><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><span class=topInner><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
<span id=read_progress></span>
</span></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))}),document.getElementById("theme-toggle-nav").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.addEventListener("scroll",function(){const t=document.getElementById("read_progress"),n=document.documentElement.scrollHeight,s=document.documentElement.clientHeight,o=document.documentElement.scrollTop||document.body.scrollTop;t.innerText=((o/(n-s)).toFixed(2)*100).toFixed(0)})</script><script>(function(e,t){var s=document,o="script",n=s.createElement(o),i=s.getElementsByTagName(o)[0];n.src=e,t&&n.addEventListener("load",function(e){t(e)}),i.parentNode.insertBefore(n,i)})("/js/pangu.js",function(){pangu.spacingPage()})</script></body></html>