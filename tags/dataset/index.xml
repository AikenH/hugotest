<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Dataset on aiken&#39;s blog</title>
    <link>https://aikenh.cn/hugotest/tags/dataset/</link>
    <description>Recent content in Dataset on aiken&#39;s blog</description>
    <generator>Hugo -- 0.137.0</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 14 Jan 2020 02:13:25 +0000</lastBuildDate>
    <atom:link href="https://aikenh.cn/hugotest/tags/dataset/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Image Caption Dataset</title>
      <link>https://aikenh.cn/hugotest/posts/imagecaptionrequirement/</link>
      <pubDate>Tue, 14 Jan 2020 02:13:25 +0000</pubDate>
      <guid>https://aikenh.cn/hugotest/posts/imagecaptionrequirement/</guid>
      <description>&lt;h2 id=&#34;goals&#34;&gt;Goals：&lt;/h2&gt;
&lt;p&gt;1.数据量要求&lt;br&gt;
2.标注的标准&lt;br&gt;
3.标注的手段&lt;/p&gt;
&lt;h2 id=&#34;microsoft-coco-captions&#34;&gt;Microsoft COCO Captions:&lt;/h2&gt;
&lt;p&gt;使用Amazon的Mechanical Turk(AMT)收集数据，再对数据进行标注。&lt;br&gt;
“Each of our captions are also generated using human subjects on AMT.”&lt;/p&gt;
&lt;h3 id=&#34;一些其他信息caption-evaluation-server&#34;&gt;一些其他信息：(Caption Evaluation Server):&lt;/h3&gt;
&lt;p&gt;好像是可以评价caption的生成质量，但是应该是仅仅针对于使用COCO数据进行的，所以这一部分就不分析了。&lt;br&gt;
文中（section 3）包含了几种不同评价方法的介绍：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;BLEU&lt;br&gt;
ROUGE&lt;br&gt;
METEOR&lt;br&gt;
CIDEr&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;在进行Evaluation之前的 Tokenization and preprocessing中：&lt;br&gt;
使用了工具来添加caption标记：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Stanford PTBTokenizer in Stanford CoreNLP tools (version 3.4.1)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这个工具是模仿的是peen treebank3.   其参考文献和相关链接如下：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“The Stanford CoreNLP natural language processing toolkit,” in Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, 2014, pp. 55–60. &lt;a href=&#34;http://www.aclweb.org/anthology/P/P14/P14-5010&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;related-link&lt;/a&gt;
&lt;/p&gt;</description>
    </item>
    <item>
      <title>表情数据集</title>
      <link>https://aikenh.cn/hugotest/posts/emotion_dataset/</link>
      <pubDate>Mon, 14 Oct 2019 21:34:54 +0000</pubDate>
      <guid>https://aikenh.cn/hugotest/posts/emotion_dataset/</guid>
      <description>&lt;p&gt;根据这次需要搜集的表情的数据集，整理一下搜索数据集的网站和思路等&lt;/p&gt;
&lt;p&gt;##PART1 &amp;ldquo;表情数据集&amp;rdquo;
下列是对数据搜集的要求：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;是否开源&lt;/li&gt;
&lt;li&gt;图片的大小和数量&lt;/li&gt;
&lt;li&gt;图片的采集方式&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;eg：&lt;strong&gt;ck+&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;==数据来源及对应的搜索结果如下：==&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://toolbox.google.com/datasetsearch&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;谷歌数据集搜索导航&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.kairos.com/blog/60-facial-recognition-databases&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;60个人脸识别的数据集汇总&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;cv方面的好几百个数据集汇总&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;www.cvpapers.com/datasets.html&#34;&gt;另一个cv方向的数据集汇总&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/ChanChiChoi/awesome-Face_Recognition&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;github-CV汇总帖&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;http://cbcsl.ece.ohio-state.edu/EmotionNetChallenge/index.html#overview&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EmotioNet&lt;/a&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;好像是一个什么挑战赛的数据集要博士后或者相应教员才能申请使用&lt;a href=&#34;http://cbcsl.ece.ohio-state.edu/dbform_compound.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;申请页面&lt;/a&gt;
   &lt;br&gt;
没有具体的用于表情识别的数据子集的信息（好像数据很多，但是不知道在哪下，除了那个博士后申请的）&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;&lt;a href=&#34;http://www.whdeng.cn/RAF/model1.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RAF&lt;/a&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;real-world Affective Face&lt;br&gt;
&lt;strong&gt;数据量&lt;/strong&gt;29672个图像，7种基本情绪，12种复合情绪，（包含种族年龄范围性别属性，5个准确定位和37个自动生成的定位）&lt;br&gt;
**数据收集方式：**来源网络，大小应该很杂 （由40个人独立标定）&lt;br&gt;
email&lt;/p&gt;
&lt;/blockquote&gt;
&lt;!-- more --&gt;
&lt;p&gt;&lt;strong&gt;onenote中标记的和google 数据集搜索&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;http://www.cs.columbia.edu/CAVE/databases/facetracer/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;FaceTracer Database&lt;/a&gt;

basic info：（有图片的原始url）（wild）(网上收集的)姿势、环境、照明、质量 等等参差不齐，&lt;strong&gt;大小不固定&lt;/strong&gt;
(针对&lt;strong&gt;非商业用途开放&lt;/strong&gt;)   (&lt;strong&gt;表情只有笑容&lt;/strong&gt;)
故而不在详细收集，其他的标注信息，文中有详细讲解。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/Tencent/tencent-ml-images?tdsourcetag=s_pctim_aiomsg#tencent-ml-images&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tencent ML-Images&lt;/a&gt;

可能会有表情吧，是一个很大规模的多标签数据集。。。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cvrl.nd.edu/projects/data/#nd-2006-data-set&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ND-2006 Dataset&lt;/a&gt;
 06年貌似
13450张图片
6种基本情感
888个对象&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://ai.google/tools/datasets/google-facial-expression/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google facial expression comparison dataset&lt;/a&gt;

没有对数据集的基本信息介绍&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;百度/CSDN搜索&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.csdn.net/mathlxj/article/details/87920084&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://blog.csdn.net/mathlxj/article/details/87920084&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.csdn.net/computerme/article/details/49469767&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://blog.csdn.net/computerme/article/details/49469767&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://zenodo.org/record/3451524#.XaQ4vm5uKmQ&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;JAFFE&lt;/a&gt;

只有219张，标签为分散离散值。
划分六种情感指标  256*256&lt;/li&gt;
&lt;li&gt;中科大的&lt;a href=&#34;http://nvie.ustc.edu.cn/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NVIE&lt;/a&gt;

其中&lt;strong&gt;正面光照103人，左侧光照99人，右侧光照103人&lt;/strong&gt;。每种光照下，每人有六种表情（&lt;strong&gt;喜悦、愤怒、哀 伤、恐惧、厌恶、惊奇&lt;/strong&gt;）中的三种以上
平静帧、最大帧都已挑出 &lt;a href=&#34;http://nvie.ustc.edu.cn/contact.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;下载协议然后发给他们，才能下载&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://sites.google.com/site/emotiwchallenge/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AFEW database&lt;/a&gt;

&lt;strong&gt;数据来源&lt;/strong&gt;：电影片段的剪辑。&lt;strong&gt;情绪类型&lt;/strong&gt;：“六类基本表情”+中性
&lt;a href=&#34;https://cs.anu.edu.au/few/emotiw2015.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SFEW database&lt;/a&gt;

&lt;strong&gt;数据来源&lt;/strong&gt;：从AFEW中抽取出来的表情的静态帧。标注都在xml中&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://liris-accede.ec-lyon.fr/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LIRIS-ACCEDE database&lt;/a&gt;

同样也是基于电影抽取的，有三种数据集，包含离散的情感数据和基于维度的情感数据&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.cs.binghamton.edu/~lijun/Research/3DFE/3DFE_Analysis.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BU-3DFE database&lt;/a&gt;

3D的人脸表情数据集 **数据来源：**找人来做实验采集，按照要求的情绪做出表情
**数据量：**2500个3d面部模型（来自100个人）
还有同类的一些包含序列的等等的数据集，估计差别不大。
同样需要email获取&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.cse.oulu.fi/CMV/Downloads/Oulu-CASIA&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Oulu-CASIA database&lt;/a&gt;

&lt;strong&gt;数据来源&lt;/strong&gt;：让80个受试者做出相应的表情并用不同相机采集（红外可见光正常光和弱光）
&lt;strong&gt;情绪类型&lt;/strong&gt;：快乐、悲伤、惊讶、愤怒、恐惧、厌恶
email&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.socsci.ru.nl:8180/RaFD2/RaFD&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RAFD&lt;/a&gt;

&lt;strong&gt;数据来源&lt;/strong&gt;：让67个受试者做出相应的表情在不同注视点和不同角度采集
&lt;strong&gt;情绪类型&lt;/strong&gt;：8种情感类型
email&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.emotionlab.se/kdef/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;KDEF database&lt;/a&gt;

&lt;strong&gt;数据来源&lt;/strong&gt;：柔和、均匀的光线，多角度拍摄表情，使用统一的T恤颜色，在拍摄过程中使用网格将参与者面部居中，以及在扫描过程中将眼睛和嘴巴定位在固定的图像坐标中。
&lt;strong&gt;数据量&lt;/strong&gt;：4900张 (70个人，一个7个情感)
页面底端超链接（没进去成功。。）&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://mmlab.ie.cuhk.edu.hk/projects/socialrelation/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ExpW&lt;/a&gt;

9w张左右，图片差不多8G&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://mohammadmahoor.com/affectnet/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AffectNet&lt;/a&gt;

百万量级数据（Emotion Net好像也是）
&lt;strong&gt;获取方式&lt;/strong&gt;：从互联网获取
&lt;strong&gt;7类情感&lt;/strong&gt;，首页有各种情感的数据量，最少的也有4k张
填写申请表email下载&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.flintbox.com/public/project/4742/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Multi-PIE Face Database&lt;/a&gt;

收钱给数据集
&lt;strong&gt;获取方式&lt;/strong&gt;：记录会话
&lt;strong&gt;数据量&lt;/strong&gt;：75w图片&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;一些视频数据集(具体的在CSDN站上)（这些我就没有去详细看了）&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
