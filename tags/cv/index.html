<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>CV | aiken's blog</title>
<meta name=keywords content><meta name=description content="Let's learn and innovate together!"><meta name=author content="aikenhong"><link rel=canonical href=https://aikenh.cn/hugotest/tags/cv/><link crossorigin=anonymous href=/hugotest/assets/css/stylesheet.2f85ca17c12c62fa86b1e474b8a51aca4856f0d645debfe4922a4d5ddc6aa978.css integrity="sha256-L4XKF8EsYvqGseR0uKUaykhW8NZF3r/kkipNXdxqqXg=" rel="preload stylesheet" as=style><link rel=icon href=https://aikenh.cn/favicon/ghost.ico><link rel=icon type=image/png sizes=16x16 href=https://aikenh.cn/favicon/ghost-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://aikenh.cn/favicon/ghost-32x32.png><link rel=apple-touch-icon href=https://aikenh.cn/favicon/ghost-apple-touch-icon.png><link rel=mask-icon href=https://aikenh.cn/favicon/ghost-192x192.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://aikenh.cn/hugotest/tags/cv/index.xml><link rel=alternate hreflang=en href=https://aikenh.cn/hugotest/tags/cv/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script src=https://cdn.jsdmirror.com/npm/jquery@3.5.1/dist/jquery.min.js></script><link rel=stylesheet href=https://cdn.jsdmirror.com/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css><script src=https://cdn.jsdmirror.com/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js></script><link rel=stylesheet href=https://cdn.jsdmirror.com/npm/katex@0.16.11/dist/katex.min.css><script defer src=https://cdn.jsdmirror.com/npm/katex@0.16.11/dist/katex.min.js></script><script defer src=https://cdn.jsdmirror.com/npm/katex@0.16.11/dist/contrib/auto-render.min.js onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><link rel=stylesheet href=https://cdn.jsdmirror.com/npm/lxgw-wenkai-webfont@1.1.0/style.css><link rel=stylesheet href=https://cdn.jsdmirror.com/npm/lxgw-wenkai-lite-webfont@1.1.0/style.css><link rel=stylesheet href=https://cdn.jsdmirror.com/npm/lxgw-wenkai-tc-webfont@1.0.0/style.css><link rel=stylesheet href=https://cdn.jsdmirror.com/npm/lxgw-wenkai-screen-webfont@1.1.0/style.css><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Open+Sans:ital,wght@0,300..800;1,300..800&family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&family=Ubuntu+Mono:ital,wght@0,400;0,700;1,400;1,700&family=Ubuntu:ital,wght@0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><meta property="og:url" content="https://aikenh.cn/hugotest/tags/cv/"><meta property="og:site_name" content="aiken's blog"><meta property="og:title" content="CV"><meta property="og:description" content="Let's learn and innovate together!"><meta property="og:locale" content="en-us"><meta property="og:type" content="website"><meta name=twitter:card content="summary"><meta name=twitter:title content="CV"><meta name=twitter:description content="Let's learn and innovate together!"></head><body class=list id=top><script type=module src=https://cdn.jsdmirror.com/npm/ionicons@7.1.0/dist/ionicons/ionicons.esm.js defer></script><script nomodule src=https://cdn.jsdmirror.com/npm/ionicons@7.1.0/dist/ionicons/ionicons.js defer></script><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://aikenh.cn/hugotest/ accesskey=h title="aiken's blog (Alt + H)">aiken's blog</a><div class=logo-switches><button id=theme-toggle-nav accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://aikenh.cn/hugotest/ title=home><span>home</span></a></li><li><a href=https://aikenh.cn/hugotest/posts/ title=posts><span>posts</span></a></li><li><a href=https://aikenh.cn/hugotest/tags/ title=tags><span>tags</span></a></li><li><a href=https://aikenh.cn/hugotest/categories/ title=categories><span>categories</span></a></li><li><a href=https://aikenh.cn/hugotest/archives/ title=archives><span>archives</span></a></li><li><a href=https://aikenh.cn/hugotest/about/ title=about><span>about</span></a></li><li><a href=https://aikenh.cn/hugotest/search title="search (Alt + /)" accesskey=/><span>search</span></a></li></ul></nav></header><div class=sidebar><ul><li class=logo style=--bg:#333><a href=#><div class=logo-icon><img src=/logo/logo.png></div><div class=logo-text>Aiken's Blog</div></a></li><div class=menulist><li style=--bg:#f44336><a href=https://aikenh.cn/hugotest/ title=home><div class=logo-icon><ion-icon name=home-outline></ion-icon></div><div class=logo-text>home</div></a></li><li style=--bg:#b145e9><a href=https://aikenh.cn/hugotest/posts/ title=posts><div class=logo-icon><ion-icon name=newspaper-outline></ion-icon></div><div class=logo-text>posts</div></a></li><li style=--bg:#0f93c7><a href=https://aikenh.cn/hugotest/tags/ title=tags><div class=logo-icon><ion-icon name=pricetags-outline></ion-icon></div><div class=logo-text>tags</div></a></li><li style=--bg:#ffa117><a href=https://aikenh.cn/hugotest/categories/ title=categories><div class=logo-icon><ion-icon name=grid-outline></ion-icon></div><div class=logo-text>categories</div></a></li><li style=--bg:#0fc70f><a href=https://aikenh.cn/hugotest/archives/ title=archives><div class=logo-icon><ion-icon name=folder-outline></ion-icon></div><div class=logo-text>archives</div></a></li><li style=--bg:#d16111><a href=https://aikenh.cn/hugotest/about/ title=about><div class=logo-icon><ion-icon name=person></ion-icon></div><div class=logo-text>about</div></a></li><li style=--bg:#15c095><a href=https://aikenh.cn/hugotest/search title="search (Alt + /)" accesskey=/><div class=logo-icon><ion-icon name=search></ion-icon></div><div class=logo-text>search</div></a></li></div><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt +T)"><li><div class=logo-icon id=moon><ion-icon name=moon-outline></ion-icon></div><div class=logo-icon id=sun><ion-icon name=sunny-outline></ion-icon></div></li></button></div></ul></div><link rel=stylesheet href=https://cdn.jsdmirror.com/npm/sakana-widget@2.7.0/lib/sakana.min.css><div id=sakana-widget></div><script>function initSakanaWidget(){const e=SakanaWidget.getCharacter("chisato");e.initialState={...e.initialState,controls:!1,t:.8,i:.002,s:1,d:.999,t:.5,w:.05},e.image="https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/mac/%E5%B7%B2%E7%A7%BB%E9%99%A4%E8%83%8C%E6%99%AF%E7%9A%84Xnip2024-11-30_00-47-03.png",SakanaWidget.registerCharacter("ronSang",e),new SakanaWidget({character:"ronSang"}).mount("#sakana-widget");const t=SakanaWidget.getCharacter("chisato");t.initialState={...t.initialState,controls:!1,t:.8,i:.002,s:1,d:.999,t:.5,w:.05},t.image="https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/mac/%E5%B7%B2%E7%A7%BB%E9%99%A4%E8%83%8C%E6%99%AF%E7%9A%84Xnip2024-11-30_00-36-37.png",SakanaWidget.registerCharacter("xinxin",t),new SakanaWidget({character:"xinxin"}).mount("#sakana-widget")}</script><script async onload=initSakanaWidget() src=https://cdn.jsdmirror.com/npm/sakana-widget@2.7.0/lib/sakana.min.js></script><main class=main><header class=page-header><div class=breadcrumbs><a href=https://aikenh.cn/hugotest/>Home</a>&nbsp;»&nbsp;<a href=https://aikenh.cn/hugotest/tags/>Tags</a></div><h1>CV
<a href=/hugotest/tags/cv/index.xml title=RSS aria-label=RSS><svg viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" height="23"><path d="M4 11a9 9 0 019 9"/><path d="M4 4a16 16 0 0116 16"/><circle cx="5" cy="19" r="1"/></svg></a></h1></header><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://aikenh.cn/cover/cover16.jpeg alt></figure><div class=post-info><header class=entry-header><h2 class=entry-hint-parent>UniFramework 01</h2></header><section class=entry-content><p>文章的部分内容被密码保护：
--- DON'T MODIFY THIS LINE ---
@aiken 2021 Framework
Abstract Try To make structure universal，编写一个自己的通用的架构，框架化，满足通过不同的model文件和特殊配置文件就能实现不同的模型的一个架构。
只是一个初步的框架集成，还有很多没有完善的地方，目前测试了ResNet18 跑Cifar10，没有什么问题，如果有什么可以改进的地方，或者你实现了一些Feature，*欢迎进行交流*！（私下联系我最好啦！）
感谢帮助
还有一些可以参数化或者可视化的地方，由于时间关系目前还没有修改，有兴趣的可以自己先添加一下
暂时只集成了分类的模块，后续可能会随缘扩展
本框架主要希望实现的是：易读性，可拓展性，以及简洁；
希望将重要的，可变的参数都尽量的分离出来，通过配置文件和命令行参数去定义和运行我们的网络，在这种情况下实现一个较好的工作流程。
Final Project Design PURPOSE：新类发现和模型自主更新；同时希望能够解决长尾分布的数据情景；
**ANALYSIS：**为了实现这种模型的自主更新过程，将整体的流程分成两个部分
启动（start）： self supervissed 等方法无监督的学习特征提取网络（这种方式是否会对Unbalance产生增益）
初始化预测模型： 基于Unbalance的数据训练一个基础的分类模型，在输出分类结果的同时需要输出对应的预测置信度，这两个其实都是一些简单的Trick，而最重要的是Backbone的分类效果需要得到保证，同时Backbone需要支撑后续的模型蒸馏更新。 模型的自主更新和迭代： Online：在线运行推断模型，通过置信度输出筛选出新类样本，将样本在样本池中收集 Offline：基于样本池的规模和评估触发离线更新：伪标签生成模型；模型蒸馏和更新 创新点：自主新类发现和学习
...</p></section><footer class=entry-footer><span title='2021-12-04 01:43:30 +0000 UTC'>December 4, 2021</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;526 words&nbsp;·&nbsp;aikenhong&nbsp;·&nbsp;<a href=/tags/cv> CV</a>&nbsp;·&nbsp;<a href=/tags/machine-learning> Machine Learning</a></footer><div class=tags style=padding:2px><div style=display:flex;flex-wrap:wrap;justify-content:flex-end;margin-top:5px><a href=/tags/cv style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#CV
</a><a href=/tags/machine-learning style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Machine Learning</a></div></div></div><a class=entry-link aria-label="post link to UniFramework 01" href=https://aikenh.cn/hugotest/posts/uniframework/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://aikenh.cn/cover/cover2.jpeg alt></figure><div class=post-info><header class=entry-header><h2 class=entry-hint-parent>MIM-V-simMIM</h2></header><section class=entry-content><p>@Author： MSRA Zhenda Xie @Source：Arxiv ， Code TBP ，Blog_CVer @Read：AikenHong 2021.11.22
“What I cannot create, I do not understand.” — Richard Feynman
Intro & Simple Conclusion Conclusion 继MAE和iBoT之后，MSRA也提出了一个图像掩码建模的新框架，SimMIM，该方法简化了最近这些提出的方法，不需要特殊设计，作者也验证了不需要那些特殊设计就已经能让模型展现出优秀的学习能力
采用中等大小的掩码块（32），对输入图像进行随机掩码，能使其成为强大的代理任务（pretext task） 直接回归预测原始像素的RGB值的效果并不比复杂设计的Patch分类方法差 Projector Head可以是轻量的Linear Layer，效果并不一定比MLP（多层）的差 Motivation 通过这种MIM方法可以实现在大量无标注的数据上得到一个表征能力up的通用特征模型，这种方式的backbone可以广泛的应用到图像上的各种子任务中（按照NLP）的经验来说，而为了类似的方式在图像上的大放异彩，我们首先需要分析Vision和Language的不同
图像有更强的局部关系：相互靠近的像素是高度相关和近似的，我们可以通过简单的copy padding复制一部分缺失 视觉信号是原始，低层次的，而文本分词是高级概念：对低层次信号的预测是否对高层次的视觉识别任务有用呢？ 视觉信号是连续的，而文本的分词是离散的： 如何基于分类的掩码语言建模方法来处理连续的视觉信号 Theoretical Design 掩码选择：同样的掩码的策略还是基于Patch进行的，对于掩码的设计来说，太大的掩码快或者太密集的掩码快，可能会导致找不到附近的像素来预测，实验证明32是一个具有竞争力的size，和文本任务的信息冗余程度不同也带来了覆盖比的选择，NLP通常是0.15，而在V中，32size可以支持0.1-0.7的覆盖率。
任务选择：使用原始像素的回归任务，因为回归任务和具有有序性的视觉信号的连续性很好的吻合。
预测头选择：使用轻量的预测头如（linear），迁移性能与繁琐的预测头相似或者略好，同时训练上更加的块。虽然较大的头或更高的分辨率通常会导致更强的生成能力，但这种更强的能力不一定有利于下游的微调任务。
...</p></section><footer class=entry-footer><span title='2021-11-23 06:38:19 +0000 UTC'>November 23, 2021</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;101 words&nbsp;·&nbsp;aikenhong&nbsp;·&nbsp;<a href=/tags/machine-learning> Machine Learning</a>&nbsp;·&nbsp;<a href=/tags/cv> CV</a></footer><div class=tags style=padding:2px><div style=display:flex;flex-wrap:wrap;justify-content:flex-end;margin-top:5px><a href=/tags/machine-learning style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Machine Learning
</a><a href=/tags/cv style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#CV</a></div></div></div><a class=entry-link aria-label="post link to MIM-V-simMIM" href=https://aikenh.cn/hugotest/posts/mim-v-simmim/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://aikenh.cn/cover/cover15.jpeg alt></figure><div class=post-info><header class=entry-header><h2 class=entry-hint-parent>MIM-V-iBOT</h2></header><section class=entry-content><p>@Read: AikenHong 2021
@Author: https://arxiv.org/abs/2111.07832 @解读：Machine Heart 基本思想 基于NLP中的MLM(Masked Language Model)的核心训练目标: 也就是遮住文本的一部分, 然后通过模型去预测和补全, 这一过程是模型学到泛化的特征, 使用这种方法来进行大规模的与训练范式.
在基本的思想上和MAE采用的是一样的设计, 但是本文中坐着认为visual tokenizer的设计才是其中的关键.
不同于 NLP 中 tokenization 通过离线的词频分析即可将语料编码为含高语义的分词，图像 patch 是连续分布的且存在大量冗余的底层细节信息。而作者认为一个能够提取图像 patch 中高层语义的 tokenizer 可帮助模型避免学习到冗余的这些细节信息。作者认为视觉的 tokenizer 应该具备两个属性：（a）具备完整表征连续图像内容的能力；(b) 像 NLP 中的 tokenizer 一样具备高层语义。
文中对tokenizer的设计为一个知识蒸馏的过程:
文中使用这种在线tokenizer同时来监督这样的MIM过程, 也就是两部分协同学习, 能够较好的保证语义的同时并将图像内容转化为连续的特征分布, 具体的, tokenizer和目标网络狗狗想网络结构, 有移动平均来得到实际的tokenizer.
...</p></section><footer class=entry-footer><span title='2021-11-18 06:35:52 +0000 UTC'>November 18, 2021</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;133 words&nbsp;·&nbsp;aikenhong&nbsp;·&nbsp;<a href=/tags/machine-learning> Machine Learning</a>&nbsp;·&nbsp;<a href=/tags/cv> CV</a></footer><div class=tags style=padding:2px><div style=display:flex;flex-wrap:wrap;justify-content:flex-end;margin-top:5px><a href=/tags/machine-learning style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Machine Learning
</a><a href=/tags/cv style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#CV</a></div></div></div><a class=entry-link aria-label="post link to MIM-V-iBOT" href=https://aikenh.cn/hugotest/posts/mim-v-ibot/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://aikenh.cn/cover/cover16.jpeg alt></figure><div class=post-info><header class=entry-header><h2 class=entry-hint-parent>MIM-V-MAE</h2></header><section class=entry-content><p>@Author：Facebook AI Research-Kaiming He Kaiming-MAE Conclusion 总而言之这是一种大模型的训练方法, 通过在少量数据的基础上实现大模型的训练.
整体的架构上是参考了NLP中的AutoEncoder机制，将原图切分patch，用mask掩盖原图，通过少量可见的Patch进行Encoder后和Mask融合，再通过非对称的Decoder进行pixel的还原。
这种设计的有点在于mask的scala是可变的，同时这种mask能减少我们训练过程中对显存和计算复杂度的损耗，同时问题本身是一个比较复杂的问题，得以训练复杂的大模型，这种方式最终呈现的效果就是训练的效率高且效益好。
体现了自监督学习在这方面的优越性，同时这种方法得以实现也是由于ViT模型对于CNN模型的取代，才使得这种序列化切块的方式容易实现和验证。
这种方式在最终体现了自监督学习对于有监督与训练的优越性，使用这种方式能够更好的得到一个模型的通用表征。
在这里论文中也说明了和NLP的不同点以及这样的模型对于decoder的要求实际上是比NLP更高的
experiment Masking：对于输入的图像进行均匀的切分并均匀的随机采样
MAE encoder: 简单的ViT模型，对输入图像进行编码后和Mask进行混合得到一个完整的令牌集合，从而确保Decode能够得到对应的位置信息。
MAE decoder：轻量级的架构，可以独立于编码器进行设计，我们使用更窄更浅的网络，计算量比编码器10%更小，这样能够更快的进行训练。解码器的最后一层是先行投影，输出的数量==补丁中像素值的数量，最后会resize层原图的维度。</p></section><footer class=entry-footer><span title='2021-11-15 12:25:45 +0000 UTC'>November 15, 2021</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;20 words&nbsp;·&nbsp;aikenhong&nbsp;·&nbsp;<a href=/tags/machine-learning> Machine Learning</a>&nbsp;·&nbsp;<a href=/tags/cv> CV</a></footer><div class=tags style=padding:2px><div style=display:flex;flex-wrap:wrap;justify-content:flex-end;margin-top:5px><a href=/tags/machine-learning style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Machine Learning
</a><a href=/tags/cv style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#CV</a></div></div></div><a class=entry-link aria-label="post link to MIM-V-MAE" href=https://aikenh.cn/hugotest/posts/mim-v-mae/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://aikenh.cn/cover/cover15.jpeg alt></figure><div class=post-info><header class=entry-header><h2 class=entry-hint-parent>OWL-survey</h2></header><section class=entry-content><p>@AikenHong2021 OWL
分析现有的OWL特点，和当前自己的研究做一个区分，也汲取一下别人的研究的要点，
Reference arxiv @ self-supervised feature improve open-world learning arxiv @ open-world semi-supervised learning arxiv @ open-world learning without labels arxiv @ unseen class discovery in open-world classification arxiv @ Open-World Active Learning with Stacking Ensemble for Self-Driving Cars www @ open-world learning and application to product classification cvpr @ open world composition zero-shot learning cvpr @ Towards Open World Object Detection [cvpr](Large-Scale Long-Tailed Recognition in an Open World (thecvf.com) ) @ Large-Scale Long-Tailed Recognition in an Open World Conclusion Papers Mulit Open world Learning Definition
...</p></section><footer class=entry-footer><span title='2021-11-12 09:40:46 +0000 UTC'>November 12, 2021</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;180 words&nbsp;·&nbsp;aikenhong&nbsp;·&nbsp;<a href=/tags/machine-learning> Machine Learning</a>&nbsp;·&nbsp;<a href=/tags/open-world-learning> Open World Learning</a>&nbsp;·&nbsp;<a href=/tags/survey> Survey</a>&nbsp;·&nbsp;<a href=/tags/cv> CV</a></footer><div class=tags style=padding:2px><div style=display:flex;flex-wrap:wrap;justify-content:flex-end;margin-top:5px><a href=/tags/machine-learning style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Machine Learning
</a><a href=/tags/open-world-learning style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Open World Learning
</a><a href=/tags/survey style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Survey
</a><a href=/tags/cv style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#CV</a></div></div></div><a class=entry-link aria-label="post link to OWL-survey" href=https://aikenh.cn/hugotest/posts/owl-survey/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://aikenh.cn/cover/cover16.jpeg alt></figure><div class=post-info><header class=entry-header><h2 class=entry-hint-parent>OW Object Detector</h2></header><section class=entry-content><p>@Aiken 2021
框架撞车系列，主要看看这一篇论文中怎么解决如下的问题👇，并从中借鉴和优化的我框架设计
思路分析 Motivation 模型实现的主要的两个TASK：
Open Set Learning ： 在没有明确监督的时候，将尚未引入的目标类别识别为未知 Incremental Learning：类别增量学习 实现这两个问题的主要思路：
自动标注：借鉴RPN的class-agnostic，以及检测和分类的显著性指标的差异，找到并自动标注NewClass **对比聚类：**使用prototype feature来进行聚类，同时计算Distance损失 it seems like contain a unknown prototype. **energy based：**亥姆霍兹自由能公式？ ENERGY BASED Feature： $F$ , Label: $L$ , Energy: $E(F,l)$
能量函数倾向于将已知的类别分类到低熵的分布上，然后我们可以根据特征在能量空间上的划分来区分新类和旧类。然后我们可以根据logits表达的softmax形式，找到输出和Gibbs distribution的相关性：
$$ p(l \mid \boldsymbol{f})=\frac{\exp \left(\frac{g_{l}(\boldsymbol{f})}{T}\right)}{\sum_{i=1}^{\mathrm{C}} \exp \left(\frac{g_{i}(\boldsymbol{f})}{T}\right)}=\frac{\exp \left(-\frac{E(\boldsymbol{f}, l)}{T}\right)}{\exp \left(-\frac{E(\boldsymbol{f})}{T}\right)}
$$
通过这个相关性，我们对自由能进行一个定义，以logits的形式表达
...</p></section><footer class=entry-footer><span title='2021-09-28 13:44:20 +0000 UTC'>September 28, 2021</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;71 words&nbsp;·&nbsp;aikenhong&nbsp;·&nbsp;<a href=/tags/machine-learning> Machine Learning</a>&nbsp;·&nbsp;<a href=/tags/open-world-learning> Open World Learning</a>&nbsp;·&nbsp;<a href=/tags/cv> CV</a></footer><div class=tags style=padding:2px><div style=display:flex;flex-wrap:wrap;justify-content:flex-end;margin-top:5px><a href=/tags/machine-learning style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Machine Learning
</a><a href=/tags/open-world-learning style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Open World Learning
</a><a href=/tags/cv style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#CV</a></div></div></div><a class=entry-link aria-label="post link to OW Object Detector" href=https://aikenh.cn/hugotest/posts/ow-od/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://aikenh.cn/cover/cover15.jpeg alt></figure><div class=post-info><header class=entry-header><h2 class=entry-hint-parent>EfficientNet</h2></header><section class=entry-content><p>Tags: Paper URL1: https://arxiv.org/pdf/1905.11946.pdf URL2: https://arxiv.org/pdf/2104.00298.pdf 提出了一种模型缩放策略，如何更高效的平衡网络的深度、宽度、和图片分辨率 1. Efficient Net: Rethinking Model Scaling for Convolutional Neural Networks 2. EfficientNetV2: Smaller Models and Faster Training
@Aiken H 2021 find detail to code his
Efficient Net V1 除了提出了缩放策略以外，还使用神经架构搜索还建立了一个新的baseline network，得到了一系列模型。
平衡网络宽度、深度、分辨率至关重要，这种平衡可以通过简单的恒定比率缩放维度来实现，于是我们提出了一种简单有效的复合缩放方法。
复合缩放的物理意义：输入图像更大的话就需要更多层来增加感受野和更多通道，从而能在更大的图像上捕获更多细粒度的图案，而宽度和深度（对于表达能力来说很重要）之间也存在着一定的关系，“我们”是第一个对此进行了建模的。
从各个维度单独的进行缩放能发现都存在着增益瓶颈，如何去得到这么一个合适的等比缩放增益
Motivation and Method 一些直观上的motivation，以及假想
...</p></section><footer class=entry-footer><span title='2021-09-28 05:34:22 +0000 UTC'>September 28, 2021</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;109 words&nbsp;·&nbsp;aikenhong&nbsp;·&nbsp;<a href=/tags/machine-learning> Machine Learning</a>&nbsp;·&nbsp;<a href=/tags/cv> CV</a></footer><div class=tags style=padding:2px><div style=display:flex;flex-wrap:wrap;justify-content:flex-end;margin-top:5px><a href=/tags/machine-learning style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Machine Learning
</a><a href=/tags/cv style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#CV</a></div></div></div><a class=entry-link aria-label="post link to EfficientNet" href=https://aikenh.cn/hugotest/posts/efficientnet/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://aikenh.cn/cover/cover9.jpeg alt></figure><div class=post-info><header class=entry-header><h2 class=entry-hint-parent>OW-openmix</h2></header><section class=entry-content><p>@Aiken 2021 究极万恶的撞车论文
Intro Motivation ：Tackle the problem of 发现无标注数据中与给定（已知）类别不相交的新类。
Related Research：
现有的方法通常1. 使用标记数据对模型进行预训练； 2. 无监督聚类在未标记的数据中识别新的类
作者认为label带来的essential knowledge在第二步中没有被充分学习利用到，这样模型就只能从第一步的现成知识中获益，而不能利用标记数据和未标记数据之间的潜在关系
Hypothesis：
有标记的类别和无标记的类别之间没有Overlap，这样导致在两个类别之间很难建立学习关系，（为啥我感觉这个说的都是屁话）
Solution：
Openmix：将标注的数据和未标注的数据同时混合起来得到一个联合标签的分布中，用两种方式来动态合成示例：
我们混合标记和未标记数据作为Training Img，混合了已知类别的先验生成的伪标签会比无监督情况下生成的伪标签跟家的可靠？防止在错误的伪标签前提下发生过拟合 在第一步的时候我们鼓励具有高类别置信度的无标记example作为可考虑的类别，然后我们将这些samples作为anchor，并将它们进一步的和无标注的samples整合，这使得我们能够对无标注数据产生更多的组合，并发现更精细的新类关系。 Detail 果然在混合的方式上和MixUp的策略进行比对了，就是diss了Mixup使用伪标签的情景可能会进一步的引入不确定性，导致算法的效果反向优化，就是再label和unlabeled数据上混用mixup，而不是单纯的对unlabel数据集进行混合。
首先将没有overlap的标签表现为联合标签分布再进行混合，也就是加长onehot，这样的标签的优越性在？对于unlabelled data引入了确定性，防止标签容易过拟合。也就是给伪标签加入了一个锚定，让他能够变化的更平滑
这尼玛这张图看了不久完事了，bibi一大堆啥的呢。主要分析一下三个损失函数代表的是什么意思。
...</p></section><footer class=entry-footer><span title='2021-06-23 13:45:50 +0000 UTC'>June 23, 2021</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;43 words&nbsp;·&nbsp;aikenhong&nbsp;·&nbsp;<a href=/tags/machine-learning> Machine Learning</a>&nbsp;·&nbsp;<a href=/tags/cv> CV</a></footer><div class=tags style=padding:2px><div style=display:flex;flex-wrap:wrap;justify-content:flex-end;margin-top:5px><a href=/tags/machine-learning style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Machine Learning
</a><a href=/tags/cv style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#CV</a></div></div></div><a class=entry-link aria-label="post link to OW-openmix" href=https://aikenh.cn/hugotest/posts/ow-openmix/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=https://aikenh.cn/cover/cover6.jpeg alt></figure><div class=post-info><header class=entry-header><h2 class=entry-hint-parent>YOLOv4 阅读笔记</h2></header><section class=entry-content><p>@AikenHong 20200726
基于YOLO v4 掌握一些CV方面训练的Trick，同时针对Typora的使用进行一个熟悉掌握。GITHUB CODE 一些相关的参考资料
⚡️https://zhuanlan.zhihu.com/p/150127712
⚡ 机器之心YOLOv4 ⚡️https://www.zhihu.com/question/390191723/answer/1177584901
本文中一些其他的收获
• 其他可替代的Object Detection的SOTA算法有哪些
• BoS，BoF方法
• 简直是一个Tricks的综述
Abstract 本文对近期再CNN上的一些Feature方法进行了尝试组合，并实现了新的SOTA，其实就是一些通用的Trick的组合尝试，包括
• 加权残差连接（WRC）
• Cross-Stage-Partial-connection，CSP
• Cross mini-Batch Normalization，CmBN
• 自对抗训练（Self-adversarial-training，SAT）
• Mish 激活（Mish-activation）
• Mosaic 数据增强
• DropBlock 正则化
• CIoU 损失
基于该文章我们可以了解一下这些方法的主要思路和后续的应用价值。YOLOv4 更快，更准确，只需要比较小的计算需求即可
...</p></section><footer class=entry-footer><span title='2020-07-26 13:16:40 +0000 UTC'>July 26, 2020</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;417 words&nbsp;·&nbsp;aikenhong&nbsp;·&nbsp;<a href=/tags/cv> CV</a>&nbsp;·&nbsp;<a href=/tags/object-detection> Object Detection</a></footer><div class=tags style=padding:2px><div style=display:flex;flex-wrap:wrap;justify-content:flex-end;margin-top:5px><a href=/tags/cv style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#CV
</a><a href=/tags/object-detection style="margin-right:5px;color:#fff;background-color:rgba(53,174,128,.7);border-radius:6px;padding:1px 10px;font-size:13px">#Object Detection</a></div></div></div><a class=entry-link aria-label="post link to YOLOv4 阅读笔记" href=https://aikenh.cn/hugotest/posts/cv-yolov4/></a></article></main><footer class=footer><span>&copy; 2024 <a href=https://aikenh.cn/hugotest/>aiken's blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a>
</span><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><span id=busuanzi_container>Visitors: <span id=busuanzi_value_site_uv></span>
Views: <span id=busuanzi_value_site_pv></span></span></footer><script>document.addEventListener("DOMContentLoaded",function(){const e=document.getElementById("busuanzi_value_site_uv"),t=document.getElementById("busuanzi_value_site_pv"),o=13863,i=16993;if(!e||!t){console.error("Busuanzi elements not found.");return}const n=new MutationObserver(e=>{for(let t of e)if(t.type==="childList"){n.disconnect(),t.target.innerHTML=parseInt(t.target.innerHTML||0)+o;break}}),s=new MutationObserver(e=>{for(let t of e)if(t.type==="childList"){s.disconnect(),t.target.innerHTML=parseInt(t.target.innerHTML||0)+i;break}});n.observe(e,{childList:!0}),s.observe(t,{childList:!0})})</script><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><span class=topInner><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
<span id=read_progress></span>
</span></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))}),document.getElementById("theme-toggle-nav").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.addEventListener("scroll",function(){const t=document.getElementById("read_progress"),n=document.documentElement.scrollHeight,s=document.documentElement.clientHeight,o=document.documentElement.scrollTop||document.body.scrollTop;t.innerText=((o/(n-s)).toFixed(2)*100).toFixed(0)})</script><script>(function(e,t){var s=document,o="script",n=s.createElement(o),i=s.getElementsByTagName(o)[0];n.src=e,t&&n.addEventListener("load",function(e){t(e)}),i.parentNode.insertBefore(n,i)})("/js/pangu.js",function(){pangu.spacingPage()})</script></body></html>