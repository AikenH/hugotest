<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Attention Mechanism | aiken's blog</title>
<meta name=keywords content="Machine Learning,Attention"><meta name=description content="@Aiken 2020.9.16
对基本注意力机制的一些资料和理解做一些简单的汇总，着重分析基本思想原理，应用和实现（即 structure），还有一些Weakness和相应的解决方案。

1.TODO-List：

根据Lil’Log的Attention？Attention！进行初步的整理
各个分类的具体含义分开整理，理解一部分整理一部分，可能结合实际的应用去整理吧。
其中很重要的一点是数学分析的部分，需要对数学原理进行整理和领会。

What’s Attention In Deep Learning
在某种程度上，注意力是由我么如何关注视觉图像的不同区域或者我们如何关联同一个句子中的不同单词所启发的：针对于问题的不同，我们会对图像的某些具体的区域重视（某些区域在视觉中呈现高分辨率，而另一些则是低分辨率的情况），或者句子中的某些词重视的情况。


  
    
  





可以解释一个句子中紧密的上下文单词之间的关系，比如我们看到eating就会期待看到food，而color对于我们来说就没有那么重要。


  
    
  




"><meta name=author content="aikenhong"><link rel=canonical href=https://hugotest-phi.vercel.app/posts/attention/><link crossorigin=anonymous href=/assets/css/stylesheet.2f85ca17c12c62fa86b1e474b8a51aca4856f0d645debfe4922a4d5ddc6aa978.css integrity="sha256-L4XKF8EsYvqGseR0uKUaykhW8NZF3r/kkipNXdxqqXg=" rel="preload stylesheet" as=style><link rel=icon href=https://hugotest-phi.vercel.app/favicon/ghost.ico><link rel=icon type=image/png sizes=16x16 href=https://hugotest-phi.vercel.app/favicon/ghost-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://hugotest-phi.vercel.app/favicon/ghost-32x32.png><link rel=apple-touch-icon href=https://hugotest-phi.vercel.app/favicon/ghost-apple-touch-icon.png><link rel=mask-icon href=https://hugotest-phi.vercel.app/favicon/ghost-192x192.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://hugotest-phi.vercel.app/posts/attention/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script src=https://cdn.jsdmirror.com/npm/jquery@3.5.1/dist/jquery.min.js></script><link rel=stylesheet href=https://cdn.jsdmirror.com/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css><script src=https://cdn.jsdmirror.com/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js></script><link rel=stylesheet href=https://cdn.jsdmirror.com/npm/katex@0.16.11/dist/katex.min.css><script defer src=https://cdn.jsdmirror.com/npm/katex@0.16.11/dist/katex.min.js></script><script defer src=https://cdn.jsdmirror.com/npm/katex@0.16.11/dist/contrib/auto-render.min.js onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><link rel=stylesheet href=https://cdn.jsdmirror.com/npm/lxgw-wenkai-webfont@1.1.0/style.css><link rel=stylesheet href=https://cdn.jsdmirror.com/npm/lxgw-wenkai-lite-webfont@1.1.0/style.css><link rel=stylesheet href=https://cdn.jsdmirror.com/npm/lxgw-wenkai-tc-webfont@1.0.0/style.css><link rel=stylesheet href=https://cdn.jsdmirror.com/npm/lxgw-wenkai-screen-webfont@1.1.0/style.css><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Open+Sans:ital,wght@0,300..800;1,300..800&family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&family=Ubuntu+Mono:ital,wght@0,400;0,700;1,400;1,700&family=Ubuntu:ital,wght@0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><meta property="og:url" content="https://hugotest-phi.vercel.app/posts/attention/"><meta property="og:site_name" content="aiken's blog"><meta property="og:title" content="Attention Mechanism"><meta property="og:description" content="@Aiken 2020.9.16
对基本注意力机制的一些资料和理解做一些简单的汇总，着重分析基本思想原理，应用和实现（即 structure），还有一些Weakness和相应的解决方案。
1.TODO-List：
根据Lil’Log的Attention？Attention！进行初步的整理 各个分类的具体含义分开整理，理解一部分整理一部分，可能结合实际的应用去整理吧。 其中很重要的一点是数学分析的部分，需要对数学原理进行整理和领会。 What’s Attention In Deep Learning 在某种程度上，注意力是由我么如何关注视觉图像的不同区域或者我们如何关联同一个句子中的不同单词所启发的：针对于问题的不同，我们会对图像的某些具体的区域重视（某些区域在视觉中呈现高分辨率，而另一些则是低分辨率的情况），或者句子中的某些词重视的情况。
可以解释一个句子中紧密的上下文单词之间的关系，比如我们看到eating就会期待看到food，而color对于我们来说就没有那么重要。"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-09-28T05:34:22+00:00"><meta property="article:modified_time" content="2021-09-28T05:34:22+00:00"><meta property="article:tag" content="Machine Learning"><meta property="article:tag" content="Attention"><meta property="og:image" content="https://hugotest-phi.vercel.app/cover/cover5.jpeg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://hugotest-phi.vercel.app/cover/cover5.jpeg"><meta name=twitter:title content="Attention Mechanism"><meta name=twitter:description content="@Aiken 2020.9.16
对基本注意力机制的一些资料和理解做一些简单的汇总，着重分析基本思想原理，应用和实现（即 structure），还有一些Weakness和相应的解决方案。

1.TODO-List：

根据Lil’Log的Attention？Attention！进行初步的整理
各个分类的具体含义分开整理，理解一部分整理一部分，可能结合实际的应用去整理吧。
其中很重要的一点是数学分析的部分，需要对数学原理进行整理和领会。

What’s Attention In Deep Learning
在某种程度上，注意力是由我么如何关注视觉图像的不同区域或者我们如何关联同一个句子中的不同单词所启发的：针对于问题的不同，我们会对图像的某些具体的区域重视（某些区域在视觉中呈现高分辨率，而另一些则是低分辨率的情况），或者句子中的某些词重视的情况。


  
    
  





可以解释一个句子中紧密的上下文单词之间的关系，比如我们看到eating就会期待看到food，而color对于我们来说就没有那么重要。


  
    
  




"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://hugotest-phi.vercel.app/posts/"},{"@type":"ListItem","position":2,"name":"Attention Mechanism","item":"https://hugotest-phi.vercel.app/posts/attention/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Attention Mechanism","name":"Attention Mechanism","description":"@Aiken 2020.9.16\n对基本注意力机制的一些资料和理解做一些简单的汇总，着重分析基本思想原理，应用和实现（即 structure），还有一些Weakness和相应的解决方案。\n1.TODO-List：\n根据Lil’Log的Attention？Attention！进行初步的整理 各个分类的具体含义分开整理，理解一部分整理一部分，可能结合实际的应用去整理吧。 其中很重要的一点是数学分析的部分，需要对数学原理进行整理和领会。 What’s Attention In Deep Learning 在某种程度上，注意力是由我么如何关注视觉图像的不同区域或者我们如何关联同一个句子中的不同单词所启发的：针对于问题的不同，我们会对图像的某些具体的区域重视（某些区域在视觉中呈现高分辨率，而另一些则是低分辨率的情况），或者句子中的某些词重视的情况。\n可以解释一个句子中紧密的上下文单词之间的关系，比如我们看到eating就会期待看到food，而color对于我们来说就没有那么重要。\n","keywords":["Machine Learning","Attention"],"articleBody":"@Aiken 2020.9.16\n对基本注意力机制的一些资料和理解做一些简单的汇总，着重分析基本思想原理，应用和实现（即 structure），还有一些Weakness和相应的解决方案。\n1.TODO-List：\n根据Lil’Log的Attention？Attention！进行初步的整理 各个分类的具体含义分开整理，理解一部分整理一部分，可能结合实际的应用去整理吧。 其中很重要的一点是数学分析的部分，需要对数学原理进行整理和领会。 What’s Attention In Deep Learning 在某种程度上，注意力是由我么如何关注视觉图像的不同区域或者我们如何关联同一个句子中的不同单词所启发的：针对于问题的不同，我们会对图像的某些具体的区域重视（某些区域在视觉中呈现高分辨率，而另一些则是低分辨率的情况），或者句子中的某些词重视的情况。\n可以解释一个句子中紧密的上下文单词之间的关系，比如我们看到eating就会期待看到food，而color对于我们来说就没有那么重要。\n简而言之，深度学习中的注意力就是针对不同问题下的重要性权重的向量，比如我们根据关联性，给上面的每个单词赋予一个相关性的向量权重，然后基于注意力加权后的总和作为目标的近似值。\nWhat’s Wrong with Seq2Seq Model seq2swq旨在将再encoder-decoder的架构下，将输入序列转换为新序列，对两个序列的长度没有要求。\nEncoder：将输入序列转换成固定长度的context vector，用来概括整个源序列的信息 Decoder：使用context vector初始化，来进行解码（转换） 这样的Encoder和Decoder架构都是Recurrent Neural Networks，就像LSTM和GRU架构。\n缺陷：固定长度的context vector可能会导致序列信息的缺失，同时可能会无法记住长句子，同时也会丢失时序的对齐信息。所以Attention就诞生了。\n:question:\nBorn for Translation 这几个部分的研究都是基于NMT自然机器翻译，来进行分析的（文本非图像）\n原本的E-D是通过Encoder的最后一个hidden states构建单个Context Vector，而Attention 做的就是在Context Vec和Source之间建立一个Shortcut（简单的Feed Forword Network），而源和目标的对齐由上下文向量来学习和控制。而上下文中应该consumes（包含）几个维度的信息\nEncoder的hidden States；\nDecoder的hidden States；\nSource 和 Target之间的对齐信息；\nEncoder Decoder实际上都是RNN的架构，S实际上就是H，隐层状态，Decoder对EOS输出了一个初始的预测Y以后，推进Decoder的进程。\n双向的recurrent network（bidirectional RNN）使得Encoder隐态同时考虑前后单词。而在Decoder中\ns_t = f(s_{t-1},y_{t-1},c_t)\n其中上下文向量c_t是输入序列的隐态之和，通过alignment score来进行加权。\n$a_{t,i}$ 将作为输入i对输出t的隐态加权（相关性），在网络中共同训练，通过tanh来进行激活处理。Score则使用下面的策略：\n$$ score(st,hi)=v_a^Ttanh(Wa[st;hi])\r$$\r其中V, W都是对齐模型中学到的参数\n最终对齐完成以后就会是：\nA Family of Attention Mechanism Summary 由于这个良好的理论基础和实现效果，以及对序列的良好兼容性，这样的算法就被很快的拓展到了计算机视觉的领域中。（学 科 交 叉）\n这些不同的score方法实际上就是对每个position上的input和当前位置（通常用上一个时刻的state）来进行相关性建模，针对这些相关性建模，来确定输入对于下一个状态的影响因子，也就是得到一个context向量。这实际上就是注意力机制的核心理念把。\nReferred to as “concat” in Luong, et al, 2015 and as “additive attention” in Vaswani, et al, 2017.(^) It adds a scaling factor 1/n−−√1/n, motivated by the concern when the input is large, the softmax function may have an extremely small gradient, hard for efficient learning.\n下面对一些更广泛类型的注意力机制做一个摘要性的总结\nSelf-Attention 这个已经在我的Onenote中整理过了，大概看看就好。这里只讲了一些应用上的点，\n自我注意，内部注意力，也就是自我内部关联的注意力关系获取；\nSoft vs Hard Attention Image Caption《show, attend and tell》，CNN获取特征，LSTM解码器利用卷积功能来逐一生成单词。通过Attention 来学习权重，可视化如下：\n根据注意力是访问整个图像还是访问图像的一个patch来定义hard和soft attention。\n**Soft Attention：**就是普通的全图Attention，将权重放在图像的所有色块上，基本没啥区别\nPro: 模型平滑容易微分 Con：expensive when the source image is large **Hard Attention：**一次只处理一小块图像，其实就相当于用0/1去对图像区域进行硬编码 ，只对一部分的图像进行处理，但是这种方式的技术实现我还是没什么概念，后面可以专门研究一下。\nPro：推理计算需求比较小 Con：不可微分，需要用更复杂的技术来进行训练（例如variance reduction or reinforcement learning） Global vs Loacal Attention Global实际上就和soft是类似的，而局部注意力机制更像是hard和soft之间的一个融合，对改进hard使其可以微分；\nthe model first predicts a single aligned position for the current target word and a window centered around the source position is then used to compute a context vector.\n后面提到了一些神经图灵机的内容就是一些基本的计算机制，实际上可能是启发LSTM设计擦除算法设计的根源。\nNeural Turing Machines Attention Mechanisms 可以将权重的看作一个神经图灵机的寻址权重：（基于位置或者基于内容的两种方式）\nContent-based addressing:\n基于内容寻址的权重设置从输入行和存储行提取的键值向量之间的相似度来创建关注向量：权重的具体计算通过余弦相似度后进行softmax归一化来进行权重的分配。\n另外通过β系数来放大或者减弱分布的焦点。\nInterpolation 通过interpolation gate scalar $g_t$ 将生成的上下文向量和上一步生成的注意力权重进行混合\n$$ w^g_t = g_tw^c_t + (1-g_t)w_{t-1}\r$$\rLocation-baesd addressing 基于位置的寻址，对注意力向量中不同位置的值进行求和，通过对允许的整数偏移位置中的权重来进行参数加权。这相当于是一个一维卷积来测试偏移量。\n然后对注意力的分布进行锐化处理，使用γ参数\n完整的注意力workflow为：\nPointer Network 解决的是输入输出都是顺序数据的问题。\nTransformer Introduction Attention is all u need ：提出的重要的Transformer的这种架构，使得算法能够完全基于注意力机制，而不需要序列对齐的recurrent Network。\nKey,Value and Query 这一块还是详细解读一下，这里说的太粗略了，没讲清楚。\nTransformer的主要重要的架构在于multi-head和self-attention，Transformer将输入堪称一个key-value pairs，均为输入长度n。\n$$ Attention(Q,K,V)=softmax(\\frac{Q K^⊤} {\\sqrt{n}})V\r$$\rKey Value Query 到底都是些什么东西。\nEncoder-Decoder and Taxonomy of Attention 基本的encoder-decoder是基于对序列处理的问题提出的，通常情况下针对的是输入输出都是序列的计算模型。下图a展示了典型的E-D机制，以及加入了self-attention的情况b。（文中提到了一些E-D机制的问题）\nSeq2Seq的RNN序列模型Encoder需要把之前的输入最终转化成单一的ht（定长），可能会造成序列信息的丢失，同时他只能平权的考虑输入，不能有所重点。同时对于时序对齐信息也会丢失，这对结构化特别重要\n注意力机制，在实现上就是通过在体系架构中，嵌入一个额外的前馈神经网络，来进行学习额外的参数（作为Decoder的补充输入，作为序列和之前信息的补充），而且通常，这个前馈的神经网络是需要和encoder-decoder协同训练的，也就是需要和整体网络共同训练。\n训练层面是否存在一些特殊的情况，对于一些特殊的Attention 模型，是否需要一些特殊的训练机制。这点如果看到的话，建议需要整理一下\n在Survey中，对注意力机制基于多种标准进行了分类，具体的分类情况可以依下图所示，还有一些具体方法的实现。\n对于几种分类方式，可以参考几个译文的解读，但是我觉得说的并不清楚，后续就分别针对各种方法进行分析吧。\nBTW：在RNN等循环结构不能并行化处理的条件下，提出的类似AM的Transformer（Attention is all u need）结构，他的encoder和decoder由两个子层堆叠而来：Feed Forward Network 和 Multi-head self attention。\nPosition-wise FFN：获取输入的顺序，在encoder阶段，对于每一个token既生成content embedding也生成position encoding。\nMulti-Head Self-Attention：在每个子层中使用self - attention来关联标记及其在相同输入序列中的位置，几个注意力层并行堆叠，对于相同的输入有不同的线性转换。这使得模型能够捕获输入的不同方面，提高表达能力。\nTransformer and Self-Attention 参考论文以及参考资料1 AND 《Attention is all you need》\nSelf-Attention ：\n下图这一段整的明明白白，把整个框架说的比较明白，对输入的embedding（分别做3次卷积，图像输入的情况），分成Query，Key，Value，然后如图进行后续的操作\n图2就表示了在CV领域，为什么需要将输入做完卷积以后再成进去，而其中的scale由于图片和序列是不一致的，他们的size本来就是统一的（基本规范的数据集中），那么就可以省略掉这一步，从而直接进行softmax，相当于在预处理的时候已经进行了这样的归一操作。\nself-attention 是 Transformer的基础，通过多个Self-Attention组成的Multi-head Attention 就是Transformer模型的基本结构。\nMulti-head Attention\nReference 参考资料\nAttention机制详解2：self-attention \u0026 Transformer origin document of ↑ Attention的数学原理 Survey:\n《An Attentive Survey of Attention Models》 mendeley\n论文翻译和解读1 :zap:\n论文解读和翻译2 论文解读和翻译3 Transformer:\n《Attention is all you need》Mendeley \u0026OneNote ","wordCount":"332","inLanguage":"en","image":"https://hugotest-phi.vercel.app/cover/cover5.jpeg","datePublished":"2021-09-28T05:34:22Z","dateModified":"2021-09-28T05:34:22Z","author":[{"@type":"Person","name":"aikenhong"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://hugotest-phi.vercel.app/posts/attention/"},"publisher":{"@type":"Organization","name":"aiken's blog","logo":{"@type":"ImageObject","url":"https://hugotest-phi.vercel.app/favicon/ghost.ico"}}}</script></head><body id=top><script type=module src=https://cdn.jsdmirror.com/npm/ionicons@7.1.0/dist/ionicons/ionicons.esm.js defer></script><script nomodule src=https://cdn.jsdmirror.com/npm/ionicons@7.1.0/dist/ionicons/ionicons.js defer></script><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://hugotest-phi.vercel.app/ accesskey=h title="aiken's blog (Alt + H)">aiken's blog</a><div class=logo-switches><button id=theme-toggle-nav accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://hugotest-phi.vercel.app/ title=home><span>home</span></a></li><li><a href=https://hugotest-phi.vercel.app/archives/ title=archives><span>archives</span></a></li><li><a href=https://hugotest-phi.vercel.app/search title="search (Alt + /)" accesskey=/><span>search</span></a></li></ul></nav></header><div class=sidebar><ul><li class=logo style=--bg:#333><a href=#><div class=logo-icon><img src=/logo/logo.png></div><div class=logo-text>Aiken's Blog</div></a></li><div class=menulist><li style=--bg:#f44336><a href=https://hugotest-phi.vercel.app/ title=home><div class=logo-icon><ion-icon name=home-outline></ion-icon></div><div class=logo-text>home</div></a></li><li style=--bg:#b145e9><a href=https://hugotest-phi.vercel.app/posts/ title=posts><div class=logo-icon><ion-icon name=newspaper-outline></ion-icon></div><div class=logo-text>posts</div></a></li><li style=--bg:#0f93c7><a href=https://hugotest-phi.vercel.app/tags/ title=tags><div class=logo-icon><ion-icon name=pricetags-outline></ion-icon></div><div class=logo-text>tags</div></a></li><li style=--bg:#ffa117><a href=https://hugotest-phi.vercel.app/categories/ title=categories><div class=logo-icon><ion-icon name=grid-outline></ion-icon></div><div class=logo-text>categories</div></a></li><li style=--bg:#0fc70f><a href=https://hugotest-phi.vercel.app/archives/ title=archives><div class=logo-icon><ion-icon name=folder-outline></ion-icon></div><div class=logo-text>archives</div></a></li><li style=--bg:#d16111><a href=https://hugotest-phi.vercel.app/about/ title=about><div class=logo-icon><ion-icon name=person></ion-icon></div><div class=logo-text>about</div></a></li><li style=--bg:#15c095><a href=https://hugotest-phi.vercel.app/search title="search (Alt + /)" accesskey=/><div class=logo-icon><ion-icon name=search></ion-icon></div><div class=logo-text>search</div></a></li></div><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt +T)"><li><div class=logo-icon id=moon><ion-icon name=moon-outline></ion-icon></div><div class=logo-icon id=sun><ion-icon name=sunny-outline></ion-icon></div></li></button></div></ul></div><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://hugotest-phi.vercel.app/>Home</a>&nbsp;»&nbsp;<a href=https://hugotest-phi.vercel.app/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Attention Mechanism</h1><div class=post-meta><span title='2021-09-28 05:34:22 +0000 UTC'>September 28, 2021</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;332 words&nbsp;·&nbsp;aikenhong&nbsp;·&nbsp;<a href=/tags/machine-learning> Machine Learning</a>&nbsp;·&nbsp;<a href=/tags/attention> Attention</a>&nbsp;|&nbsp;<a href=https://github.com/%3cpath_to_repo%3e/content/posts/Attention.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><figure class=entry-cover><img loading=eager src=https://hugotest-phi.vercel.app/cover/cover5.jpeg alt></figure><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#whats-attention-in-deep-learning aria-label="What’s Attention In Deep Learning">What’s Attention In Deep Learning</a></li><li><a href=#whats-wrong-with-seq2seq-model aria-label="What’s Wrong with Seq2Seq Model">What’s Wrong with Seq2Seq Model</a></li><li><a href=#born-for-translation aria-label="Born for Translation">Born for Translation</a></li><li><a href=#a-family-of-attention-mechanism aria-label="A Family of Attention Mechanism">A Family of Attention Mechanism</a><ul class=header-level-2><li><a href=#summary aria-label=Summary>Summary</a></li><li><a href=#self-attention aria-label=Self-Attention>Self-Attention</a></li><li><a href=#soft-vs-hard-attention aria-label="Soft vs Hard Attention">Soft vs Hard Attention</a></li><li><a href=#global-vs-loacal-attention aria-label="Global vs Loacal Attention">Global vs Loacal Attention</a></li></ul></li><li><a href=#neural-turing-machines aria-label="Neural Turing Machines">Neural Turing Machines</a><ul class=header-level-2><li><a href=#attention-mechanisms aria-label="Attention Mechanisms">Attention Mechanisms</a></li></ul></li><li><a href=#pointer-network aria-label="Pointer Network">Pointer Network</a></li><li><a href=#transformer-introduction aria-label="Transformer Introduction">Transformer Introduction</a><ul class=header-level-2><li><a href=#keyvalue-and-query aria-label="Key,Value and Query">Key,Value and Query</a></li></ul></li><li><a href=#encoder-decoder-and-taxonomy-of-attention aria-label="Encoder-Decoder and Taxonomy of Attention">Encoder-Decoder and Taxonomy of Attention</a></li><li><a href=#transformer-and-self-attention aria-label="Transformer and Self-Attention">Transformer and Self-Attention</a></li><li><a href=#reference aria-label=Reference>Reference</a></li></ul></div></details></div></aside><script>let activeElement,elements;document.addEventListener("DOMContentLoaded",function(){if(checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),elements.length>0){activeElement=elements[0];const e=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${e}"]`).classList.add("active")}const t=document.getElementById("top-link");t&&t.addEventListener("click",e=>{e.preventDefault(),window.scrollTo({top:0,behavior:"smooth"})})},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{const e=window.pageYOffset||document.documentElement.scrollTop;if(e===0)return;elements&&elements.length>0&&(elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase(),n=document.querySelector(`.inner ul li a[href="#${t}"]`);n.classList.remove("read")}),activeElement=Array.from(elements).find(t=>{if(getOffsetTop(t)-e>0&&getOffsetTop(t)-e<window.innerHeight/2)return t})||activeElement,elements.forEach((t)=>{const o=encodeURI(t.getAttribute("id")).toLowerCase(),s=document.querySelector(`.inner ul li a[href="#${o}"]`);if(t===activeElement){s.classList.add("active");const e=document.querySelector(".toc .inner"),t=s.offsetTop,n=e.clientHeight,o=s.clientHeight,i=t-n/2+o/2;e.scrollTo({top:i,behavior:"smooth"})}else getOffsetTop(t)<e&&s.classList.add("read"),s.classList.remove("active")}))},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return!document.querySelector(".hugo-encryptor-prompt")&&elements.length!=0&&(elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),console.log("Elements re-queried:",elements)),0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><p>@Aiken 2020.9.16</p><p>对基本注意力机制的一些资料和理解做一些简单的汇总，着重分析基本思想原理，应用和实现（即 structure），还有一些Weakness和相应的解决方案。</p><p><strong>1.TODO-List：</strong></p><ul><li>根据Lil’Log的Attention？Attention！进行初步的整理</li><li>各个分类的具体含义分开整理，理解一部分整理一部分，可能结合实际的应用去整理吧。</li><li>其中很重要的一点是数学分析的部分，需要对数学原理进行整理和领会。</li></ul><h2 id=whats-attention-in-deep-learning>What’s Attention In Deep Learning<a hidden class=anchor aria-hidden=true href=#whats-attention-in-deep-learning>#</a></h2><p>在某种程度上，注意力是由我么如何关注视觉图像的不同区域或者我们如何关联同一个句子中的不同单词所启发的：针对于问题的不同，我们会对图像的某些具体的区域重视（某些区域在视觉中呈现高分辨率，而另一些则是低分辨率的情况），或者句子中的某些词重视的情况。</p><p><div class=post-img-view><a data-fancybox=gallery href=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210911182724.png><img alt=image-20210118210915289 loading=lazy src=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210911182724.png class=responsive-image src=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210911182724.png style="display:block;margin:0 auto" alt=image-20210118210915289></a></div><script>document.addEventListener("DOMContentLoaded",function(){var e=document.querySelectorAll(".responsive-image"),t=window.innerHeight/2.5;e.forEach(function(e){e.style.maxHeight=t+"px"})})</script></p><p>可以解释一个句子中紧密的上下文单词之间的关系，比如我们看到eating就会期待看到food，而color对于我们来说就没有那么重要。</p><p><div class=post-img-view><a data-fancybox=gallery href=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210911182729.png><img alt=image-20210118210936862 loading=lazy src=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210911182729.png class=responsive-image src=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210911182729.png style="display:block;margin:0 auto" alt=image-20210118210936862></a></div><script>document.addEventListener("DOMContentLoaded",function(){var e=document.querySelectorAll(".responsive-image"),t=window.innerHeight/2.5;e.forEach(function(e){e.style.maxHeight=t+"px"})})</script></p><p>简而言之，深度学习中的注意力就是针对不同问题下的重要性权重的向量，比如我们根据关联性，给上面的每个单词赋予一个相关性的向量权重，然后基于注意力加权后的总和作为目标的近似值。</p><h2 id=whats-wrong-with-seq2seq-model>What’s Wrong with Seq2Seq Model<a hidden class=anchor aria-hidden=true href=#whats-wrong-with-seq2seq-model>#</a></h2><p>seq2swq旨在将再encoder-decoder的架构下，将输入序列转换为新序列，对两个序列的长度没有要求。</p><ul><li>Encoder：将输入序列转换成固定长度的context vector，用来概括整个源序列的信息</li><li>Decoder：使用context vector初始化，来进行解码（转换）</li></ul><p>这样的Encoder和Decoder架构都是Recurrent Neural Networks，就像LSTM和GRU架构。</p><p><div class=post-img-view><a data-fancybox=gallery href=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210911182731.png><img alt=image-20210118214147224 loading=lazy src=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210911182731.png class=responsive-image src=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210911182731.png style="display:block;margin:0 auto" alt=image-20210118214147224></a></div><script>document.addEventListener("DOMContentLoaded",function(){var e=document.querySelectorAll(".responsive-image"),t=window.innerHeight/2.5;e.forEach(function(e){e.style.maxHeight=t+"px"})})</script></p><p>缺陷：固定长度的context vector可能会导致序列信息的缺失，同时可能会无法记住长句子，同时也会丢失<strong>时序的对齐</strong>信息。所以Attention就诞生了。</p><p>:question:</p><h2 id=born-for-translation>Born for Translation<a hidden class=anchor aria-hidden=true href=#born-for-translation>#</a></h2><p>这几个部分的研究都是基于NMT自然机器翻译，来进行分析的（文本非图像）</p><p><strong>原本的E-D</strong>是通过Encoder的最后一个hidden states构建单个Context Vector，而<strong>Attention</strong>
做的就是在Context Vec和Source之间建立一个Shortcut（简单的Feed Forword
Network），而源和目标的对齐由上下文向量来学习和控制。而上下文中应该consumes（包含）几个维度的信息</p><p>Encoder的hidden States；</p><p>Decoder的hidden States；</p><p>Source 和 Target之间的对齐信息；</p><p><div class=post-img-view><a data-fancybox=gallery href=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210911182734.png><img alt=image-20210119164039458 loading=lazy src=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210911182734.png class=responsive-image src=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210911182734.png style="display:block;margin:0 auto" alt=image-20210119164039458></a></div><script>document.addEventListener("DOMContentLoaded",function(){var e=document.querySelectorAll(".responsive-image"),t=window.innerHeight/2.5;e.forEach(function(e){e.style.maxHeight=t+"px"})})</script></p><p>Encoder Decoder实际上都是RNN的架构，S实际上就是H，隐层状态，Decoder对EOS输出了一个初始的预测Y以后，推进Decoder的进程。</p><p>双向的recurrent network（bidirectional RNN）使得Encoder隐态同时考虑前后单词。而在Decoder中</p><p>s_t = f(s_{t-1},y_{t-1},c_t)</p><p>其中上下文向量c_t是输入序列的隐态之和，通过alignment score来进行加权。</p><p><div class=post-img-view><a data-fancybox=gallery href=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210911182738.png><img alt=image-20210119171524970 loading=lazy src=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210911182738.png class=responsive-image src=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210911182738.png style="display:block;margin:0 auto" alt=image-20210119171524970></a></div><script>document.addEventListener("DOMContentLoaded",function(){var e=document.querySelectorAll(".responsive-image"),t=window.innerHeight/2.5;e.forEach(function(e){e.style.maxHeight=t+"px"})})</script></p><p>$a_{t,i}$ 将作为输入i对输出t的隐态加权（相关性），在网络中共同训练，通过tanh来进行激活处理。Score则使用下面的策略：</p><div>$$
score(st,hi)=v_a^Ttanh(Wa[st;hi])
$$</div><p>其中V, W都是对齐模型中学到的参数</p><p>最终对齐完成以后就会是：</p><p><div class=post-img-view><a data-fancybox=gallery href=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210911183010.png><img alt=image-20210120112808197 loading=lazy src=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210911183010.png class=responsive-image src=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210911183010.png style="display:block;margin:0 auto" alt=image-20210120112808197></a></div><script>document.addEventListener("DOMContentLoaded",function(){var e=document.querySelectorAll(".responsive-image"),t=window.innerHeight/2.5;e.forEach(function(e){e.style.maxHeight=t+"px"})})</script></p><h2 id=a-family-of-attention-mechanism>A Family of Attention Mechanism<a hidden class=anchor aria-hidden=true href=#a-family-of-attention-mechanism>#</a></h2><h3 id=summary>Summary<a hidden class=anchor aria-hidden=true href=#summary>#</a></h3><p>由于这个良好的理论基础和实现效果，以及对序列的良好兼容性，这样的算法就被很快的拓展到了计算机视觉的领域中。（学 科 交 叉）</p><p><div class=post-img-view><a data-fancybox=gallery href=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210911183207.png><img alt=image-20210120162959426 loading=lazy src=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210911183207.png class=responsive-image src=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210911183207.png style="display:block;margin:0 auto" alt=image-20210120162959426></a></div><script>document.addEventListener("DOMContentLoaded",function(){var e=document.querySelectorAll(".responsive-image"),t=window.innerHeight/2.5;e.forEach(function(e){e.style.maxHeight=t+"px"})})</script></p><p>这些不同的score方法实际上就是对每个position上的input和当前位置（通常用上一个时刻的state）来进行相关性建模，针对这些相关性建模，来确定输入对于下一个状态的影响因子，也就是得到一个context向量。这实际上就是注意力机制的核心理念把。</p><p>Referred to as “concat” in Luong, et al,
2015 and as “additive attention” in Vaswani, et al,
2017.(^) It adds a scaling factor 1/n−−√1/n,
motivated by the concern when the input is large, the
softmax function may have an extremely small gradient, hard for efficient learning.</p><p>下面对一些更广泛类型的注意力机制做一个摘要性的总结</p><h3 id=self-attention>Self-Attention<a hidden class=anchor aria-hidden=true href=#self-attention>#</a></h3><p>这个已经在我的Onenote中整理过了，大概看看就好。这里只讲了一些应用上的点，</p><p>自我注意，内部注意力，也就是自我内部关联的注意力关系获取；</p><p><div class=post-img-view><a data-fancybox=gallery href=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210911183010.png><img alt=image-20210120164912683 loading=lazy src=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210911183010.png class=responsive-image src=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210911183010.png style="display:block;margin:0 auto" alt=image-20210120164912683></a></div><script>document.addEventListener("DOMContentLoaded",function(){var e=document.querySelectorAll(".responsive-image"),t=window.innerHeight/2.5;e.forEach(function(e){e.style.maxHeight=t+"px"})})</script></p><h3 id=soft-vs-hard-attention>Soft vs Hard Attention<a hidden class=anchor aria-hidden=true href=#soft-vs-hard-attention>#</a></h3><p>Image Caption《show, attend and tell》，CNN获取特征，LSTM解码器利用卷积功能来逐一生成单词。通过Attention 来学习权重，可视化如下：</p><p><div class=post-img-view><a data-fancybox=gallery href=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210911183524.png><img alt=image-20210120165709998 loading=lazy src=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210911183524.png class=responsive-image src=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210911183524.png style="display:block;margin:0 auto" alt=image-20210120165709998></a></div><script>document.addEventListener("DOMContentLoaded",function(){var e=document.querySelectorAll(".responsive-image"),t=window.innerHeight/2.5;e.forEach(function(e){e.style.maxHeight=t+"px"})})</script></p><p>根据注意力是访问整个图像还是访问图像的一个patch来定义hard和soft attention。</p><p>**Soft Attention：**就是普通的全图Attention，将权重放在图像的所有色块上，基本没啥区别</p><ul><li>Pro: 模型平滑容易微分</li><li>Con：expensive when the source image is large</li></ul><p>**Hard Attention：**一次只处理一小块图像，其实就相当于用0/1去对图像区域进行<a href=https://www.pianshen.com/article/86961257993/ target=_blank rel=noopener>硬编码</a>
，只对一部分的图像进行处理，但是这种方式的技术实现我还是没什么概念，后面可以专门研究一下。</p><ul><li>Pro：推理计算需求比较小</li><li>Con：不可微分，需要用更复杂的技术来进行训练（例如variance reduction or reinforcement learning）</li></ul><h3 id=global-vs-loacal-attention>Global vs Loacal Attention<a hidden class=anchor aria-hidden=true href=#global-vs-loacal-attention>#</a></h3><p>Global实际上就和soft是类似的，而局部注意力机制更像是hard和soft之间的一个融合，对改进hard使其可以微分；</p><blockquote><p>the model first predicts a single aligned position for the current target word
and a window centered
around the source position is then used to compute a context vector.</p></blockquote><p><div class=post-img-view><a data-fancybox=gallery href=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210911183441.png><img alt=image-20210120171755507 loading=lazy src=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210911183441.png class=responsive-image src=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210911183441.png style="display:block;margin:0 auto" alt=image-20210120171755507></a></div><script>document.addEventListener("DOMContentLoaded",function(){var e=document.querySelectorAll(".responsive-image"),t=window.innerHeight/2.5;e.forEach(function(e){e.style.maxHeight=t+"px"})})</script></p><p>后面提到了一些神经图灵机的内容就是一些基本的计算机制，实际上可能是启发LSTM设计擦除算法设计的根源。</p><h2 id=neural-turing-machines>Neural Turing Machines<a hidden class=anchor aria-hidden=true href=#neural-turing-machines>#</a></h2><h3 id=attention-mechanisms>Attention Mechanisms<a hidden class=anchor aria-hidden=true href=#attention-mechanisms>#</a></h3><p>可以将权重的看作一个神经图灵机的寻址权重：（基于位置或者基于内容的两种方式）</p><p><strong>Content-based addressing:</strong></p><p>基于内容寻址的权重设置从输入行和存储行提取的键值向量之间的相似度来创建关注向量：权重的具体计算通过余弦相似度后进行softmax归一化来进行权重的分配。</p><p>另外通过β系数来放大或者减弱分布的焦点。</p><p><div class=post-img-view><a data-fancybox=gallery href=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210911183438.png><img alt=image-20210120215549507 loading=lazy src=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210911183438.png class=responsive-image src=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210911183438.png style="display:block;margin:0 auto" alt=image-20210120215549507></a></div><script>document.addEventListener("DOMContentLoaded",function(){var e=document.querySelectorAll(".responsive-image"),t=window.innerHeight/2.5;e.forEach(function(e){e.style.maxHeight=t+"px"})})</script></p><p><strong>Interpolation</strong>
通过interpolation gate scalar $g_t$ 将生成的上下文向量和上一步生成的注意力权重进行混合</p><div>$$
w^g_t = g_tw^c_t + (1-g_t)w_{t-1}
$$</div><p><strong>Location-baesd addressing</strong>
基于位置的寻址，对注意力向量中不同位置的值进行求和，通过对允许的整数偏移位置中的权重来进行参数加权。这相当于是一个一维卷积来测试偏移量。</p><p><div class=post-img-view><a data-fancybox=gallery href=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210911183435.png><img alt=image-20210120220824648 loading=lazy src=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210911183435.png class=responsive-image src=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210911183435.png style="display:block;margin:0 auto" alt=image-20210120220824648></a></div><script>document.addEventListener("DOMContentLoaded",function(){var e=document.querySelectorAll(".responsive-image"),t=window.innerHeight/2.5;e.forEach(function(e){e.style.maxHeight=t+"px"})})</script></p><p>然后对注意力的分布进行锐化处理，使用γ参数</p><p><div class=post-img-view><a data-fancybox=gallery href=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210911183433.png><img alt=image-20210120221036004 loading=lazy src=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210911183433.png class=responsive-image src=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210911183433.png style="display:block;margin:0 auto" alt=image-20210120221036004></a></div><script>document.addEventListener("DOMContentLoaded",function(){var e=document.querySelectorAll(".responsive-image"),t=window.innerHeight/2.5;e.forEach(function(e){e.style.maxHeight=t+"px"})})</script></p><p>完整的注意力workflow为：</p><p><div class=post-img-view><a data-fancybox=gallery href=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210911183429.png><img alt=image-20210120221108972 loading=lazy src=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210911183429.png class=responsive-image src=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210911183429.png style="display:block;margin:0 auto" alt=image-20210120221108972></a></div><script>document.addEventListener("DOMContentLoaded",function(){var e=document.querySelectorAll(".responsive-image"),t=window.innerHeight/2.5;e.forEach(function(e){e.style.maxHeight=t+"px"})})</script></p><h2 id=pointer-network>Pointer Network<a hidden class=anchor aria-hidden=true href=#pointer-network>#</a></h2><p>解决的是输入输出都是顺序数据的问题。</p><h2 id=transformer-introduction>Transformer Introduction<a hidden class=anchor aria-hidden=true href=#transformer-introduction>#</a></h2><p><strong>Attention is all u need</strong>
：提出的重要的Transformer的这种架构，使得算法能够完全基于注意力机制，而不需要序列对齐的recurrent Network。</p><h3 id=keyvalue-and-query>Key,Value and Query<a hidden class=anchor aria-hidden=true href=#keyvalue-and-query>#</a></h3><p><strong>这一块还是详细解读一下，这里说的太粗略了，没讲清楚。</strong></p><p>Transformer的主要重要的架构在于multi-head和self-attention，Transformer将输入堪称一个key-value pairs，均为输入长度n。</p><div>$$
Attention(Q,K,V)=softmax(\frac{Q K^⊤} {\sqrt{n}})V
$$</div><p>Key Value Query 到底都是些什么东西。</p><h2 id=encoder-decoder-and-taxonomy-of-attention>Encoder-Decoder and Taxonomy of Attention<a hidden class=anchor aria-hidden=true href=#encoder-decoder-and-taxonomy-of-attention>#</a></h2><p>基本的encoder-decoder是基于对序列处理的问题提出的，通常情况下针对的是输入输出都是序列的计算模型。下图a展示了<strong>典型的E-D机制</strong>，以及加入了self-attention的情况b。（文中提到了一些E-D机制的问题）</p><blockquote><p>Seq2Seq的RNN序列模型Encoder需要把之前的输入最终转化成单一的ht（定长），可能会造成序列信息的丢失，同时他只能<strong>平权的</strong>考虑输入，不能有所<strong>重点</strong>。同时对于<strong>时序对齐</strong>信息也会丢失，这对结构化特别重要</p></blockquote><p><div class=post-img-view><a data-fancybox=gallery href=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210911183425><img alt=img loading=lazy src=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210911183425 class=responsive-image src=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210911183425 style="display:block;margin:0 auto" alt=img></a></div><script>document.addEventListener("DOMContentLoaded",function(){var e=document.querySelectorAll(".responsive-image"),t=window.innerHeight/2.5;e.forEach(function(e){e.style.maxHeight=t+"px"})})</script></p><p>注意力机制，在实现上就是通过在体系架构中，嵌入一个额外的前馈神经网络，来进行学习额外的参数（作为Decoder的补充输入，作为序列和之前信息的补充），而且通常，这个前馈的神经网络是需要和encoder-decoder协同训练的，也就是需要和整体网络<strong>共同训练</strong>。</p><blockquote><p>训练层面是否存在一些特殊的情况，对于一些特殊的Attention 模型，是否需要一些特殊的训练机制。这点如果看到的话，建议需要整理一下</p></blockquote><p>在<strong>Survey</strong>中，对注意力机制基于多种标准进行了分类，具体的分类情况可以依下图所示，还有一些具体方法的实现。</p><p><div class=post-img-view><a data-fancybox=gallery href=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210911183358><img alt=img loading=lazy src=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210911183358 class=responsive-image src=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210911183358 style="display:block;margin:0 auto" alt=img></a></div><script>document.addEventListener("DOMContentLoaded",function(){var e=document.querySelectorAll(".responsive-image"),t=window.innerHeight/2.5;e.forEach(function(e){e.style.maxHeight=t+"px"})})</script></p><p><div class=post-img-view><a data-fancybox=gallery href=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210911183402.png><img alt=uploading-image-657692.png loading=lazy src=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210911183402.png class=responsive-image src=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210911183402.png style="display:block;margin:0 auto" alt=uploading-image-657692.png></a></div><script>document.addEventListener("DOMContentLoaded",function(){var e=document.querySelectorAll(".responsive-image"),t=window.innerHeight/2.5;e.forEach(function(e){e.style.maxHeight=t+"px"})})</script></p><p>对于几种分类方式，可以参考几个译文的解读，但是我觉得说的并不清楚，后续就分别针对各种方法进行分析吧。</p><p>BTW：在RNN等循环结构不能并行化处理的条件下，提出的类似AM的<strong>Transformer</strong>（Attention is all u
need）结构，他的encoder和decoder由两个子层堆叠而来：Feed Forward Network 和 Multi-head self attention。</p><blockquote><p>Position-wise FFN：获取输入的顺序，在encoder阶段，对于每一个token既生成content embedding也生成position encoding。</p><p>Multi-Head Self-Attention：在每个子层中使用self - attention来关联标记及其在相同输入序列中的位置，几个注意力层并行堆叠，对于相同的输入有不同的线性转换。这使得模型能够捕获输入的不同方面，提高表达能力。</p></blockquote><h2 id=transformer-and-self-attention>Transformer and Self-Attention<a hidden class=anchor aria-hidden=true href=#transformer-and-self-attention>#</a></h2><p>参考论文以及参考资料1 AND 《Attention is all you need》</p><p><strong>Self-Attention ：</strong></p><p>下图这一段整的明明白白，把整个框架说的比较明白，对输入的embedding（分别做3次卷积，图像输入的情况），分成Query，Key，Value，然后如图进行后续的操作</p><p><div class=post-img-view><a data-fancybox=gallery href=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210911183407.png><img alt=image-20200926224839989 loading=lazy src=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210911183407.png class=responsive-image src=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210911183407.png style="display:block;margin:0 auto" alt=image-20200926224839989></a></div><script>document.addEventListener("DOMContentLoaded",function(){var e=document.querySelectorAll(".responsive-image"),t=window.innerHeight/2.5;e.forEach(function(e){e.style.maxHeight=t+"px"})})</script><div class=post-img-view><a data-fancybox=gallery href=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210911183414.png><img alt=image-20200927160231116 loading=lazy src=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210911183414.png class=responsive-image src=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/imgs/img/20210911183414.png style="display:block;margin:0 auto" alt=image-20200927160231116></a></div><script>document.addEventListener("DOMContentLoaded",function(){var e=document.querySelectorAll(".responsive-image"),t=window.innerHeight/2.5;e.forEach(function(e){e.style.maxHeight=t+"px"})})</script></p><p>图2就表示了在CV领域，为什么需要将输入做完卷积以后再成进去，而其中的scale由于图片和序列是不一致的，他们的size本来就是统一的（基本规范的数据集中），那么就可以省略掉这一步，从而直接进行softmax，相当于在预处理的时候已经进行了这样的归一操作。</p><p>self-attention 是 Transformer的基础，通过<strong>多个Self-Attention组成的Multi-head Attention</strong> 就是Transformer模型的基本结构。</p><p>Multi-head Attention</p><h2 id=reference>Reference<a hidden class=anchor aria-hidden=true href=#reference>#</a></h2><p>参考资料</p><p><a href=https://zhuanlan.zhihu.com/p/47282410 target=_blank rel=noopener>Attention机制详解2：self-attention & Transformer</a></p><p><a href=https://jalammar.github.io/illustrated-transformer/ target=_blank rel=noopener>origin document of ↑</a></p><p><a href=https://www.jianshu.com/p/8115c7cebc59 target=_blank rel=noopener>Attention的数学原理</a></p><p>Survey:</p><ul><li><p>《An Attentive Survey of Attention Models》 mendeley</p></li><li><p><a href=https://www.cnblogs.com/ydcode/p/11040811.html target=_blank rel=noopener>论文翻译和解读1</a>
:zap:</p></li><li><p><a href=https://blog.csdn.net/qq_29305911/article/details/89380387 target=_blank rel=noopener>论文解读和翻译2</a></p></li><li><p><a href=https://zhuanlan.zhihu.com/p/62445564 target=_blank rel=noopener>论文解读和翻译3</a></p></li></ul><p>Transformer:</p><ul><li>《Attention is all you need》Mendeley &amp;OneNote</li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://hugotest-phi.vercel.app/tags/machine-learning/>Machine Learning</a></li><li><a href=https://hugotest-phi.vercel.app/tags/attention/>Attention</a></li></ul><nav class=paginav><a class=prev href=https://hugotest-phi.vercel.app/posts/ow-od/><span class=title>« Prev</span><br><span>OW Object Detector</span>
</a><a class=next href=https://hugotest-phi.vercel.app/posts/efficientnet/><span class=title>Next »</span><br><span>EfficientNet</span></a></nav></footer><div id=disqus_thread></div><script>function loadDisqus(){var e=document,t=e.createElement("script");t.src="https://aiken-hugo.disqus.com/embed.js",t.setAttribute("data-timestamp",+new Date),(e.head||e.body).appendChild(t),window.disqus_config=function(){this.page.url=window.location.href,this.page.identifier=window.location.href.substring(18)}}var runningOnBrowser=typeof window!="undefined",isBot=runningOnBrowser&&!("onscroll"in window)||typeof navigator!="undefined"&&/(gle|ing|ro|msn)bot|crawl|spider|yand|duckgo/i.test(navigator.userAgent),supportsIntersectionObserver=runningOnBrowser&&"IntersectionObserver"in window;setTimeout(function(){if(!isBot&&supportsIntersectionObserver){var e=new IntersectionObserver(function(t){t[0].isIntersecting&&(loadDisqus(),e.disconnect())},{threshold:[0]});e.observe(document.getElementById("disqus_thread"))}else loadDisqus()},1)</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by
Disqus.</a></noscript></article></main><footer class=footer><span>&copy; 2024 <a href=https://hugotest-phi.vercel.app/>aiken's blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a>
</span><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><span id=busuanzi_container>Visitors: <span id=busuanzi_value_site_uv></span>
Views: <span id=busuanzi_value_site_pv></span></span></footer><script>document.addEventListener("DOMContentLoaded",function(){const e=document.getElementById("busuanzi_value_site_uv"),t=document.getElementById("busuanzi_value_site_pv"),o=13863,i=16993;if(!e||!t){console.error("Busuanzi elements not found.");return}const n=new MutationObserver(e=>{for(let t of e)if(t.type==="childList"){n.disconnect(),t.target.innerHTML=parseInt(t.target.innerHTML||0)+o;break}}),s=new MutationObserver(e=>{for(let t of e)if(t.type==="childList"){s.disconnect(),t.target.innerHTML=parseInt(t.target.innerHTML||0)+i;break}});n.observe(e,{childList:!0}),s.observe(t,{childList:!0})})</script><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><span class=topInner><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
<span id=read_progress></span>
</span></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))}),document.getElementById("theme-toggle-nav").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script><script>document.addEventListener("scroll",function(){const t=document.getElementById("read_progress"),n=document.documentElement.scrollHeight,s=document.documentElement.clientHeight,o=document.documentElement.scrollTop||document.body.scrollTop;t.innerText=((o/(n-s)).toFixed(2)*100).toFixed(0)})</script><script>(function(e,t){var s=document,o="script",n=s.createElement(o),i=s.getElementsByTagName(o)[0];n.src=e,t&&n.addEventListener("load",function(e){t(e)}),i.parentNode.insertBefore(n,i)})("/js/pangu.js",function(){pangu.spacingPage()})</script></body></html>