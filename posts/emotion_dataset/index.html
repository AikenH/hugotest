<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>表情数据集 | aiken's blog</title>
<meta name=keywords content="Dataset,Emotion"><meta name=description content="根据这次需要搜集的表情的数据集，整理一下搜索数据集的网站和思路等
##PART1 &ldquo;表情数据集&rdquo;
下列是对数据搜集的要求：

是否开源
图片的大小和数量
图片的采集方式

eg：ck+
==数据来源及对应的搜索结果如下：==

谷歌数据集搜索导航

60个人脸识别的数据集汇总

cv方面的好几百个数据集汇总

另一个cv方向的数据集汇总

github-CV汇总帖



EmotioNet



好像是一个什么挑战赛的数据集要博士后或者相应教员才能申请使用申请页面
   
没有具体的用于表情识别的数据子集的信息（好像数据很多，但是不知道在哪下，除了那个博士后申请的）


RAF



real-world Affective Face
数据量29672个图像，7种基本情绪，12种复合情绪，（包含种族年龄范围性别属性，5个准确定位和37个自动生成的定位）
**数据收集方式：**来源网络，大小应该很杂 （由40个人独立标定）
email


onenote中标记的和google 数据集搜索

FaceTracer Database

basic info：（有图片的原始url）（wild）(网上收集的)姿势、环境、照明、质量 等等参差不齐，大小不固定
(针对非商业用途开放)   (表情只有笑容)
故而不在详细收集，其他的标注信息，文中有详细讲解。
Tencent ML-Images

可能会有表情吧，是一个很大规模的多标签数据集。。。
ND-2006 Dataset
 06年貌似
13450张图片
6种基本情感
888个对象
Google facial expression comparison dataset

没有对数据集的基本信息介绍

百度/CSDN搜索

https://blog.csdn.net/mathlxj/article/details/87920084

https://blog.csdn.net/computerme/article/details/49469767



JAFFE

只有219张，标签为分散离散值。
划分六种情感指标  256*256
中科大的NVIE

其中正面光照103人，左侧光照99人，右侧光照103人。每种光照下，每人有六种表情（喜悦、愤怒、哀 伤、恐惧、厌恶、惊奇）中的三种以上
平静帧、最大帧都已挑出 下载协议然后发给他们，才能下载

AFEW database

数据来源：电影片段的剪辑。情绪类型：“六类基本表情”+中性
SFEW database

数据来源：从AFEW中抽取出来的表情的静态帧。标注都在xml中
LIRIS-ACCEDE database

同样也是基于电影抽取的，有三种数据集，包含离散的情感数据和基于维度的情感数据
BU-3DFE database

3D的人脸表情数据集 **数据来源：**找人来做实验采集，按照要求的情绪做出表情
**数据量：**2500个3d面部模型（来自100个人）
还有同类的一些包含序列的等等的数据集，估计差别不大。
同样需要email获取
Oulu-CASIA database

数据来源：让80个受试者做出相应的表情并用不同相机采集（红外可见光正常光和弱光）
情绪类型：快乐、悲伤、惊讶、愤怒、恐惧、厌恶
email
RAFD

数据来源：让67个受试者做出相应的表情在不同注视点和不同角度采集
情绪类型：8种情感类型
email
KDEF database

数据来源：柔和、均匀的光线，多角度拍摄表情，使用统一的T恤颜色，在拍摄过程中使用网格将参与者面部居中，以及在扫描过程中将眼睛和嘴巴定位在固定的图像坐标中。
数据量：4900张 (70个人，一个7个情感)
页面底端超链接（没进去成功。。）
ExpW

9w张左右，图片差不多8G
AffectNet

百万量级数据（Emotion Net好像也是）
获取方式：从互联网获取
7类情感，首页有各种情感的数据量，最少的也有4k张
填写申请表email下载
Multi-PIE Face Database

收钱给数据集
获取方式：记录会话
数据量：75w图片

一些视频数据集(具体的在CSDN站上)（这些我就没有去详细看了）"><meta name=author content="aikenhong"><link rel=canonical href=https://aikenh.cn/hugotest/posts/emotion_dataset/><link crossorigin=anonymous href=/hugotest/assets/css/stylesheet.2f85ca17c12c62fa86b1e474b8a51aca4856f0d645debfe4922a4d5ddc6aa978.css integrity="sha256-L4XKF8EsYvqGseR0uKUaykhW8NZF3r/kkipNXdxqqXg=" rel="preload stylesheet" as=style><link rel=icon href=https://aikenh.cn/favicon/ghost.ico><link rel=icon type=image/png sizes=16x16 href=https://aikenh.cn/favicon/ghost-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://aikenh.cn/favicon/ghost-32x32.png><link rel=apple-touch-icon href=https://aikenh.cn/favicon/ghost-apple-touch-icon.png><link rel=mask-icon href=https://aikenh.cn/favicon/ghost-192x192.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://aikenh.cn/hugotest/posts/emotion_dataset/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script src=https://cdn.jsdmirror.com/npm/jquery@3.5.1/dist/jquery.min.js></script><link rel=stylesheet href=https://cdn.jsdmirror.com/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css><script src=https://cdn.jsdmirror.com/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js></script><link rel=stylesheet href=https://cdn.jsdmirror.com/npm/katex@0.16.11/dist/katex.min.css><script defer src=https://cdn.jsdmirror.com/npm/katex@0.16.11/dist/katex.min.js></script><script defer src=https://cdn.jsdmirror.com/npm/katex@0.16.11/dist/contrib/auto-render.min.js onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><link rel=stylesheet href=https://cdn.jsdmirror.com/npm/lxgw-wenkai-webfont@1.1.0/style.css><link rel=stylesheet href=https://cdn.jsdmirror.com/npm/lxgw-wenkai-lite-webfont@1.1.0/style.css><link rel=stylesheet href=https://cdn.jsdmirror.com/npm/lxgw-wenkai-tc-webfont@1.0.0/style.css><link rel=stylesheet href=https://cdn.jsdmirror.com/npm/lxgw-wenkai-screen-webfont@1.1.0/style.css><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Open+Sans:ital,wght@0,300..800;1,300..800&family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&family=Ubuntu+Mono:ital,wght@0,400;0,700;1,400;1,700&family=Ubuntu:ital,wght@0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><meta property="og:url" content="https://aikenh.cn/hugotest/posts/emotion_dataset/"><meta property="og:site_name" content="aiken's blog"><meta property="og:title" content="表情数据集"><meta property="og:description" content="根据这次需要搜集的表情的数据集，整理一下搜索数据集的网站和思路等
##PART1 “表情数据集” 下列是对数据搜集的要求：
是否开源 图片的大小和数量 图片的采集方式 eg：ck+
==数据来源及对应的搜索结果如下：==
谷歌数据集搜索导航 60个人脸识别的数据集汇总 cv方面的好几百个数据集汇总 另一个cv方向的数据集汇总 github-CV汇总帖 EmotioNet 好像是一个什么挑战赛的数据集要博士后或者相应教员才能申请使用申请页面 没有具体的用于表情识别的数据子集的信息（好像数据很多，但是不知道在哪下，除了那个博士后申请的）
RAF real-world Affective Face
数据量29672个图像，7种基本情绪，12种复合情绪，（包含种族年龄范围性别属性，5个准确定位和37个自动生成的定位）
**数据收集方式：**来源网络，大小应该很杂 （由40个人独立标定）
email
onenote中标记的和google 数据集搜索
FaceTracer Database basic info：（有图片的原始url）（wild）(网上收集的)姿势、环境、照明、质量 等等参差不齐，大小不固定 (针对非商业用途开放) (表情只有笑容) 故而不在详细收集，其他的标注信息，文中有详细讲解。 Tencent ML-Images 可能会有表情吧，是一个很大规模的多标签数据集。。。 ND-2006 Dataset 06年貌似 13450张图片 6种基本情感 888个对象 Google facial expression comparison dataset 没有对数据集的基本信息介绍 百度/CSDN搜索
https://blog.csdn.net/mathlxj/article/details/87920084 https://blog.csdn.net/computerme/article/details/49469767 JAFFE 只有219张，标签为分散离散值。 划分六种情感指标 256*256 中科大的NVIE 其中正面光照103人，左侧光照99人，右侧光照103人。每种光照下，每人有六种表情（喜悦、愤怒、哀 伤、恐惧、厌恶、惊奇）中的三种以上 平静帧、最大帧都已挑出 下载协议然后发给他们，才能下载 AFEW database 数据来源：电影片段的剪辑。情绪类型：“六类基本表情”+中性 SFEW database 数据来源：从AFEW中抽取出来的表情的静态帧。标注都在xml中 LIRIS-ACCEDE database 同样也是基于电影抽取的，有三种数据集，包含离散的情感数据和基于维度的情感数据 BU-3DFE database 3D的人脸表情数据集 **数据来源：**找人来做实验采集，按照要求的情绪做出表情 **数据量：**2500个3d面部模型（来自100个人） 还有同类的一些包含序列的等等的数据集，估计差别不大。 同样需要email获取 Oulu-CASIA database 数据来源：让80个受试者做出相应的表情并用不同相机采集（红外可见光正常光和弱光） 情绪类型：快乐、悲伤、惊讶、愤怒、恐惧、厌恶 email RAFD 数据来源：让67个受试者做出相应的表情在不同注视点和不同角度采集 情绪类型：8种情感类型 email KDEF database 数据来源：柔和、均匀的光线，多角度拍摄表情，使用统一的T恤颜色，在拍摄过程中使用网格将参与者面部居中，以及在扫描过程中将眼睛和嘴巴定位在固定的图像坐标中。 数据量：4900张 (70个人，一个7个情感) 页面底端超链接（没进去成功。。） ExpW 9w张左右，图片差不多8G AffectNet 百万量级数据（Emotion Net好像也是） 获取方式：从互联网获取 7类情感，首页有各种情感的数据量，最少的也有4k张 填写申请表email下载 Multi-PIE Face Database 收钱给数据集 获取方式：记录会话 数据量：75w图片 一些视频数据集(具体的在CSDN站上)（这些我就没有去详细看了）"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2019-10-14T21:34:54+00:00"><meta property="article:modified_time" content="2019-10-14T21:34:54+00:00"><meta property="article:tag" content="Dataset"><meta property="article:tag" content="Emotion"><meta property="og:image" content="https://aikenh.cn/cover/cover20.jpeg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://aikenh.cn/cover/cover20.jpeg"><meta name=twitter:title content="表情数据集"><meta name=twitter:description content="根据这次需要搜集的表情的数据集，整理一下搜索数据集的网站和思路等
##PART1 &ldquo;表情数据集&rdquo;
下列是对数据搜集的要求：

是否开源
图片的大小和数量
图片的采集方式

eg：ck+
==数据来源及对应的搜索结果如下：==

谷歌数据集搜索导航

60个人脸识别的数据集汇总

cv方面的好几百个数据集汇总

另一个cv方向的数据集汇总

github-CV汇总帖



EmotioNet



好像是一个什么挑战赛的数据集要博士后或者相应教员才能申请使用申请页面
   
没有具体的用于表情识别的数据子集的信息（好像数据很多，但是不知道在哪下，除了那个博士后申请的）


RAF



real-world Affective Face
数据量29672个图像，7种基本情绪，12种复合情绪，（包含种族年龄范围性别属性，5个准确定位和37个自动生成的定位）
**数据收集方式：**来源网络，大小应该很杂 （由40个人独立标定）
email


onenote中标记的和google 数据集搜索

FaceTracer Database

basic info：（有图片的原始url）（wild）(网上收集的)姿势、环境、照明、质量 等等参差不齐，大小不固定
(针对非商业用途开放)   (表情只有笑容)
故而不在详细收集，其他的标注信息，文中有详细讲解。
Tencent ML-Images

可能会有表情吧，是一个很大规模的多标签数据集。。。
ND-2006 Dataset
 06年貌似
13450张图片
6种基本情感
888个对象
Google facial expression comparison dataset

没有对数据集的基本信息介绍

百度/CSDN搜索

https://blog.csdn.net/mathlxj/article/details/87920084

https://blog.csdn.net/computerme/article/details/49469767



JAFFE

只有219张，标签为分散离散值。
划分六种情感指标  256*256
中科大的NVIE

其中正面光照103人，左侧光照99人，右侧光照103人。每种光照下，每人有六种表情（喜悦、愤怒、哀 伤、恐惧、厌恶、惊奇）中的三种以上
平静帧、最大帧都已挑出 下载协议然后发给他们，才能下载

AFEW database

数据来源：电影片段的剪辑。情绪类型：“六类基本表情”+中性
SFEW database

数据来源：从AFEW中抽取出来的表情的静态帧。标注都在xml中
LIRIS-ACCEDE database

同样也是基于电影抽取的，有三种数据集，包含离散的情感数据和基于维度的情感数据
BU-3DFE database

3D的人脸表情数据集 **数据来源：**找人来做实验采集，按照要求的情绪做出表情
**数据量：**2500个3d面部模型（来自100个人）
还有同类的一些包含序列的等等的数据集，估计差别不大。
同样需要email获取
Oulu-CASIA database

数据来源：让80个受试者做出相应的表情并用不同相机采集（红外可见光正常光和弱光）
情绪类型：快乐、悲伤、惊讶、愤怒、恐惧、厌恶
email
RAFD

数据来源：让67个受试者做出相应的表情在不同注视点和不同角度采集
情绪类型：8种情感类型
email
KDEF database

数据来源：柔和、均匀的光线，多角度拍摄表情，使用统一的T恤颜色，在拍摄过程中使用网格将参与者面部居中，以及在扫描过程中将眼睛和嘴巴定位在固定的图像坐标中。
数据量：4900张 (70个人，一个7个情感)
页面底端超链接（没进去成功。。）
ExpW

9w张左右，图片差不多8G
AffectNet

百万量级数据（Emotion Net好像也是）
获取方式：从互联网获取
7类情感，首页有各种情感的数据量，最少的也有4k张
填写申请表email下载
Multi-PIE Face Database

收钱给数据集
获取方式：记录会话
数据量：75w图片

一些视频数据集(具体的在CSDN站上)（这些我就没有去详细看了）"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://aikenh.cn/hugotest/posts/"},{"@type":"ListItem","position":2,"name":"表情数据集","item":"https://aikenh.cn/hugotest/posts/emotion_dataset/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"表情数据集","name":"表情数据集","description":"根据这次需要搜集的表情的数据集，整理一下搜索数据集的网站和思路等\n##PART1 \u0026ldquo;表情数据集\u0026rdquo; 下列是对数据搜集的要求：\n是否开源 图片的大小和数量 图片的采集方式 eg：ck+\n==数据来源及对应的搜索结果如下：==\n谷歌数据集搜索导航 60个人脸识别的数据集汇总 cv方面的好几百个数据集汇总 另一个cv方向的数据集汇总 github-CV汇总帖 EmotioNet 好像是一个什么挑战赛的数据集要博士后或者相应教员才能申请使用申请页面 没有具体的用于表情识别的数据子集的信息（好像数据很多，但是不知道在哪下，除了那个博士后申请的）\nRAF real-world Affective Face\n数据量29672个图像，7种基本情绪，12种复合情绪，（包含种族年龄范围性别属性，5个准确定位和37个自动生成的定位）\n**数据收集方式：**来源网络，大小应该很杂 （由40个人独立标定）\nemail\nonenote中标记的和google 数据集搜索\nFaceTracer Database basic info：（有图片的原始url）（wild）(网上收集的)姿势、环境、照明、质量 等等参差不齐，大小不固定 (针对非商业用途开放) (表情只有笑容) 故而不在详细收集，其他的标注信息，文中有详细讲解。 Tencent ML-Images 可能会有表情吧，是一个很大规模的多标签数据集。。。 ND-2006 Dataset 06年貌似 13450张图片 6种基本情感 888个对象 Google facial expression comparison dataset 没有对数据集的基本信息介绍 百度/CSDN搜索\nhttps://blog.csdn.net/mathlxj/article/details/87920084 https://blog.csdn.net/computerme/article/details/49469767 JAFFE 只有219张，标签为分散离散值。 划分六种情感指标 256*256 中科大的NVIE 其中正面光照103人，左侧光照99人，右侧光照103人。每种光照下，每人有六种表情（喜悦、愤怒、哀 伤、恐惧、厌恶、惊奇）中的三种以上 平静帧、最大帧都已挑出 下载协议然后发给他们，才能下载 AFEW database 数据来源：电影片段的剪辑。情绪类型：“六类基本表情”+中性 SFEW database 数据来源：从AFEW中抽取出来的表情的静态帧。标注都在xml中 LIRIS-ACCEDE database 同样也是基于电影抽取的，有三种数据集，包含离散的情感数据和基于维度的情感数据 BU-3DFE database 3D的人脸表情数据集 **数据来源：**找人来做实验采集，按照要求的情绪做出表情 **数据量：**2500个3d面部模型（来自100个人） 还有同类的一些包含序列的等等的数据集，估计差别不大。 同样需要email获取 Oulu-CASIA database 数据来源：让80个受试者做出相应的表情并用不同相机采集（红外可见光正常光和弱光） 情绪类型：快乐、悲伤、惊讶、愤怒、恐惧、厌恶 email RAFD 数据来源：让67个受试者做出相应的表情在不同注视点和不同角度采集 情绪类型：8种情感类型 email KDEF database 数据来源：柔和、均匀的光线，多角度拍摄表情，使用统一的T恤颜色，在拍摄过程中使用网格将参与者面部居中，以及在扫描过程中将眼睛和嘴巴定位在固定的图像坐标中。 数据量：4900张 (70个人，一个7个情感) 页面底端超链接（没进去成功。。） ExpW 9w张左右，图片差不多8G AffectNet 百万量级数据（Emotion Net好像也是） 获取方式：从互联网获取 7类情感，首页有各种情感的数据量，最少的也有4k张 填写申请表email下载 Multi-PIE Face Database 收钱给数据集 获取方式：记录会话 数据量：75w图片 一些视频数据集(具体的在CSDN站上)（这些我就没有去详细看了）\n","keywords":["Dataset","Emotion"],"articleBody":"根据这次需要搜集的表情的数据集，整理一下搜索数据集的网站和思路等\n##PART1 “表情数据集” 下列是对数据搜集的要求：\n是否开源 图片的大小和数量 图片的采集方式 eg：ck+\n==数据来源及对应的搜索结果如下：==\n谷歌数据集搜索导航 60个人脸识别的数据集汇总 cv方面的好几百个数据集汇总 另一个cv方向的数据集汇总 github-CV汇总帖 EmotioNet 好像是一个什么挑战赛的数据集要博士后或者相应教员才能申请使用申请页面 没有具体的用于表情识别的数据子集的信息（好像数据很多，但是不知道在哪下，除了那个博士后申请的）\nRAF real-world Affective Face\n数据量29672个图像，7种基本情绪，12种复合情绪，（包含种族年龄范围性别属性，5个准确定位和37个自动生成的定位）\n**数据收集方式：**来源网络，大小应该很杂 （由40个人独立标定）\nemail\nonenote中标记的和google 数据集搜索\nFaceTracer Database basic info：（有图片的原始url）（wild）(网上收集的)姿势、环境、照明、质量 等等参差不齐，大小不固定 (针对非商业用途开放) (表情只有笑容) 故而不在详细收集，其他的标注信息，文中有详细讲解。 Tencent ML-Images 可能会有表情吧，是一个很大规模的多标签数据集。。。 ND-2006 Dataset 06年貌似 13450张图片 6种基本情感 888个对象 Google facial expression comparison dataset 没有对数据集的基本信息介绍 百度/CSDN搜索\nhttps://blog.csdn.net/mathlxj/article/details/87920084 https://blog.csdn.net/computerme/article/details/49469767 JAFFE 只有219张，标签为分散离散值。 划分六种情感指标 256*256 中科大的NVIE 其中正面光照103人，左侧光照99人，右侧光照103人。每种光照下，每人有六种表情（喜悦、愤怒、哀 伤、恐惧、厌恶、惊奇）中的三种以上 平静帧、最大帧都已挑出 下载协议然后发给他们，才能下载 AFEW database 数据来源：电影片段的剪辑。情绪类型：“六类基本表情”+中性 SFEW database 数据来源：从AFEW中抽取出来的表情的静态帧。标注都在xml中 LIRIS-ACCEDE database 同样也是基于电影抽取的，有三种数据集，包含离散的情感数据和基于维度的情感数据 BU-3DFE database 3D的人脸表情数据集 **数据来源：**找人来做实验采集，按照要求的情绪做出表情 **数据量：**2500个3d面部模型（来自100个人） 还有同类的一些包含序列的等等的数据集，估计差别不大。 同样需要email获取 Oulu-CASIA database 数据来源：让80个受试者做出相应的表情并用不同相机采集（红外可见光正常光和弱光） 情绪类型：快乐、悲伤、惊讶、愤怒、恐惧、厌恶 email RAFD 数据来源：让67个受试者做出相应的表情在不同注视点和不同角度采集 情绪类型：8种情感类型 email KDEF database 数据来源：柔和、均匀的光线，多角度拍摄表情，使用统一的T恤颜色，在拍摄过程中使用网格将参与者面部居中，以及在扫描过程中将眼睛和嘴巴定位在固定的图像坐标中。 数据量：4900张 (70个人，一个7个情感) 页面底端超链接（没进去成功。。） ExpW 9w张左右，图片差不多8G AffectNet 百万量级数据（Emotion Net好像也是） 获取方式：从互联网获取 7类情感，首页有各种情感的数据量，最少的也有4k张 填写申请表email下载 Multi-PIE Face Database 收钱给数据集 获取方式：记录会话 数据量：75w图片 一些视频数据集(具体的在CSDN站上)（这些我就没有去详细看了）\nHUMAINE Database 应为带表情标签的视频数据集（CSDN用户表示下载后没有标签）(我翻墙也进不去很奇怪) Recola database MMI RU-FACS database-未公开 Belfast naturalistic database -主要是演讲时候的情感识别 VAM corpus 也是演讲的 AVEC系列数据集 ","wordCount":"124","inLanguage":"en","image":"https://aikenh.cn/cover/cover20.jpeg","datePublished":"2019-10-14T21:34:54Z","dateModified":"2019-10-14T21:34:54Z","author":[{"@type":"Person","name":"aikenhong"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://aikenh.cn/hugotest/posts/emotion_dataset/"},"publisher":{"@type":"Organization","name":"aiken's blog","logo":{"@type":"ImageObject","url":"https://aikenh.cn/favicon/ghost.ico"}}}</script></head><body id=top><script type=module src=https://cdn.jsdmirror.com/npm/ionicons@7.1.0/dist/ionicons/ionicons.esm.js defer></script><script nomodule src=https://cdn.jsdmirror.com/npm/ionicons@7.1.0/dist/ionicons/ionicons.js defer></script><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://aikenh.cn/hugotest/ accesskey=h title="aiken's blog (Alt + H)">aiken's blog</a><div class=logo-switches><button id=theme-toggle-nav accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://aikenh.cn/hugotest/ title=home><span>home</span></a></li><li><a href=https://aikenh.cn/hugotest/posts/ title=posts><span>posts</span></a></li><li><a href=https://aikenh.cn/hugotest/tags/ title=tags><span>tags</span></a></li><li><a href=https://aikenh.cn/hugotest/categories/ title=categories><span>categories</span></a></li><li><a href=https://aikenh.cn/hugotest/archives/ title=archives><span>archives</span></a></li><li><a href=https://aikenh.cn/hugotest/about/ title=about><span>about</span></a></li><li><a href=https://aikenh.cn/hugotest/search title="search (Alt + /)" accesskey=/><span>search</span></a></li></ul></nav></header><div class=sidebar><ul><li class=logo style=--bg:#333><a href=#><div class=logo-icon><img src=/logo/logo.png></div><div class=logo-text>Aiken's Blog</div></a></li><div class=menulist><li style=--bg:#f44336><a href=https://aikenh.cn/hugotest/ title=home><div class=logo-icon><ion-icon name=home-outline></ion-icon></div><div class=logo-text>home</div></a></li><li style=--bg:#b145e9><a href=https://aikenh.cn/hugotest/posts/ title=posts><div class=logo-icon><ion-icon name=newspaper-outline></ion-icon></div><div class=logo-text>posts</div></a></li><li style=--bg:#0f93c7><a href=https://aikenh.cn/hugotest/tags/ title=tags><div class=logo-icon><ion-icon name=pricetags-outline></ion-icon></div><div class=logo-text>tags</div></a></li><li style=--bg:#ffa117><a href=https://aikenh.cn/hugotest/categories/ title=categories><div class=logo-icon><ion-icon name=grid-outline></ion-icon></div><div class=logo-text>categories</div></a></li><li style=--bg:#0fc70f><a href=https://aikenh.cn/hugotest/archives/ title=archives><div class=logo-icon><ion-icon name=folder-outline></ion-icon></div><div class=logo-text>archives</div></a></li><li style=--bg:#d16111><a href=https://aikenh.cn/hugotest/about/ title=about><div class=logo-icon><ion-icon name=person></ion-icon></div><div class=logo-text>about</div></a></li><li style=--bg:#15c095><a href=https://aikenh.cn/hugotest/search title="search (Alt + /)" accesskey=/><div class=logo-icon><ion-icon name=search></ion-icon></div><div class=logo-text>search</div></a></li></div><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt +T)"><li><div class=logo-icon id=moon><ion-icon name=moon-outline></ion-icon></div><div class=logo-icon id=sun><ion-icon name=sunny-outline></ion-icon></div></li></button></div></ul></div><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://aikenh.cn/hugotest/>Home</a>&nbsp;»&nbsp;<a href=https://aikenh.cn/hugotest/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">表情数据集</h1><div class=post-meta><span title='2019-10-14 21:34:54 +0000 UTC'>October 14, 2019</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;124 words&nbsp;·&nbsp;aikenhong&nbsp;·&nbsp;<a href=/tags/dataset> Dataset</a>&nbsp;·&nbsp;<a href=/tags/emotion> Emotion</a>&nbsp;|&nbsp;<a href=https://github.com/%3cpath_to_repo%3e/content/posts/Emotion_dataset.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><figure class=entry-cover><img loading=eager src=https://aikenh.cn/cover/cover20.jpeg alt></figure><div class=post-content><p>根据这次需要搜集的表情的数据集，整理一下搜索数据集的网站和思路等</p><p>##PART1 &ldquo;表情数据集&rdquo;
下列是对数据搜集的要求：</p><ul><li>是否开源</li><li>图片的大小和数量</li><li>图片的采集方式</li></ul><p>eg：<strong>ck+</strong></p><p>==数据来源及对应的搜索结果如下：==</p><ul><li><a href=https://toolbox.google.com/datasetsearch target=_blank rel=noopener>谷歌数据集搜索导航</a></li><li><a href=https://www.kairos.com/blog/60-facial-recognition-databases target=_blank rel=noopener>60个人脸识别的数据集汇总</a></li><li><a href=http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm target=_blank rel=noopener>cv方面的好几百个数据集汇总</a></li><li><a href=www.cvpapers.com/datasets.html>另一个cv方向的数据集汇总</a></li><li><a href=https://github.com/ChanChiChoi/awesome-Face_Recognition target=_blank rel=noopener>github-CV汇总帖</a></li></ul><ol><li><a href=http://cbcsl.ece.ohio-state.edu/EmotionNetChallenge/index.html#overview target=_blank rel=noopener>EmotioNet</a></li></ol><blockquote><p>好像是一个什么挑战赛的数据集要博士后或者相应教员才能申请使用<a href=http://cbcsl.ece.ohio-state.edu/dbform_compound.html target=_blank rel=noopener>申请页面</a><br>没有具体的用于表情识别的数据子集的信息（好像数据很多，但是不知道在哪下，除了那个博士后申请的）</p></blockquote><ol start=2><li><a href=http://www.whdeng.cn/RAF/model1.html target=_blank rel=noopener>RAF</a></li></ol><blockquote><p>real-world Affective Face<br><strong>数据量</strong>29672个图像，7种基本情绪，12种复合情绪，（包含种族年龄范围性别属性，5个准确定位和37个自动生成的定位）<br>**数据收集方式：**来源网络，大小应该很杂 （由40个人独立标定）<br>email</p></blockquote><p><strong>onenote中标记的和google 数据集搜索</strong></p><ol><li><a href=http://www.cs.columbia.edu/CAVE/databases/facetracer/ target=_blank rel=noopener>FaceTracer Database</a>
basic info：（有图片的原始url）（wild）(网上收集的)姿势、环境、照明、质量 等等参差不齐，<strong>大小不固定</strong>
(针对<strong>非商业用途开放</strong>) (<strong>表情只有笑容</strong>)
故而不在详细收集，其他的标注信息，文中有详细讲解。</li><li><a href="https://github.com/Tencent/tencent-ml-images?tdsourcetag=s_pctim_aiomsg#tencent-ml-images" target=_blank rel=noopener>Tencent ML-Images</a>
可能会有表情吧，是一个很大规模的多标签数据集。。。</li><li><a href=https://cvrl.nd.edu/projects/data/#nd-2006-data-set target=_blank rel=noopener>ND-2006 Dataset</a>
06年貌似
13450张图片
6种基本情感
888个对象</li><li><a href=https://ai.google/tools/datasets/google-facial-expression/ target=_blank rel=noopener>Google facial expression comparison dataset</a>
没有对数据集的基本信息介绍</li></ol><p><strong>百度/CSDN搜索</strong></p><ul><li><a href=https://blog.csdn.net/mathlxj/article/details/87920084 target=_blank rel=noopener>https://blog.csdn.net/mathlxj/article/details/87920084</a></li><li><a href=https://blog.csdn.net/computerme/article/details/49469767 target=_blank rel=noopener>https://blog.csdn.net/computerme/article/details/49469767</a></li></ul><ol><li><a href=https://zenodo.org/record/3451524#.XaQ4vm5uKmQ target=_blank rel=noopener>JAFFE</a>
只有219张，标签为分散离散值。
划分六种情感指标 256*256</li><li>中科大的<a href=http://nvie.ustc.edu.cn/ target=_blank rel=noopener>NVIE</a>
其中<strong>正面光照103人，左侧光照99人，右侧光照103人</strong>。每种光照下，每人有六种表情（<strong>喜悦、愤怒、哀 伤、恐惧、厌恶、惊奇</strong>）中的三种以上
平静帧、最大帧都已挑出 <a href=http://nvie.ustc.edu.cn/contact.html target=_blank rel=noopener>下载协议然后发给他们，才能下载</a></li><li><a href=https://sites.google.com/site/emotiwchallenge/ target=_blank rel=noopener>AFEW database</a>
<strong>数据来源</strong>：电影片段的剪辑。<strong>情绪类型</strong>：“六类基本表情”+中性
<a href=https://cs.anu.edu.au/few/emotiw2015.html target=_blank rel=noopener>SFEW database</a>
<strong>数据来源</strong>：从AFEW中抽取出来的表情的静态帧。标注都在xml中</li><li><a href=https://liris-accede.ec-lyon.fr/ target=_blank rel=noopener>LIRIS-ACCEDE database</a>
同样也是基于电影抽取的，有三种数据集，包含离散的情感数据和基于维度的情感数据</li><li><a href=http://www.cs.binghamton.edu/~lijun/Research/3DFE/3DFE_Analysis.html target=_blank rel=noopener>BU-3DFE database</a>
3D的人脸表情数据集 **数据来源：**找人来做实验采集，按照要求的情绪做出表情
**数据量：**2500个3d面部模型（来自100个人）
还有同类的一些包含序列的等等的数据集，估计差别不大。
同样需要email获取</li><li><a href=http://www.cse.oulu.fi/CMV/Downloads/Oulu-CASIA target=_blank rel=noopener>Oulu-CASIA database</a>
<strong>数据来源</strong>：让80个受试者做出相应的表情并用不同相机采集（红外可见光正常光和弱光）
<strong>情绪类型</strong>：快乐、悲伤、惊讶、愤怒、恐惧、厌恶
email</li><li><a href=http://www.socsci.ru.nl:8180/RaFD2/RaFD target=_blank rel=noopener>RAFD</a>
<strong>数据来源</strong>：让67个受试者做出相应的表情在不同注视点和不同角度采集
<strong>情绪类型</strong>：8种情感类型
email</li><li><a href=https://www.emotionlab.se/kdef/ target=_blank rel=noopener>KDEF database</a>
<strong>数据来源</strong>：柔和、均匀的光线，多角度拍摄表情，使用统一的T恤颜色，在拍摄过程中使用网格将参与者面部居中，以及在扫描过程中将眼睛和嘴巴定位在固定的图像坐标中。
<strong>数据量</strong>：4900张 (70个人，一个7个情感)
页面底端超链接（没进去成功。。）</li><li><a href=http://mmlab.ie.cuhk.edu.hk/projects/socialrelation/index.html target=_blank rel=noopener>ExpW</a>
9w张左右，图片差不多8G</li><li><a href=http://mohammadmahoor.com/affectnet/ target=_blank rel=noopener>AffectNet</a>
百万量级数据（Emotion Net好像也是）
<strong>获取方式</strong>：从互联网获取
<strong>7类情感</strong>，首页有各种情感的数据量，最少的也有4k张
填写申请表email下载</li><li><a href=http://www.flintbox.com/public/project/4742/ target=_blank rel=noopener>Multi-PIE Face Database</a>
收钱给数据集
<strong>获取方式</strong>：记录会话
<strong>数据量</strong>：75w图片</li></ol><p>一些视频数据集(具体的在CSDN站上)（这些我就没有去详细看了）</p><ul><li><a href=https://humaine-db.sspnet.eu/ target=_blank rel=noopener>HUMAINE Database</a>
应为带表情标签的视频数据集（CSDN用户表示下载后没有标签）(我翻墙也进不去很奇怪)</li><li><a href>Recola database</a></li><li><a href>MMI</a></li><li>RU-FACS database-未公开</li><li><a href>Belfast naturalistic database</a>
-主要是演讲时候的情感识别</li><li><a href>VAM corpus</a>
也是演讲的</li><li><a href>AVEC系列数据集</a></li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://aikenh.cn/hugotest/tags/dataset/>Dataset</a></li><li><a href=https://aikenh.cn/hugotest/tags/emotion/>Emotion</a></li></ul><nav class=paginav><a class=prev href=https://aikenh.cn/hugotest/posts/imagecaptionrequirement/><span class=title>« Prev</span><br><span>Image Caption Dataset</span></a></nav></footer><div id=disqus_thread></div><script>function loadDisqus(){var e=document,t=e.createElement("script");t.src="https://aiken-hugo.disqus.com/embed.js",t.setAttribute("data-timestamp",+new Date),(e.head||e.body).appendChild(t),window.disqus_config=function(){this.page.url=window.location.href,this.page.identifier=window.location.href.substring(18)}}var runningOnBrowser=typeof window!="undefined",isBot=runningOnBrowser&&!("onscroll"in window)||typeof navigator!="undefined"&&/(gle|ing|ro|msn)bot|crawl|spider|yand|duckgo/i.test(navigator.userAgent),supportsIntersectionObserver=runningOnBrowser&&"IntersectionObserver"in window;setTimeout(function(){if(!isBot&&supportsIntersectionObserver){var e=new IntersectionObserver(function(t){t[0].isIntersecting&&(loadDisqus(),e.disconnect())},{threshold:[0]});e.observe(document.getElementById("disqus_thread"))}else loadDisqus()},1)</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by
Disqus.</a></noscript></article></main><footer class=footer><span>&copy; 2024 <a href=https://aikenh.cn/hugotest/>aiken's blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a>
</span><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><span id=busuanzi_container>Visitors: <span id=busuanzi_value_site_uv></span>
Views: <span id=busuanzi_value_site_pv></span></span></footer><script>document.addEventListener("DOMContentLoaded",function(){const e=document.getElementById("busuanzi_value_site_uv"),t=document.getElementById("busuanzi_value_site_pv"),o=13863,i=16993;if(!e||!t){console.error("Busuanzi elements not found.");return}const n=new MutationObserver(e=>{for(let t of e)if(t.type==="childList"){n.disconnect(),t.target.innerHTML=parseInt(t.target.innerHTML||0)+o;break}}),s=new MutationObserver(e=>{for(let t of e)if(t.type==="childList"){s.disconnect(),t.target.innerHTML=parseInt(t.target.innerHTML||0)+i;break}});n.observe(e,{childList:!0}),s.observe(t,{childList:!0})})</script><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><span class=topInner><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
<span id=read_progress></span>
</span></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))}),document.getElementById("theme-toggle-nav").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script><script>document.addEventListener("scroll",function(){const t=document.getElementById("read_progress"),n=document.documentElement.scrollHeight,s=document.documentElement.clientHeight,o=document.documentElement.scrollTop||document.body.scrollTop;t.innerText=((o/(n-s)).toFixed(2)*100).toFixed(0)})</script><script>(function(e,t){var s=document,o="script",n=s.createElement(o),i=s.getElementsByTagName(o)[0];n.src=e,t&&n.addEventListener("load",function(e){t(e)}),i.parentNode.insertBefore(n,i)})("/js/pangu.js",function(){pangu.spacingPage()})</script></body></html>