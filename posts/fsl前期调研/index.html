<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>FSL前期调研 | aiken's blog</title>
<meta name=keywords content="Machine Learning,Survey,FSL"><meta name=description content="
    
    







  
    
      文章的部分内容被密码保护：
    
  
  
    
    
  
  
    --- DON'T MODIFY THIS LINE ---
    主要是limited labels & Few Samples & Data programing

Weakly supervised learning
semi-supervised in video field
if we can recoding this work?
多指标下降（LOSS的耦合或者循环的选择）、相关的CV最新论文等等会在后续关注
元学习、浅层神经网络的概念等等  semi-supervised

PART1 Limited Labels （base on LiFeiFei‘s reference）
in this part we may list the paper which is useful for my recoding.
还有一些其他重要的可能在对论文进行重新精读的时候要记得注意reference：就比如说在loss变换和决策树生成那一块。
distant supervision(it&rsquo;s kind of early) can be another baseline for our method, we need to understand how this method work for that situation
distant supervisor到底是什么机制可以去CSDN什么的看一下"><meta name=author content="aikenhong"><link rel=canonical href=https://hugotest-phi.vercel.app/posts/fsl%E5%89%8D%E6%9C%9F%E8%B0%83%E7%A0%94/><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><link rel=icon href=https://hugotest-phi.vercel.app/favicon/ghost.ico><link rel=icon type=image/png sizes=16x16 href=https://hugotest-phi.vercel.app/favicon/ghost-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://hugotest-phi.vercel.app/favicon/ghost-32x32.png><link rel=apple-touch-icon href=https://hugotest-phi.vercel.app/favicon/ghost-apple-touch-icon.png><link rel=mask-icon href=https://hugotest-phi.vercel.app/favicon/ghost-192x192.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://hugotest-phi.vercel.app/posts/fsl%E5%89%8D%E6%9C%9F%E8%B0%83%E7%A0%94/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script src=https://cdn.jsdmirror.com/npm/jquery@3.5.1/dist/jquery.min.js></script><link rel=stylesheet href=https://cdn.jsdmirror.com/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css><script src=https://cdn.jsdmirror.com/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js></script><link rel=stylesheet href=https://cdn.jsdmirror.com/npm/katex@0.16.11/dist/katex.min.css><script defer src=https://cdn.jsdmirror.com/npm/katex@0.16.11/dist/katex.min.js></script><script defer src=https://cdn.jsdmirror.com/npm/katex@0.16.11/dist/contrib/auto-render.min.js onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><link rel=stylesheet href=https://cdn.jsdmirror.com/npm/lxgw-wenkai-webfont@1.1.0/style.css><link rel=stylesheet href=https://cdn.jsdmirror.com/npm/lxgw-wenkai-lite-webfont@1.1.0/style.css><link rel=stylesheet href=https://cdn.jsdmirror.com/npm/lxgw-wenkai-tc-webfont@1.0.0/style.css><link rel=stylesheet href=https://cdn.jsdmirror.com/npm/lxgw-wenkai-screen-webfont@1.1.0/style.css><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Open+Sans:ital,wght@0,300..800;1,300..800&family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&family=Ubuntu+Mono:ital,wght@0,400;0,700;1,400;1,700&family=Ubuntu:ital,wght@0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><meta property="og:url" content="https://hugotest-phi.vercel.app/posts/fsl%E5%89%8D%E6%9C%9F%E8%B0%83%E7%A0%94/"><meta property="og:site_name" content="aiken's blog"><meta property="og:title" content="FSL前期调研"><meta property="og:description" content="文章的部分内容被密码保护：
--- DON'T MODIFY THIS LINE ---主要是limited labels & Few Samples & Data programing Weakly supervised learning
semi-supervised in video field
if we can recoding this work?
多指标下降（LOSS的耦合或者循环的选择）、相关的CV最新论文等等会在后续关注
元学习、浅层神经网络的概念等等 semi-supervised
PART1 Limited Labels （base on LiFeiFei‘s reference） in this part we may list the paper which is useful for my recoding.
还有一些其他重要的可能在对论文进行重新精读的时候要记得注意reference：就比如说在loss变换和决策树生成那一块。
distant supervision(it’s kind of early) can be another baseline for our method, we need to understand how this method work for that situation
distant supervisor到底是什么机制可以去CSDN什么的看一下"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-11-29T13:12:05+00:00"><meta property="article:modified_time" content="2021-11-29T13:12:05+00:00"><meta property="article:tag" content="Machine Learning"><meta property="article:tag" content="Survey"><meta property="article:tag" content="FSL"><meta property="og:image" content="https://hugotest-phi.vercel.app/cover/cover11.jpeg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://hugotest-phi.vercel.app/cover/cover11.jpeg"><meta name=twitter:title content="FSL前期调研"><meta name=twitter:description content="
    
    







  
    
      文章的部分内容被密码保护：
    
  
  
    
    
  
  
    --- DON'T MODIFY THIS LINE ---
    主要是limited labels & Few Samples & Data programing

Weakly supervised learning
semi-supervised in video field
if we can recoding this work?
多指标下降（LOSS的耦合或者循环的选择）、相关的CV最新论文等等会在后续关注
元学习、浅层神经网络的概念等等  semi-supervised

PART1 Limited Labels （base on LiFeiFei‘s reference）
in this part we may list the paper which is useful for my recoding.
还有一些其他重要的可能在对论文进行重新精读的时候要记得注意reference：就比如说在loss变换和决策树生成那一块。
distant supervision(it&rsquo;s kind of early) can be another baseline for our method, we need to understand how this method work for that situation
distant supervisor到底是什么机制可以去CSDN什么的看一下"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://hugotest-phi.vercel.app/posts/"},{"@type":"ListItem","position":2,"name":"FSL前期调研","item":"https://hugotest-phi.vercel.app/posts/fsl%E5%89%8D%E6%9C%9F%E8%B0%83%E7%A0%94/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"FSL前期调研","name":"FSL前期调研","description":"\r文章的部分内容被密码保护：\n--- DON'T MODIFY THIS LINE ---\r主要是limited labels \u0026amp; Few Samples \u0026amp; Data programing Weakly supervised learning\nsemi-supervised in video field\nif we can recoding this work?\n多指标下降（LOSS的耦合或者循环的选择）、相关的CV最新论文等等会在后续关注\n元学习、浅层神经网络的概念等等 semi-supervised\nPART1 Limited Labels （base on LiFeiFei‘s reference） in this part we may list the paper which is useful for my recoding.\n还有一些其他重要的可能在对论文进行重新精读的时候要记得注意reference：就比如说在loss变换和决策树生成那一块。\ndistant supervision(it\u0026rsquo;s kind of early) can be another baseline for our method, we need to understand how this method work for that situation\ndistant supervisor到底是什么机制可以去CSDN什么的看一下\n","keywords":["Machine Learning","Survey","FSL"],"articleBody":"\r文章的部分内容被密码保护：\n--- DON'T MODIFY THIS LINE ---\r主要是limited labels \u0026 Few Samples \u0026 Data programing Weakly supervised learning\nsemi-supervised in video field\nif we can recoding this work?\n多指标下降（LOSS的耦合或者循环的选择）、相关的CV最新论文等等会在后续关注\n元学习、浅层神经网络的概念等等 semi-supervised\nPART1 Limited Labels （base on LiFeiFei‘s reference） in this part we may list the paper which is useful for my recoding.\n还有一些其他重要的可能在对论文进行重新精读的时候要记得注意reference：就比如说在loss变换和决策树生成那一块。\ndistant supervision(it’s kind of early) can be another baseline for our method, we need to understand how this method work for that situation\ndistant supervisor到底是什么机制可以去CSDN什么的看一下\nTransfer Learning\\label propagation算法也是这一块重要的baseline\nBaseline ：scene graph prediction with limited labels\nreference：\n×Induction of decision trees if want download, try google it Pattern Learning for Relation Extraction with a Hierarchical Topic Model maybe we’ll need this paper,when we try to recoding.\nnope.当我们写论文需要理论基础的时候可能需要， √Data Programming: Creating Large Training Sets, Quickly it’s important to see if this article have same idea with me? it’s kind of learning paradigm,\n是一种构建数据集中，标注数据的范式，通过这样的method可以对多种labeling function进行整合，同时减少标注的误差和overlap情况的解决。后续我们实现方法的时候可以参考一下这个的数学理论，帮助在实际中进行应用。\n（本文中对这里的noise-aware的损失函数进行了应用，使其适应概率标签从而抑制噪声。）\n严重怀疑这是snorkel算法中的引文，直接引用过来\n×Knowledge-Based Weak Supervision for Information Extraction of Overlapping Relations figure out how can knowledge-based benefit weakly supervised learning ?\nnope. NIP method. 运用语言结构，类似于发的东西，来对文本进行补全。如果我们需要了解基础知识怎么使用，可以尝试参考。 ※Realistic Evaluation of DeepSemi-Supervised Learning Algorithms NIPS 2018\n深度半监督学习的现实性评估：公布了统一的重新实现和评估的平台（方式？），（针对于算法在生产现实之中的适用性发布的一个标准。）\nbased on analysis of image algorithm：半监督的方法通过对未标注的数据中的结构范式进行学习，从而降低了对标注数据的需求，也就是说输入的数据是大部分未标注和少量标注数据，就可以逼近完全标记数据集的表现效果。[32,50,39]，这几个针对于图像情况的方法。\n分析后发现，他们评估算法的方式并不适用于将算法推广到实际的生产领域中，于是我们对于评估ssl算法的方式提出了一些新的看法。\n说的就是以前这些ssl在算法的效果上可能作弊的一些方面，如果使用这样统一的标准对算法进行评估的话，才能使得算法得到一个好的效果，此外他也提出了一些ssl在训练过程中的一些棉铃的问题：比如说假如我们把其他类别的数据，混入其中的话，那么所有这些ssl的算法的效果都会受到极大的影响。 ×Learning from labeled and unlabeled data with label propagation 这是一个很重要的方法，但是没想到这个竟然这么早？（math or algorithm？）（2002）（最近读的另一篇论文好像就借鉴的这个思路）\n将数据从标记的高密度区域向未标记的数据区域进行传播，这一篇论文的话，主要存在一些数学推导，我建议从19的那一篇新的标签传播开始阅读，通过这篇来补全需要的数学基础，如果另一边已经讲述的很完备了就不需要这篇的内容了 PART2 key words searching（such as few samples etc.） limited labels on github：\n※microsoft’s work:github |paper i think it’ll be important one，we‘ll need to think carefully about this.seems like they have already make it great.\n思路上可以给我们启发：提出了一个针对few samples的通用框架（通过度度量传播来进行的label propagation方法），来解决无论是transfer、semi-supervised、few-shot这样解决的问题，并有了一个巨大的提升。\n将少量标记的label propagation到大量的未标注数据上，从而创建训练数据。主要贡献：用于传播的相似性度量从其他相关域转移时，这样的标签传播方法非常有效。\n这个算法框架可以细读一下，后续关照一下具体的思路和实现 ※The Limited Multi-Label Projection Layer:github |paper CVPR19 LML projection Layer 是一种几何的K类预测，用来再多类别少样本的情况下取代softmax的一个映射函数，这一篇主要是数学理论，在最后实现的话，要进行一定的参考和学习。 ※Learning Classifiers for Target Domain with Limited or No Labelspaper ICML2019\n从所有的训练数据中学习一个混杂（mixture）的“原型”，然后将原型撕裂成一个个part/part-type（用attention机制来实现）、 然后通过多个part的概率组合来表示一个new instance。（use MACNN）。（即将变成低维的概率向量组成的编码=低维视觉属性LDVA）到时候找个图把这些方法全都对比一下。md花样太多了，玩晕了。 Hand and Face Segmentation with Deep Convolutional Networks using Limited Labelled Data(论文还没出)github 一些奇奇怪怪的github项目github1 limited labels on google scholar：\n√Large Scale Sentiment Learning with Limited Labels SIGKDD2017\n他是通过对tweet的表情数据进行标注，建立的数据集，使用了self-learning和co-training两种WSL的方法，来对未标注的数据进行标注两种方法的具体注解我已经放在pdf上了 ×[Large-Scale Video Understanding with Limited Training Labels](google it)\n这尼玛是本综述的书，吓老子一跳 few samples on google scholar：\n×Learning Convolutional nerual networks from few samples 2013\nthis paper use the method of pre-trained (transfer learning instead nowadays) to get a satisfatory result.\n这篇文章太早了，需要的话再重新说吧。先不看 ※※Few-Shot Learning with Graph Neural Networks （two-version）（2017）（2018ICLR）\nusing graph network to implement semi-supervised. this research prove that the graph method perform well on ‘relathinal’ tasks.\n定义了一种图模型的网络框架来实现few-shot等few samples的任务，表明这样的图网络架构能够很好的实现关系这样的处理，也很容易在这样的情境下进行拓展，这也是一个框架设计的任务。但是我们能够从中学习一下图模型如何针对关系网络进行学习和训练的，以及探讨一下图网络的优势。这一篇文章也探讨了度量学习和元学习的一些东西，这一篇可以给一个高的阅读优先级。 data programing:\nend Label propagation:\n※Active Frame Selection for Label Propagation in Videos ECCV2012\ndecide how many frames we’ll need to mark by human for the best result .\n文章通过动态规划来选定视频中的k个frame，作为key frame，通过这几个frame的人工标记，能够最大的降低算法在label propagation中的标记误差，（其中num of k和误差的权衡还不是特别清楚）取代了以往这个key frame选择的随机性，带来更好的性能。\n此外这个方法还关注于帧数选择的动态性，由于视频的独立性，所以固定帧数的选择不一定是合适的，应该根据视频本身的特性来选择才是更好的。（但是不知道时空复杂度怎么说）\n值得一提的是，文中还提到了一些辅助人工标注的算法，这些算法有时间的话可以通过CSDN去调研一下。（防撞车） √Dynamic Label Propagation for Semi-Supervised Multi-class Multi-label Classification ICCV2013\n是一个基于图的方法，和eccv2012一致的地方在于，都认为视频任务的标注任务中，动态规划的part是需要的，上一篇用动态规划来实现keyframe的选择，这篇文章这是完全的semi-supervised的任务，他用dynamic的办法，动态的对多标签和多类信心进行拟合，从而动态的去更新相似性的度量，使用KNN来保留数据的固有结构。 ※※Label Propagation via Teaching-to-Learn and Learning-to-Teach 2016TNNLS\n一个迭代的label propagation方法，结合了一定self-learning 的机制，从dataset中迭代的选出易于分类的部分，然后通过不断的对这种易于标注的数据中去self-learning，从而提高分类器的性能，然后逐步的去针对模糊边界进行propagate。感觉是一个好方法\nintro中简要的对比介绍了这之前的一些label propagation方法，包括DLP。 based on the sota LPmethod，所以之前的一些可能可以不用看了， ※※※Learning to Propagate Labels: Transductive Propagation Network for Few-shot Learning ICLR2019\n结合了meta-learning/label propagation/transductive inference的方法，细看细看，这一篇一定要细看。太强了兄弟。intro里面也包含了很多的东西。 PART3 few-shot learning etc.（including one-shot learning) 淦淦淦，这尼玛比的定义能不能统一一哈\nFew-Shot Learning:\n√Prototypical Networks for Few-shot Learning 2017\\NIPS\n思路上好像和19年的cvpr那片有点像，先学习一个overall 再通过度量空间对newdata进行适应性的分配和训练。通过intro，我认为更像是一个简单的embedding的办法，将sample聚集到embedding space的一个原型上，在对其进行近邻标签传播算法把。\n但是里面有一些数学推导，可能是关于距离的，在我们后续需要划分指标的时候可以来看看这篇到底说了啥。（原型网络的数学推导。） √Meta-Learning for Semi-Supervised Few-Shot Classification 2018、ICLR\n正好是上面那片原型网络的升级方法，这也太巧了把。重开一个新的课题，设置环境成为一个wild的环境，存在干扰项，将未标注的data也混杂进原型的训练中。 ×Conditional Networks for Few-Shot Semantic Segmentation 2018\\ICLRworkshop track\n貌似有点弟弟，没提出什么有用的东西 ※Few-Shot Object Detection via Feature Reweighting 2019/ICCV\n在一个base class 的dataset上进行meta training，然后通过 reweighting 操作，adapt to novel classes。\nglobal原型，meta的场景学习策略，transfer的reweighting操作，以及在few-shot问题种加入了很多算法并没有考虑的localization问题。\n这篇论文看起来还行。 ※※Meta-Transfer Learning for Few-Shot Learning 2019\\CVPR\n通过多次的meta学习，来找到参数相对于原DNN网络（普通的meta都是用的浅层网络）而言的scaling和shifting，感觉和上一篇reweighting方法存在一定的相似性。 同时我们也知道基本的meta-learning 方法和场景图应用的方法存在极大的相似性。 此外在训练策略上，采用了一个HTmini-batch的变体策略。（figure1有简要说明，结合后面的策略观看） ×Deep Learning Models for Few-shot and Metric Learning 这一篇看不了 √Learning to Compare: Relation Network for Few-Shot Learning 2018\\CVPR\nmeta-learning 中的query 和support 不要搞混了。\n（前面还有一步是通过embedding来学习一个合适的feature）\n感觉上是一个基础的meta-learning框架，通过训练过程中对metric distance的学习，得到一个模型框架，然后通过模型将support data在metric space中与query data进行distance的衡量，从中选择shortest one作为classification的指标。 ※Dynamic Few-Shot Visual Learning Without Forgetting 2018\\CVPR\n为了使得模型在学习新的类别的时候，对旧的类别的识别能力依旧能保留下来，提出了两个策略，一个是基于attention 的分类权重生成器，二是对ConvNet进行重新设计，使其提取出feature的表征向量和分类权重向量之间的余弦相似性。？具体的还没看。但我认为主要努力的方向好像不是很对。 Few-Shot Human Motion Prediction via Meta-Learning 2018\\CVPR\n是一种结合了MAML、MRN、Meta-Learning的策略，本质还是一个few-shot的工作，没有提到怎么把这样的工作适应到真实的应用上，\n这一篇论文非常需要机器详细的阅读，不然的话不知道他到底是怎么操作的。 最终我们可以提出一个framework，通过对弱监督方法的嵌入，使得标注的任务变成一个人机交互的loop，通过我们对算法的干预，他将从标签的概率预测变成一个确定的指标预测，然后执行self-learning的方法，让自己逐渐变得更好，设定一个drop out，可以计算一个算法的最终所求时间。\n","wordCount":"549","inLanguage":"en","image":"https://hugotest-phi.vercel.app/cover/cover11.jpeg","datePublished":"2021-11-29T13:12:05Z","dateModified":"2021-11-29T13:12:05Z","author":[{"@type":"Person","name":"aikenhong"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://hugotest-phi.vercel.app/posts/fsl%E5%89%8D%E6%9C%9F%E8%B0%83%E7%A0%94/"},"publisher":{"@type":"Organization","name":"aiken's blog","logo":{"@type":"ImageObject","url":"https://hugotest-phi.vercel.app/favicon/ghost.ico"}}}</script></head><body id=top><script type=module src=https://cdn.jsdmirror.com/npm/ionicons@7.1.0/dist/ionicons/ionicons.esm.js defer></script><script nomodule src=https://cdn.jsdmirror.com/npm/ionicons@7.1.0/dist/ionicons/ionicons.js defer></script><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://hugotest-phi.vercel.app/ accesskey=h title="aiken's blog (Alt + H)">aiken's blog</a><div class=logo-switches><button id=theme-toggle-nav accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://hugotest-phi.vercel.app/ title=home><span>home</span></a></li><li><a href=https://hugotest-phi.vercel.app/archives/ title=archives><span>archives</span></a></li><li><a href=https://hugotest-phi.vercel.app/search title="search (Alt + /)" accesskey=/><span>search</span></a></li></ul></nav></header><div class=sidebar><ul><li class=logo style=--bg:#333><a href=#><div class=logo-icon><img src=/logo/logo.png></div><div class=logo-text>Aiken's Blog</div></a></li><div class=menulist><li style=--bg:#f44336><a href=https://hugotest-phi.vercel.app/ title=home><div class=logo-icon><ion-icon name=home-outline></ion-icon></div><div class=logo-text>home</div></a></li><li style=--bg:#b145e9><a href=https://hugotest-phi.vercel.app/posts/ title=posts><div class=logo-icon><ion-icon name=newspaper-outline></ion-icon></div><div class=logo-text>posts</div></a></li><li style=--bg:#0f93c7><a href=https://hugotest-phi.vercel.app/tags/ title=tags><div class=logo-icon><ion-icon name=pricetags-outline></ion-icon></div><div class=logo-text>tags</div></a></li><li style=--bg:#ffa117><a href=https://hugotest-phi.vercel.app/categories/ title=categories><div class=logo-icon><ion-icon name=grid-outline></ion-icon></div><div class=logo-text>categories</div></a></li><li style=--bg:#0fc70f><a href=https://hugotest-phi.vercel.app/archives/ title=archives><div class=logo-icon><ion-icon name=folder-outline></ion-icon></div><div class=logo-text>archives</div></a></li><li style=--bg:#d16111><a href=https://hugotest-phi.vercel.app/about/ title=about><div class=logo-icon><ion-icon name=person></ion-icon></div><div class=logo-text>about</div></a></li><li style=--bg:#15c095><a href=https://hugotest-phi.vercel.app/search title="search (Alt + /)" accesskey=/><div class=logo-icon><ion-icon name=search></ion-icon></div><div class=logo-text>search</div></a></li></div><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt +T)"><li><div class=logo-icon id=moon><ion-icon name=moon-outline></ion-icon></div><div class=logo-icon id=sun><ion-icon name=sunny-outline></ion-icon></div></li></button></div></ul></div><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://hugotest-phi.vercel.app/>Home</a>&nbsp;»&nbsp;<a href=https://hugotest-phi.vercel.app/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">FSL前期调研</h1><div class=post-meta><span title='2021-11-29 13:12:05 +0000 UTC'>November 29, 2021</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;549 words&nbsp;·&nbsp;aikenhong&nbsp;·&nbsp;<a href=/tags/machine-learning> Machine Learning</a>&nbsp;·&nbsp;<a href=/tags/survey> Survey</a>&nbsp;·&nbsp;<a href=/tags/fsl> FSL</a>&nbsp;|&nbsp;<a href=https://github.com/%3cpath_to_repo%3e/content/posts/FSL%e5%89%8d%e6%9c%9f%e8%b0%83%e7%a0%94.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><figure class=entry-cover><img loading=eager src=https://hugotest-phi.vercel.app/cover/cover11.jpeg alt></figure><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e4%b8%bb%e8%a6%81%e6%98%aflimited-labels--few-samples--data-programing aria-label="主要是limited labels & Few Samples & Data programing">主要是limited labels & Few Samples & Data programing</a><ul class=header-level-2><li><a href=#part1-limited-labels-base-on-lifeifeis-reference aria-label="PART1 Limited Labels （base on LiFeiFei‘s reference）">PART1 Limited Labels （base on LiFeiFei‘s reference）</a></li><li><a href=#part2-key-words-searchingsuch-as-few-samples-etc aria-label="PART2 key words searching（such as few samples etc.）">PART2 key words searching（such as few samples etc.）</a></li><li><a href=#part3-few-shot-learning--etcincluding-one-shot-learning aria-label="PART3 few-shot learning  etc.（including one-shot learning)">PART3 few-shot learning etc.（including one-shot learning)</a></li></ul></li></ul></div></details></div></aside><script>let activeElement,elements;document.addEventListener("DOMContentLoaded",function(){if(checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),elements.length>0){activeElement=elements[0];const e=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${e}"]`).classList.add("active")}const t=document.getElementById("top-link");t&&t.addEventListener("click",e=>{e.preventDefault(),window.scrollTo({top:0,behavior:"smooth"})})},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{const e=window.pageYOffset||document.documentElement.scrollTop;if(e===0)return;elements&&elements.length>0&&(elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase(),n=document.querySelector(`.inner ul li a[href="#${t}"]`);n.classList.remove("read")}),activeElement=Array.from(elements).find(t=>{if(getOffsetTop(t)-e>0&&getOffsetTop(t)-e<window.innerHeight/2)return t})||activeElement,elements.forEach((t)=>{const o=encodeURI(t.getAttribute("id")).toLowerCase(),s=document.querySelector(`.inner ul li a[href="#${o}"]`);if(t===activeElement){s.classList.add("active");const e=document.querySelector(".toc .inner"),t=s.offsetTop,n=e.clientHeight,o=s.clientHeight,i=t-n/2+o/2;e.scrollTo({top:i,behavior:"smooth"})}else getOffsetTop(t)<e&&s.classList.add("read"),s.classList.remove("active")}))},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return!document.querySelector(".hugo-encryptor-prompt")&&elements.length!=0&&(elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),console.log("Elements re-queried:",elements)),0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><head><script src=https://mirrors.sustech.edu.cn/cdnjs/ajax/libs/crypto-js/3.1.9-1/crypto-js.js></script></head><div class=hugo-encryptor-container><div class=hugo-encryptor-prompt><p>文章的部分内容被密码保护：</p></div><div class=hugo-encryptor-form><input class=hugo-encryptor-input placeholder=请输入密码>
<input class=hugo-encryptor-button type=button value=CLICK onclick=_click_handler(this)></div><div class=hugo-encryptor-cipher-text data-password=aikenhong_blog style=display:none><span style=display:none>--- DON'T MODIFY THIS LINE ---</span><h2 id=主要是limited-labels--few-samples--data-programing>主要是limited labels & Few Samples & Data programing<a hidden class=anchor aria-hidden=true href=#主要是limited-labels--few-samples--data-programing>#</a></h2><hr><p><del>Weakly supervised learning</del><br><del>semi-supervised in video field</del><br>if we can recoding this work?<br><del>多指标下降（LOSS的耦合或者循环的选择）、相关的CV最新论文等等会在后续关注</del><br><del>元学习、浅层神经网络的概念等等</del> <del>semi-supervised</del></p><hr><h3 id=part1-limited-labels-base-on-lifeifeis-reference>PART1 Limited Labels （base on LiFeiFei‘s reference）<a hidden class=anchor aria-hidden=true href=#part1-limited-labels-base-on-lifeifeis-reference>#</a></h3><p><em>in this part we may list the paper which is useful for my recoding.</em><br>还有一些其他重要的可能在对论文进行重新精读的时候要记得注意reference：就比如说在loss变换和决策树生成那一块。<br><em>distant supervision(it&rsquo;s kind of early) can be another baseline for our method, we need to understand how this method work for that situation</em><br>distant supervisor到底是什么机制可以去CSDN什么的看一下</p><hr><p>Transfer Learning\label propagation算法也是这一块重要的baseline<br><a href=https://arxiv.org/abs/1904.11622 target=_blank rel=noopener>Baseline</a>
：scene graph prediction with limited labels</p><hr><p><strong>reference：</strong></p><blockquote><ul><li><del>×<a href=https://www.semanticscholar.org/paper/Induction-of-decision-trees-Quinlan/058bb096ce1507cd65b91e341317a8ab11a675de target=_blank rel=noopener>Induction of decision trees</a></del><br><del>if want download, try google it</del></li><li><a href=https://www.semanticscholar.org/paper/01e2ed7202e357209c855927d23352b0f882c0a0 target=_blank rel=noopener>Pattern Learning for Relation Extraction with a Hierarchical Topic Model</a><br>maybe we&rsquo;ll need this paper,when we try to recoding.<br>nope.当我们写论文需要理论基础的时候可能需要，</li><li>√<a href=https://www.semanticscholar.org/paper/37acbbbcfe9d8eb89e5b01da28dac6d44c3903ee target=_blank rel=noopener>Data Programming: Creating Large Training Sets, Quickly</a><br>it&rsquo;s important to see if this article have same idea with me?</li></ul></blockquote><p>it&rsquo;s kind of learning paradigm,<br>是一种构建数据集中，标注数据的范式，通过这样的method可以对多种labeling function进行整合，同时减少标注的误差和overlap情况的解决。后续我们实现方法的时候可以参考一下这个的数学理论，帮助在实际中进行应用。<br>（本文中对这里的noise-aware的损失函数进行了应用，使其适应概率标签从而抑制噪声。）<br><strong>严重怀疑这是snorkel算法中的引文，直接引用过来</strong></p><blockquote><ul><li>×<a href=https://www.semanticscholar.org/paper/d48edf9e81653f4c3da716b037b0b50d54c5b034 target=_blank rel=noopener>Knowledge-Based Weak Supervision for Information Extraction of Overlapping Relations</a><br>figure out how can knowledge-based benefit weakly supervised learning ?<br>nope. NIP method. 运用语言结构，类似于发的东西，来对文本进行补全。如果我们需要了解基础知识怎么使用，可以尝试参考。</li></ul></blockquote><blockquote><ul><li>※<a href=https://arxiv.org/pdf/1804.09170.pdf target=_blank rel=noopener>Realistic Evaluation of DeepSemi-Supervised Learning Algorithms</a>
NIPS 2018<br>深度半监督学习的现实性评估：公布了统一的重新实现和评估的平台（方式？），（针对于算法在生产现实之中的适用性发布的一个标准。）<br>based on analysis of image algorithm：半监督的方法通过对未标注的数据中的结构范式进行学习，从而降低了对标注数据的需求，也就是说输入的数据是大部分未标注和少量标注数据，就可以逼近完全标记数据集的表现效果。[32,50,39]，这几个针对于图像情况的方法。<br>分析后发现，他们评估算法的方式并不适用于将算法推广到实际的生产领域中，于是我们对于评估ssl算法的方式提出了一些新的看法。<br>说的就是以前这些ssl在算法的效果上可能作弊的一些方面，如果使用这样统一的标准对算法进行评估的话，才能使得算法得到一个好的效果，此外他也提出了一些ssl在训练过程中的一些棉铃的问题：比如说假如我们把其他类别的数据，混入其中的话，那么所有这些ssl的算法的效果都会受到极大的影响。</li><li><del>×<a href=https://www.semanticscholar.org/paper/71a6896c98672fc9f41f48b7c10688d06d56437b target=_blank rel=noopener>Learning from labeled and unlabeled data with label propagation</a></del><br><del>这是一个很重要的方法，但是没想到这个竟然这么早？（math or algorithm？）（2002）（最近读的另一篇论文好像就借鉴的这个思路）<br>将数据从标记的高密度区域向未标记的数据区域进行传播，这一篇论文的话，主要存在一些数学推导，我建议从19的那一篇新的标签传播开始阅读，通过这篇来补全需要的数学基础，如果另一边已经讲述的很完备了就不需要这篇的内容了</del></li></ul></blockquote><hr><h3 id=part2-key-words-searchingsuch-as-few-samples-etc>PART2 key words searching（such as few samples etc.）<a hidden class=anchor aria-hidden=true href=#part2-key-words-searchingsuch-as-few-samples-etc>#</a></h3><p><strong>limited labels on github：</strong></p><blockquote><ul><li><strong>※microsoft&rsquo;s work:<a href=https://github.com/microsoft/metric-transfer.pytorch target=_blank rel=noopener>github</a>
|<a href=https://arxiv.org/abs/1812.08781 target=_blank rel=noopener>paper</a></strong><br>i think it’ll be important one，we‘ll need to think carefully about this.seems like they have already make it great.<br>思路上可以给我们启发：提出了一个针对few samples的通用框架（通过度度量传播来进行的label propagation方法），来解决无论是transfer、semi-supervised、few-shot这样解决的问题，并有了一个巨大的提升。<br>将少量标记的label propagation到大量的未标注数据上，从而创建训练数据。主要贡献：用于传播的相似性度量从其他相关域转移时，这样的标签传播方法非常有效。<br><strong>这个算法框架可以细读一下，后续关照一下具体的思路和实现</strong></li></ul></blockquote><blockquote><ul><li>※The Limited Multi-Label Projection Layer:<a href=https://github.com/locuslab/lml target=_blank rel=noopener>github</a>
|<a href=https://arxiv.org/abs/1906.08707 target=_blank rel=noopener>paper</a>
CVPR19<br>LML projection Layer 是一种几何的K类预测，用来再<strong>多类别少样本</strong>的情况下<strong>取代softmax</strong>的一个映射函数，这一篇主要是数学理论，在最后实现的话，要进行一定的参考和学习。</li><li>※Learning Classifiers for Target Domain with Limited or No Labels<a href=https://arxiv.org/abs/1901.09079v2 target=_blank rel=noopener>paper</a>
ICML2019<br>从所有的训练数据中学习一个混杂（mixture）的“原型”，然后将原型撕裂成一个个part/part-type（用attention机制来实现）、 然后通过多个part的概率组合来表示一个new instance。（use MACNN）。（即将变成低维的概率向量组成的编码=低维视觉属性LDVA）到时候找个图把这些方法全都对比一下。md花样太多了，玩晕了。</li><li>Hand and Face Segmentation with Deep Convolutional Networks using Limited Labelled Data(论文还没出)<a href=https://github.com/au-cvml-lab/Hand-and-Face-Segmentation-With-Limited-Data target=_blank rel=noopener>github</a></li><li>一些奇奇怪怪的github项目<a href=https://github.com/UdayaSameer/DeepSiameseNetwork target=_blank rel=noopener>github1</a></li></ul></blockquote><hr><p><strong>limited labels on google scholar：</strong></p><blockquote><ul><li>√<a href="https://dl.acm.org/citation.cfm?id=3098159" target=_blank rel=noopener>Large Scale Sentiment Learning with Limited Labels</a>
SIGKDD2017<br>他是通过对tweet的表情数据进行标注，建立的数据集，使用了<strong>self-learning</strong>和<strong>co-training</strong>两种WSL的方法，来对未标注的数据进行标注两种方法的具体注解我已经放在pdf上了</li><li><del>×[Large-Scale Video Understanding with Limited Training Labels](google it)</del><br><del>这尼玛是本综述的书，吓老子一跳</del></li></ul></blockquote><hr><p><strong>few samples on google scholar：</strong></p><blockquote><ul><li><del>×<a href=https://ieeexplore.ieee.org/abstract/document/6706969 target=_blank rel=noopener>Learning Convolutional nerual networks from few samples</a>
2013<br>this paper use the method of pre-trained (transfer learning instead nowadays) to get a satisfatory result.<br>这篇文章太早了，需要的话再重新说吧。先不看</del></li><li><strong>※※<a href=https://arxiv.org/abs/1711.04043 target=_blank rel=noopener>Few-Shot Learning with Graph Neural Networks</a>
</strong>（two-version）（2017）（2018ICLR）<br>using graph network to implement semi-supervised. this research prove that the graph method perform well on &lsquo;relathinal&rsquo; tasks.<br>定义了一种图模型的网络框架来实现few-shot等few samples的任务，表明这样的图网络架构能够很好的实现关系这样的处理，也很容易在这样的情境下进行拓展，这也是一个框架设计的任务。但是我们能够从中学习一下图模型如何针对关系网络进行学习和训练的，以及探讨一下图网络的优势。这一篇文章也探讨了度量学习和元学习的一些东西，这一篇可以给一个高的阅读优先级。</li></ul></blockquote><hr><p><strong>data programing:</strong></p><blockquote><ul><li>end</li></ul></blockquote><hr><p><strong>Label propagation:</strong></p><blockquote><ul><li>※<a href=https://link.springer.com/chapter/10.1007/978-3-642-33715-4_36 target=_blank rel=noopener>Active Frame Selection for Label Propagation in Videos</a>
ECCV2012<br>decide how many frames we&rsquo;ll need to mark by human for the best result .<br>文章通过动态规划来选定视频中的k个frame，作为key frame，通过这几个frame的人工标记，能够最大的降低算法在label propagation中的标记误差，（其中num of k和误差的权衡还不是特别清楚）取代了以往这个key frame选择的随机性，带来更好的性能。<br>此外这个方法还关注于帧数选择的动态性，由于视频的独立性，所以固定帧数的选择不一定是合适的，应该根据视频本身的特性来选择才是更好的。（但是不知道时空复杂度怎么说）<br>值得一提的是，文中还提到了一些辅助人工标注的算法，这些算法有时间的话可以通过CSDN去调研一下。（防撞车）</li><li>√<a href=http://openaccess.thecvf.com/content_iccv_2013/html/Wang_Dynamic_Label_Propagation_2013_ICCV_paper.html target=_blank rel=noopener>Dynamic Label Propagation for Semi-Supervised Multi-class Multi-label Classification</a>
ICCV2013<br>是一个基于图的方法，和eccv2012一致的地方在于，都认为视频任务的标注任务中，动态规划的part是需要的，上一篇用动态规划来实现keyframe的选择，这篇文章这是完全的semi-supervised的任务，他用dynamic的办法，动态的对多标签和多类信心进行拟合，从而动态的去更新相似性的度量，使用KNN来保留数据的固有结构。</li><li>※※<a href=https://ieeexplore.ieee.org/abstract/document/7447818 target=_blank rel=noopener>Label Propagation via Teaching-to-Learn and Learning-to-Teach</a>
2016TNNLS<br>一个迭代的label propagation方法，结合了一定self-learning 的机制，从dataset中迭代的选出易于分类的部分，然后通过不断的对这种易于标注的数据中去self-learning，从而提高分类器的性能，然后逐步的去针对模糊边界进行propagate。感觉是一个好方法<br>intro中简要的对比介绍了这之前的一些label propagation方法，包括DLP。
based on the sota LPmethod，所以之前的一些可能可以不用看了，</li><li>※※※<a href=https://arxiv.org/abs/1805.10002 target=_blank rel=noopener>Learning to Propagate Labels: Transductive Propagation Network for Few-shot Learning</a>
ICLR2019<br>结合了meta-learning/label propagation/transductive inference的方法，细看细看，这一篇一定要细看。太强了兄弟。intro里面也包含了很多的东西。</li></ul></blockquote><hr><h3 id=part3-few-shot-learning--etcincluding-one-shot-learning>PART3 few-shot learning etc.（including one-shot learning)<a hidden class=anchor aria-hidden=true href=#part3-few-shot-learning--etcincluding-one-shot-learning>#</a></h3><p>淦淦淦，这尼玛比的定义能不能统一一哈</p><p><strong>Few-Shot Learning:</strong></p><blockquote><ul><li>√<a href=http://papers.nips.cc/paper/6996-prototypical-networks-for-few-shot-learning target=_blank rel=noopener>Prototypical Networks for Few-shot Learning</a>
2017\NIPS<br>思路上好像和19年的cvpr那片有点像，先学习一个overall 再通过度量空间对newdata进行适应性的分配和训练。通过intro，我认为更像是一个简单的embedding的办法，将sample聚集到embedding space的一个原型上，在对其进行近邻标签传播算法把。<br>但是里面有一些数学推导，可能是关于距离的，在我们后续需要划分指标的时候可以来看看这篇到底说了啥。（原型网络的数学推导。）</li><li>√<a href=https://arxiv.org/abs/1803.00676 target=_blank rel=noopener>Meta-Learning for Semi-Supervised Few-Shot Classification</a>
2018、ICLR<br>正好是上面那片原型网络的升级方法，这也太巧了把。重开一个新的课题，设置环境成为一个wild的环境，存在干扰项，将未标注的data也混杂进原型的训练中。</li><li><del>×<a href="https://openreview.net/forum?id=SkMjFKJwG" target=_blank rel=noopener>Conditional Networks for Few-Shot Semantic Segmentation</a>
2018\ICLRworkshop track<br>貌似有点弟弟，没提出什么有用的东西</del></li><li>※<a href=http://openaccess.thecvf.com/content_ICCV_2019/html/Kang_Few-Shot_Object_Detection_via_Feature_Reweighting_ICCV_2019_paper.html target=_blank rel=noopener>Few-Shot Object Detection via Feature Reweighting</a>
2019/ICCV<br>在一个base class 的dataset上进行meta training，然后通过 reweighting 操作，adapt to novel classes。<br>global原型，meta的场景学习策略，transfer的reweighting操作，以及在few-shot问题种加入了很多算法并没有考虑的localization问题。<br>这篇论文看起来还行。</li><li>※※<a href=http://openaccess.thecvf.com/content_CVPR_2019/html/Sun_Meta-Transfer_Learning_for_Few-Shot_Learning_CVPR_2019_paper.html target=_blank rel=noopener>Meta-Transfer Learning for Few-Shot Learning</a>
2019\CVPR<br>通过多次的meta学习，来找到参数相对于原DNN网络（普通的meta都是用的浅层网络）而言的scaling和shifting，感觉和上一篇reweighting方法存在一定的相似性。 同时我们也知道基本的meta-learning 方法和场景图应用的方法存在极大的相似性。
此外在训练策略上，采用了一个HTmini-batch的变体策略。（figure1有简要说明，结合后面的策略观看）</li><li><del>×<a href=http://104.211.88.42:8080/handle/2005/4275 target=_blank rel=noopener>Deep Learning Models for Few-shot and Metric Learning</a><br>这一篇看不了</del></li><li>√<a href=http://openaccess.thecvf.com/content_cvpr_2018/html/Sung_Learning_to_Compare_CVPR_2018_paper.html target=_blank rel=noopener>Learning to Compare: Relation Network for Few-Shot Learning</a>
2018\CVPR<br>meta-learning 中的query 和support 不要搞混了。<br>（前面还有一步是通过embedding来学习一个合适的feature）<br>感觉上是一个基础的meta-learning框架，通过训练过程中对metric distance的学习，得到一个模型框架，然后通过模型将support data在metric space中与query data进行distance的衡量，从中选择shortest one作为classification的指标。</li><li>※<a href=http://openaccess.thecvf.com/content_cvpr_2018/html/Gidaris_Dynamic_Few-Shot_Visual_CVPR_2018_paper.html target=_blank rel=noopener>Dynamic Few-Shot Visual Learning Without Forgetting</a>
2018\CVPR<br>为了使得模型在学习新的类别的时候，对旧的类别的识别能力依旧能保留下来，提出了两个策略，一个是基于attention 的分类权重生成器，二是对ConvNet进行重新设计，使其提取出feature的表征向量和分类权重向量之间的余弦相似性。？具体的还没看。但我认为主要努力的方向好像不是很对。</li><li><a href=http://openaccess.thecvf.com/content_ECCV_2018/html/Liangyan_Gui_Few-Shot_Human_Motion_ECCV_2018_paper.html target=_blank rel=noopener>Few-Shot Human Motion Prediction via Meta-Learning</a>
2018\CVPR<br>是一种结合了MAML、MRN、Meta-Learning的策略，本质还是一个few-shot的工作，没有提到怎么把这样的工作适应到真实的应用上，<br>这一篇论文非常需要机器详细的阅读，不然的话不知道他到底是怎么操作的。</li></ul></blockquote><p>最终我们可以提出一个framework，通过对弱监督方法的嵌入，使得标注的任务变成一个人机交互的loop，通过我们对算法的干预，他将从标签的概率预测变成一个确定的指标预测，然后执行self-learning的方法，让自己逐渐变得更好，设定一个drop out，可以计算一个算法的最终所求时间。</p></div></div><script>function sanitizeContent(e){return e.replace(/[\x00-\x1F\x7F]/g,"").trim()}function encryptContent(e,t){const n=CryptoJS.MD5(e).toString(),s=n.substring(16),o=padContent(t),i=CryptoJS.AES.encrypt(o,CryptoJS.enc.Utf8.parse(n),{iv:CryptoJS.enc.Utf8.parse(s),mode:CryptoJS.mode.CBC,padding:CryptoJS.pad.Pkcs7});return CryptoJS.enc.Base64.stringify(i.ciphertext)}function padContent(e){const t=32,n=t-e.length%t;return e}function processEncryptedBlocks(){const t=document.querySelectorAll(".hugo-encryptor-cipher-text");t.forEach(e=>{const t=e.getAttribute("data-password"),n=e.innerHTML.trim(),s=sanitizeContent(n),o=encryptContent(t,s);e.innerHTML=o,e.removeAttribute("data-password")});const e=document.createElement("script");e.src="/js/decrypt.js",document.body.appendChild(e)}document.addEventListener("DOMContentLoaded",processEncryptedBlocks)</script></div><footer class=post-footer><ul class=post-tags><li><a href=https://hugotest-phi.vercel.app/tags/machine-learning/>Machine Learning</a></li><li><a href=https://hugotest-phi.vercel.app/tags/survey/>Survey</a></li><li><a href=https://hugotest-phi.vercel.app/tags/fsl/>FSL</a></li></ul><nav class=paginav><a class=prev href=https://hugotest-phi.vercel.app/posts/cpp/><span class=title>« Prev</span><br><span>Cherno的cpp教程笔记</span>
</a><a class=next href=https://hugotest-phi.vercel.app/posts/il-mgsvf/><span class=title>Next »</span><br><span>IL-MgSvF</span></a></nav></footer><div id=disqus_thread></div><script>function loadDisqus(){var e=document,t=e.createElement("script");t.src="https://aiken-hugo.disqus.com/embed.js",t.setAttribute("data-timestamp",+new Date),(e.head||e.body).appendChild(t),window.disqus_config=function(){this.page.url=window.location.href,this.page.identifier=window.location.href.substring(18)}}var runningOnBrowser=typeof window!="undefined",isBot=runningOnBrowser&&!("onscroll"in window)||typeof navigator!="undefined"&&/(gle|ing|ro|msn)bot|crawl|spider|yand|duckgo/i.test(navigator.userAgent),supportsIntersectionObserver=runningOnBrowser&&"IntersectionObserver"in window;setTimeout(function(){if(!isBot&&supportsIntersectionObserver){var e=new IntersectionObserver(function(t){t[0].isIntersecting&&(loadDisqus(),e.disconnect())},{threshold:[0]});e.observe(document.getElementById("disqus_thread"))}else loadDisqus()},1)</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by
Disqus.</a></noscript></article></main><footer class=footer><span>&copy; 2024 <a href=https://hugotest-phi.vercel.app/>aiken's blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a>
</span><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><span id=busuanzi_container>Visitors: <span id=busuanzi_value_site_uv></span>
Views: <span id=busuanzi_value_site_pv></span></span></footer><script>document.addEventListener("DOMContentLoaded",function(){const e=document.getElementById("busuanzi_value_site_uv"),t=document.getElementById("busuanzi_value_site_pv"),o=13863,i=16993;if(!e||!t){console.error("Busuanzi elements not found.");return}const n=new MutationObserver(e=>{for(let t of e)if(t.type==="childList"){n.disconnect(),t.target.innerHTML=parseInt(t.target.innerHTML||0)+o;break}}),s=new MutationObserver(e=>{for(let t of e)if(t.type==="childList"){s.disconnect(),t.target.innerHTML=parseInt(t.target.innerHTML||0)+i;break}});n.observe(e,{childList:!0}),s.observe(t,{childList:!0})})</script><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><span class=topInner><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
<span id=read_progress></span>
</span></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))}),document.getElementById("theme-toggle-nav").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script><script>document.addEventListener("scroll",function(){const t=document.getElementById("read_progress"),n=document.documentElement.scrollHeight,s=document.documentElement.clientHeight,o=document.documentElement.scrollTop||document.body.scrollTop;t.innerText=((o/(n-s)).toFixed(2)*100).toFixed(0)})</script><script>(function(e,t){var s=document,o="script",n=s.createElement(o),i=s.getElementsByTagName(o)[0];n.src=e,t&&n.addEventListener("load",function(e){t(e)}),i.parentNode.insertBefore(n,i)})("/js/pangu.js",function(){pangu.spacingPage()})</script></body></html>