<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>AIGC05 Stable Diffusion Model Training | aiken's blog</title>
<meta name=keywords content="AI"><meta name=description content="
该章节主要介绍 Stable-Diffusion 中模型的训练，考虑到硬件条件的限制，实际上这里介绍的训练，都是针对大模型的各种微调技术（Lora，Dreambooth，HyperNetwork, &mldr;），这里会以 LoRA 模型的训练为主。

参考文献：

AIGC教程：Stable Diffusion精进，如何训练特定画风LoRA模型？ | 游戏大观 | GameLook.com.cn

stable diffusion打造自己专属的LORA模型 - 王清培 - 博客园 (cnblogs.com)

sd-scripts/train_README-zh.md at main · kohya-ss/sd-scripts · GitHub


Train LoRA

LoRA 的优势就是其模型更小，且更加模块化；也就是说其的训练成本和要求都更低，同时使用代价小，可以作为某种风格插件或者角色插件来使用。


使用 LoRA 进行 Stable Diffusion 的高效参数微调 (huggingface.co)

[2106.09685] LoRA: Low-Rank Adaptation of Large Language Models (arxiv.org)




  
    
  





其中蓝色的是预训练好的源网络，而橙色的是新加的网络，通过控制 R 的宽度（文章主要论证了大模型的参数可能存在较低维度的秩，因此可以使用较小的 R 来对大模型的参数造成有效的影响），可以有效的减少需要训练的网络的 Size。
事前准备

这里只介绍本地训练，训练也可以在 Colab Notebook 等在线训练集群中进行，这里就不进行介绍了


WebUI + 想训练的基础 SD 模型
.txt 带说明的文本文件
Training Repo（sd-script
、lora-script
）
数据集准备（准备好训练图像）

训练包准备
这里我们使用 lora-script 来进行模型训练，lora-script 实际上是 sd-script 之外在包了一层，新增了一些可视化的功能和一些其他的脚本，让 sd-script 更加易用，它调用 sd 中的脚本来实现训练，但是封装了一些注释和整理，此外还支持的 tensorboard 可视化。

sd-script 本身包含了训练 lora、dreambooth、text-embedding、UNet、Text Encoder、图像生成、模型转换等多种功能。lora-script 还是主要专注于 LoRA 训练

查看 repo 也能知道 lora-script 中包含了 sd-script，所以我们部署的时候只需


1


git clone --recurse-submodules https://github.com/Akegarasu/lora-scripts


即可将需要的库安装下来，然后安装环境和相关以来只需要执行 .\install.ps1 即可（该脚本有 cn 版本，但是可能会出现问题），其会安装 sd-scripts 和 lora-scripts 需要的库。具体的可以参考相关 repo（sd-script 详细说明，lora-script 有简化版说明）。

安装的时候可能会出现虚拟环境未激活的问题，我们可以提前在改目录执行一次 python -m venv venv 一次即可。

Finish."><meta name=author content="aikenhong"><link rel=canonical href=https://aikenh.cn/hugotest/posts/stablediffusiontraining/><link crossorigin=anonymous href=/hugotest/assets/css/stylesheet.2f85ca17c12c62fa86b1e474b8a51aca4856f0d645debfe4922a4d5ddc6aa978.css integrity="sha256-L4XKF8EsYvqGseR0uKUaykhW8NZF3r/kkipNXdxqqXg=" rel="preload stylesheet" as=style><link rel=icon href=https://aikenh.cn/favicon/ghost.ico><link rel=icon type=image/png sizes=16x16 href=https://aikenh.cn/favicon/ghost-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://aikenh.cn/favicon/ghost-32x32.png><link rel=apple-touch-icon href=https://aikenh.cn/favicon/ghost-apple-touch-icon.png><link rel=mask-icon href=https://aikenh.cn/favicon/ghost-192x192.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://aikenh.cn/hugotest/posts/stablediffusiontraining/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script src=https://cdn.jsdmirror.com/npm/jquery@3.5.1/dist/jquery.min.js></script><link rel=stylesheet href=https://cdn.jsdmirror.com/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css><script src=https://cdn.jsdmirror.com/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js></script><link rel=stylesheet href=https://cdn.jsdmirror.com/npm/katex@0.16.11/dist/katex.min.css><script defer src=https://cdn.jsdmirror.com/npm/katex@0.16.11/dist/katex.min.js></script><script defer src=https://cdn.jsdmirror.com/npm/katex@0.16.11/dist/contrib/auto-render.min.js onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><link rel=stylesheet href=https://cdn.jsdmirror.com/npm/lxgw-wenkai-webfont@1.1.0/style.css><link rel=stylesheet href=https://cdn.jsdmirror.com/npm/lxgw-wenkai-lite-webfont@1.1.0/style.css><link rel=stylesheet href=https://cdn.jsdmirror.com/npm/lxgw-wenkai-tc-webfont@1.0.0/style.css><link rel=stylesheet href=https://cdn.jsdmirror.com/npm/lxgw-wenkai-screen-webfont@1.1.0/style.css><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Open+Sans:ital,wght@0,300..800;1,300..800&family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&family=Ubuntu+Mono:ital,wght@0,400;0,700;1,400;1,700&family=Ubuntu:ital,wght@0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><meta property="og:url" content="https://aikenh.cn/hugotest/posts/stablediffusiontraining/"><meta property="og:site_name" content="aiken's blog"><meta property="og:title" content="AIGC05 Stable Diffusion Model Training"><meta property="og:description" content=" 该章节主要介绍 Stable-Diffusion 中模型的训练，考虑到硬件条件的限制，实际上这里介绍的训练，都是针对大模型的各种微调技术（Lora，Dreambooth，HyperNetwork, …），这里会以 LoRA 模型的训练为主。
参考文献：
AIGC教程：Stable Diffusion精进，如何训练特定画风LoRA模型？ | 游戏大观 | GameLook.com.cn stable diffusion打造自己专属的LORA模型 - 王清培 - 博客园 (cnblogs.com) sd-scripts/train_README-zh.md at main · kohya-ss/sd-scripts · GitHub Train LoRA LoRA 的优势就是其模型更小，且更加模块化；也就是说其的训练成本和要求都更低，同时使用代价小，可以作为某种风格插件或者角色插件来使用。
使用 LoRA 进行 Stable Diffusion 的高效参数微调 (huggingface.co) [2106.09685] LoRA: Low-Rank Adaptation of Large Language Models (arxiv.org) 其中蓝色的是预训练好的源网络，而橙色的是新加的网络，通过控制 R 的宽度（文章主要论证了大模型的参数可能存在较低维度的秩，因此可以使用较小的 R 来对大模型的参数造成有效的影响），可以有效的减少需要训练的网络的 Size。
事前准备 这里只介绍本地训练，训练也可以在 Colab Notebook 等在线训练集群中进行，这里就不进行介绍了
WebUI + 想训练的基础 SD 模型 .txt 带说明的文本文件 Training Repo（sd-script 、lora-script ） 数据集准备（准备好训练图像） 训练包准备 这里我们使用 lora-script 来进行模型训练，lora-script 实际上是 sd-script 之外在包了一层，新增了一些可视化的功能和一些其他的脚本，让 sd-script 更加易用，它调用 sd 中的脚本来实现训练，但是封装了一些注释和整理，此外还支持的 tensorboard 可视化。
sd-script 本身包含了训练 lora、dreambooth、text-embedding、UNet、Text Encoder、图像生成、模型转换等多种功能。lora-script 还是主要专注于 LoRA 训练
查看 repo 也能知道 lora-script 中包含了 sd-script，所以我们部署的时候只需
1 git clone --recurse-submodules https://github.com/Akegarasu/lora-scripts 即可将需要的库安装下来，然后安装环境和相关以来只需要执行 .\install.ps1 即可（该脚本有 cn 版本，但是可能会出现问题），其会安装 sd-scripts 和 lora-scripts 需要的库。具体的可以参考相关 repo（sd-script 详细说明，lora-script 有简化版说明）。
安装的时候可能会出现虚拟环境未激活的问题，我们可以提前在改目录执行一次 python -m venv venv 一次即可。
Finish."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-05-06T23:43:41+00:00"><meta property="article:modified_time" content="2023-05-06T23:43:41+00:00"><meta property="article:tag" content="AI"><meta property="og:image" content="https://aikenh.cn/cover/cover12.jpeg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://aikenh.cn/cover/cover12.jpeg"><meta name=twitter:title content="AIGC05 Stable Diffusion Model Training"><meta name=twitter:description content="
该章节主要介绍 Stable-Diffusion 中模型的训练，考虑到硬件条件的限制，实际上这里介绍的训练，都是针对大模型的各种微调技术（Lora，Dreambooth，HyperNetwork, &mldr;），这里会以 LoRA 模型的训练为主。

参考文献：

AIGC教程：Stable Diffusion精进，如何训练特定画风LoRA模型？ | 游戏大观 | GameLook.com.cn

stable diffusion打造自己专属的LORA模型 - 王清培 - 博客园 (cnblogs.com)

sd-scripts/train_README-zh.md at main · kohya-ss/sd-scripts · GitHub


Train LoRA

LoRA 的优势就是其模型更小，且更加模块化；也就是说其的训练成本和要求都更低，同时使用代价小，可以作为某种风格插件或者角色插件来使用。


使用 LoRA 进行 Stable Diffusion 的高效参数微调 (huggingface.co)

[2106.09685] LoRA: Low-Rank Adaptation of Large Language Models (arxiv.org)




  
    
  





其中蓝色的是预训练好的源网络，而橙色的是新加的网络，通过控制 R 的宽度（文章主要论证了大模型的参数可能存在较低维度的秩，因此可以使用较小的 R 来对大模型的参数造成有效的影响），可以有效的减少需要训练的网络的 Size。
事前准备

这里只介绍本地训练，训练也可以在 Colab Notebook 等在线训练集群中进行，这里就不进行介绍了


WebUI + 想训练的基础 SD 模型
.txt 带说明的文本文件
Training Repo（sd-script
、lora-script
）
数据集准备（准备好训练图像）

训练包准备
这里我们使用 lora-script 来进行模型训练，lora-script 实际上是 sd-script 之外在包了一层，新增了一些可视化的功能和一些其他的脚本，让 sd-script 更加易用，它调用 sd 中的脚本来实现训练，但是封装了一些注释和整理，此外还支持的 tensorboard 可视化。

sd-script 本身包含了训练 lora、dreambooth、text-embedding、UNet、Text Encoder、图像生成、模型转换等多种功能。lora-script 还是主要专注于 LoRA 训练

查看 repo 也能知道 lora-script 中包含了 sd-script，所以我们部署的时候只需


1


git clone --recurse-submodules https://github.com/Akegarasu/lora-scripts


即可将需要的库安装下来，然后安装环境和相关以来只需要执行 .\install.ps1 即可（该脚本有 cn 版本，但是可能会出现问题），其会安装 sd-scripts 和 lora-scripts 需要的库。具体的可以参考相关 repo（sd-script 详细说明，lora-script 有简化版说明）。

安装的时候可能会出现虚拟环境未激活的问题，我们可以提前在改目录执行一次 python -m venv venv 一次即可。

Finish."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://aikenh.cn/hugotest/posts/"},{"@type":"ListItem","position":2,"name":"AIGC05 Stable Diffusion Model Training","item":"https://aikenh.cn/hugotest/posts/stablediffusiontraining/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"AIGC05 Stable Diffusion Model Training","name":"AIGC05 Stable Diffusion Model Training","description":" 该章节主要介绍 Stable-Diffusion 中模型的训练，考虑到硬件条件的限制，实际上这里介绍的训练，都是针对大模型的各种微调技术（Lora，Dreambooth，HyperNetwork, \u0026hellip;），这里会以 LoRA 模型的训练为主。\n参考文献：\nAIGC教程：Stable Diffusion精进，如何训练特定画风LoRA模型？ | 游戏大观 | GameLook.com.cn stable diffusion打造自己专属的LORA模型 - 王清培 - 博客园 (cnblogs.com) sd-scripts/train_README-zh.md at main · kohya-ss/sd-scripts · GitHub Train LoRA LoRA 的优势就是其模型更小，且更加模块化；也就是说其的训练成本和要求都更低，同时使用代价小，可以作为某种风格插件或者角色插件来使用。\n使用 LoRA 进行 Stable Diffusion 的高效参数微调 (huggingface.co) [2106.09685] LoRA: Low-Rank Adaptation of Large Language Models (arxiv.org) 其中蓝色的是预训练好的源网络，而橙色的是新加的网络，通过控制 R 的宽度（文章主要论证了大模型的参数可能存在较低维度的秩，因此可以使用较小的 R 来对大模型的参数造成有效的影响），可以有效的减少需要训练的网络的 Size。\n事前准备 这里只介绍本地训练，训练也可以在 Colab Notebook 等在线训练集群中进行，这里就不进行介绍了\nWebUI + 想训练的基础 SD 模型 .txt 带说明的文本文件 Training Repo（sd-script 、lora-script ） 数据集准备（准备好训练图像） 训练包准备 这里我们使用 lora-script 来进行模型训练，lora-script 实际上是 sd-script 之外在包了一层，新增了一些可视化的功能和一些其他的脚本，让 sd-script 更加易用，它调用 sd 中的脚本来实现训练，但是封装了一些注释和整理，此外还支持的 tensorboard 可视化。\nsd-script 本身包含了训练 lora、dreambooth、text-embedding、UNet、Text Encoder、图像生成、模型转换等多种功能。lora-script 还是主要专注于 LoRA 训练\n查看 repo 也能知道 lora-script 中包含了 sd-script，所以我们部署的时候只需\n1 git clone --recurse-submodules https://github.com/Akegarasu/lora-scripts 即可将需要的库安装下来，然后安装环境和相关以来只需要执行 .\\install.ps1 即可（该脚本有 cn 版本，但是可能会出现问题），其会安装 sd-scripts 和 lora-scripts 需要的库。具体的可以参考相关 repo（sd-script 详细说明，lora-script 有简化版说明）。\n安装的时候可能会出现虚拟环境未激活的问题，我们可以提前在改目录执行一次 python -m venv venv 一次即可。\nFinish.\n","keywords":["AI"],"articleBody":" 该章节主要介绍 Stable-Diffusion 中模型的训练，考虑到硬件条件的限制，实际上这里介绍的训练，都是针对大模型的各种微调技术（Lora，Dreambooth，HyperNetwork, …），这里会以 LoRA 模型的训练为主。\n参考文献：\nAIGC教程：Stable Diffusion精进，如何训练特定画风LoRA模型？ | 游戏大观 | GameLook.com.cn stable diffusion打造自己专属的LORA模型 - 王清培 - 博客园 (cnblogs.com) sd-scripts/train_README-zh.md at main · kohya-ss/sd-scripts · GitHub Train LoRA LoRA 的优势就是其模型更小，且更加模块化；也就是说其的训练成本和要求都更低，同时使用代价小，可以作为某种风格插件或者角色插件来使用。\n使用 LoRA 进行 Stable Diffusion 的高效参数微调 (huggingface.co) [2106.09685] LoRA: Low-Rank Adaptation of Large Language Models (arxiv.org) 其中蓝色的是预训练好的源网络，而橙色的是新加的网络，通过控制 R 的宽度（文章主要论证了大模型的参数可能存在较低维度的秩，因此可以使用较小的 R 来对大模型的参数造成有效的影响），可以有效的减少需要训练的网络的 Size。\n事前准备 这里只介绍本地训练，训练也可以在 Colab Notebook 等在线训练集群中进行，这里就不进行介绍了\nWebUI + 想训练的基础 SD 模型 .txt 带说明的文本文件 Training Repo（sd-script 、lora-script ） 数据集准备（准备好训练图像） 训练包准备 这里我们使用 lora-script 来进行模型训练，lora-script 实际上是 sd-script 之外在包了一层，新增了一些可视化的功能和一些其他的脚本，让 sd-script 更加易用，它调用 sd 中的脚本来实现训练，但是封装了一些注释和整理，此外还支持的 tensorboard 可视化。\nsd-script 本身包含了训练 lora、dreambooth、text-embedding、UNet、Text Encoder、图像生成、模型转换等多种功能。lora-script 还是主要专注于 LoRA 训练\n查看 repo 也能知道 lora-script 中包含了 sd-script，所以我们部署的时候只需\n1 git clone --recurse-submodules https://github.com/Akegarasu/lora-scripts 即可将需要的库安装下来，然后安装环境和相关以来只需要执行 .\\install.ps1 即可（该脚本有 cn 版本，但是可能会出现问题），其会安装 sd-scripts 和 lora-scripts 需要的库。具体的可以参考相关 repo（sd-script 详细说明，lora-script 有简化版说明）。\n安装的时候可能会出现虚拟环境未激活的问题，我们可以提前在改目录执行一次 python -m venv venv 一次即可。\nFinish.\n数据集准备 准备数据集的时候，要根据当前的设备显存对图片进行预处理，避免图片的分辨率过大，导致显存爆了，这里可以使用微软自己的 Powertoys 对文件进行批量 resize。\n数据需求：\n如果希望有更好的泛化性，训练素材中应该包含各种：角度、表情、光线。 如果是角色的话，建议尽可能的手机正面，侧面，背面，头像特写， 画风素材可以多一点 可以考虑扣白底： https://pickwant.com/home 简单预处理：调整分辨率（64 的倍数），裁剪。 数据量：如果是角色训练集在 20~50 左右足够，但是重要的是训练数据和训练轮次之间需要根据可视化做一个协调，避免过拟合（有时候可能也允许过拟合，取决于使用的场景）或者欠拟合的情况发生，原则上讲数据数量和轮次是正相关的关系。 预处理之生成图像描述：图像描述实际上是作为训练的标签存在，而显然，我们需要自动生成描述，如果还记得之前的文档，图生图功能中有反向推演提示词的方法，同样我们也会用该 Deepbooru 方法去生成标签，webui 中提供了一个专门的入口：\n在 resize 完成后，我们执行改生成图像描述的方法，指定好原图像位置和预处理之后的文件夹名字位置即可。\n预处理之后的文件夹会包含原图和对应生成的描述 txt 文件。 生成的描述文件可以 check 一下进行手动修改，错误的标签可能会引导出错误的训练方向。 为了便于 LoRA 的后续描述生成相同的效果，可以删除部分不必要的标签，令重要的标签和效果的相关度提高，并覆盖部分细节标签，可以理解成把一系列标签打包成一个标签。 训练数据就位： 将训练数据转移到 \u003c...\u003e/lora-scripts/train/ 中，如果没有该 train 目录就创建一个，单次的训练数据放在一个文件夹中，例如 \u003c..\u003e/train/train_1_person/\n正则化数据准备(option)：在 train 中新建一个 reg 文件夹，用于正则化训练，命名和此次训练的文件夹名相同，例如：\u003c..\u003e/train/reg/train_1_person/，在其中放的数据和训练数据应该是同类的不同对象。\n正则化的作用通常是避免模型过拟合，对模型添加额外的约束。例如我们训练一个猫，正则文件夹里应该同样存放猫（别的猫）的照片。\n基础模型就位：将训练用到的基础模型复制到 \u003c...\u003e/lora-scripts/sd-models/ 中，lora 是针对基础模型的注入（额外的 FC），对原模型进行部分的调整。\n注意事项： 避免文件名重复，使用相同的文件后缀能够避免文件名重复的问题。\n训练脚本编辑 better read: sd-scripts/train_network_README-zh.md at main · kohya-ss/sd-scripts (github.com) 接下来就是编辑训练脚本中的基础设置，lora-scripts 中的注释已经非常详细的写好了每个参数的含义，这里就介绍一些可能需要或者常用的参数设置：\n训练相关的一些设置如下\nsave_every_n_epochs 会决定我们最终获得几个模型，这里也可以设置一下。 reg_data_dir 设置正则化的目录，为空为默认不启用。 … 其他的看看注释 还有一些输出相关的设置，包括输出模型的目录和名称，名称最好修改，避免一直都是默认的错误覆盖了：\n目录最好用 10_ 或者纯数字的目录，然后路径填写的时候只填写到上级目录，比如说数据存放在 \u003c...\u003e/train/10_ron，那么脚本中就填写到 ./train，正则化目录同理。\n其他参数设置简单推荐：\nunet_lr 一般=lr lr_scheduler: constant_with_warmup 用的蛮多 优化器： use_lion 目前最好的，但是有正则化素材最好别开 编辑完成后直接执行该 ps1 脚本即可。\n脚本运行过程中可能会出现 nomodule named triton 的错误，可能是由于 windows 不支持该模块导致的，但是不影响最后的生成。\nLoRA 模型测试和选择 将模型导入 /extensions/sd-webui-additional-networks/models/lora 中，然后利用之前介绍过的 scripts 的 xyzplot 功能，分别测试和对比各个模型的效果，选择其中效果好的保留。\n成品展示 \u0026 简单心得 用我家猫小荣的 10-14 张照片训练出来了小荣的 Lora 模型（DreamBooth）也训了一下，但是显存（3070）不太够。\n训练 Lora 中使用了如下的参数配置：\nClilloutmix 作为底层模型 使用训练 DreamBooth 中生成的 700 张猫的图片作为正则化数据集 BatchSize=2，Lr=e-4, Lion, Cosine_with_restarts, Max_train_epoch=10 此外，由于图片少（一致性太强）训练轮次多，应该是有些过拟合，Lora 调用时只能在 3 以下，不然难以和原图区分，可以对训练轮次和图片数量做一下调整。\n训练的时候打标签除了自动生成的，最好还是主动去修改，各种细节剖开来，描述好姿势，颜色，背景，表情等，后面学到的模型对各个部分进行修改的化比较方便区分，避免全都耦合在一起。\n此外标签中要给特定的对象一个特定的标签，方便我们产出该指定效果（或者角色）。\n\u003c!DOCTYPE html\u003e\r00009-3706840146.png\r00021-1903439770.png\r00031-4141742147.png\r00034-1003575578.png\r00000-4244496505.png\r00021-3282103832.png\r00024-4215830362.png\r00056-533810957.png\r00001-2335627500.png\rDreambooth DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation。 这种训练方式旨在微调 diffusion model 使其生成的图像专注于某个 object，具体可以看下面的示例图\n相关资源链接： webui训练插件 | offical repo | Paper | Dreambooth原理与实践 如果需要深入了解实现原理的可以查看官方论文和对应的 repo 内容。\n其具备以下的优势：\n仅需要同一个主体（动物，模型，等） 的少量图片即可（3 到 5 张可以，但是越多越好） 可以基于该主体生成各种图片 针对该场景考虑将训练数据扣成白底可能也会使得生成的效果更好。\n具体的训练的详细介绍，可以参考训练插件页面的说明进行尝试，这里简单介绍一下工作流程：\nA. 模型创建：Dreambooth tab -\u003e “Create Model” sub-tab -\u003e 确定一个新的模型名称 -\u003e选择本地的模型 / HF(model URL \u0026 token) -\u003e 确定后源 ckpt 会暂时存储在 models\\dreambooth\\MODELNAME\\working -\u003e 点击 Create\nB. 参数设置：Settings tab -\u003e 是否使用 LoRA、BatchSize、Epochs 、学习率等等的设置都是基本的参数就不在介绍\n设置(settings)中的 performance wizard (WIP) 中可以查看建议的参数。\nC. Concepts 设置：主要有两个类别：数据路径 + （class + prompt）\n数据路径不在赘述 Prompt 指的是我们的目标样本应该用什么提示词 Class 填入的是与目标同类但是不同个体的样本，避免过拟合 D. Saving 设置： 主要是设置我们各种模型的保存策略，类似步长之类的。\nE. Generate 设置：生成过程中的调整\n设置完成后开始训练即可，可以看训练中的 loss 和模型训练过程中生成的图片，AFK，训练完成后就可以看到训练好的 CKPT 了。\nTextual Inversion 相关资源链接：论文 | 官方代码 | webui说明 | textual-inversion.github.io 其全称为 An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion\nTextual Inversion： 功能和其名字相互对应，反转 Text2Img 的过程，Img2Text 地建立图像与指定文本（Prompt）的关联。也就是说，当我们希望输入特定的 Prompt 能稳定的产生某个效果的时候，可以利用 Textual Inversion 技术来对模型进行改造，使得该特定的 prompt 能取得特定效果。\n实际上该技术也能取得和 Dreambooth 类似的效果，具体可以看论文和例子，这里也只简单介绍一下训练和使用。\nA. 创建 Word Embedding（指定 Prompt）：\n在 Train 标签页下创建 Embedding 页面，name 指定一个关键字（prompt），该 prompt 代表我们后续要训练出来的概念，创建完成后，就可以在 /\u003c...\u003e/stable-diffusion-webui/embeddings 路径下看到我们创建的 pt 文件\nB.数据预处理：同 Lora 介绍的预处理图像，准备好训练数据\nC.参数设置：\nTrain-\u003e Train 页面中，只训练 Embedding，所以 Hypernetwork 的地方放空。 填写的基本学习率等参数就不再赘述， 填写：数据集目录、Log 目录等即可 “Prompt template\"需要注意下, 提供了几种可选的训练模式: style_filewords.txt: 表示训练画风 subject_filewords.txt: 表示训练人物或物体 D.选择训练 Embedding：即可开始训练\nE.使用：放在 /\u003c...\u003e/stable-diffusion-webui/embeddings 中，在相关生成过程中，填写 Prompt 的时候像 easynagetive 启用即可。\nHyperNetworks NovelAI Improvements on Stable Diffusion | by NovelAI | Medium Hypernetwork 是一种微调技术（by Novel AI ），它是一个小型附加神经网络附加在 Stable Diffusion 模型上以修改其风格，这种方式和当时的论文并不一致，当时的 HyperNetwork 是通过修改权重来进行调整，NovelAI 中使用到的则是添加一个小型的线性附加网络。\n具体而言，其插入到噪声预测器 UNet 的交叉注意力模块中，（通常情况下是一个简单的神经网络：具有 dropout 和激活函数的全连接线性网络）通过插入两个转换 key 和 query 向量的网络来劫持交叉注意力模块。\n其训练过程与 Text Inversion 几乎一致，这里就不在赘述，只描述部分不同：\n创建模型的时候在 Train/Create Hypernetwork 选项卡 训练的时候选择 train hypernetwork 使用时存放的目录（应该也可以存放在 Addition Network 的文件夹中）为： stable_difusion\\stable-diffusion-webui\\models\\hypernetworks 像 Lora 一样启用即可。 FI stable diffusion 的介绍暂时就到这边了，原理相关的东西就先不介绍了，以后如果有必要的话，或者谁有需求的话可以联系我更新。\n通过训练小荣来给出一个详细的参数设置（Dreambooth 和 Lora 和 TextInversion） ","wordCount":"501","inLanguage":"en","image":"https://aikenh.cn/cover/cover12.jpeg","datePublished":"2023-05-06T23:43:41Z","dateModified":"2023-05-06T23:43:41Z","author":[{"@type":"Person","name":"aikenhong"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://aikenh.cn/hugotest/posts/stablediffusiontraining/"},"publisher":{"@type":"Organization","name":"aiken's blog","logo":{"@type":"ImageObject","url":"https://aikenh.cn/favicon/ghost.ico"}}}</script></head><body id=top><script type=module src=https://cdn.jsdmirror.com/npm/ionicons@7.1.0/dist/ionicons/ionicons.esm.js defer></script><script nomodule src=https://cdn.jsdmirror.com/npm/ionicons@7.1.0/dist/ionicons/ionicons.js defer></script><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://aikenh.cn/hugotest/ accesskey=h title="aiken's blog (Alt + H)">aiken's blog</a><div class=logo-switches><button id=theme-toggle-nav accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://aikenh.cn/hugotest/ title=home><span>home</span></a></li><li><a href=https://aikenh.cn/hugotest/posts/ title=posts><span>posts</span></a></li><li><a href=https://aikenh.cn/hugotest/tags/ title=tags><span>tags</span></a></li><li><a href=https://aikenh.cn/hugotest/categories/ title=categories><span>categories</span></a></li><li><a href=https://aikenh.cn/hugotest/archives/ title=archives><span>archives</span></a></li><li><a href=https://aikenh.cn/hugotest/about/ title=about><span>about</span></a></li><li><a href=https://aikenh.cn/hugotest/search title="search (Alt + /)" accesskey=/><span>search</span></a></li></ul></nav></header><div class=sidebar><ul><li class=logo style=--bg:#333><a href=#><div class=logo-icon><img src=/logo/logo.png></div><div class=logo-text>Aiken's Blog</div></a></li><div class=menulist><li style=--bg:#f44336><a href=https://aikenh.cn/hugotest/ title=home><div class=logo-icon><ion-icon name=home-outline></ion-icon></div><div class=logo-text>home</div></a></li><li style=--bg:#b145e9><a href=https://aikenh.cn/hugotest/posts/ title=posts><div class=logo-icon><ion-icon name=newspaper-outline></ion-icon></div><div class=logo-text>posts</div></a></li><li style=--bg:#0f93c7><a href=https://aikenh.cn/hugotest/tags/ title=tags><div class=logo-icon><ion-icon name=pricetags-outline></ion-icon></div><div class=logo-text>tags</div></a></li><li style=--bg:#ffa117><a href=https://aikenh.cn/hugotest/categories/ title=categories><div class=logo-icon><ion-icon name=grid-outline></ion-icon></div><div class=logo-text>categories</div></a></li><li style=--bg:#0fc70f><a href=https://aikenh.cn/hugotest/archives/ title=archives><div class=logo-icon><ion-icon name=folder-outline></ion-icon></div><div class=logo-text>archives</div></a></li><li style=--bg:#d16111><a href=https://aikenh.cn/hugotest/about/ title=about><div class=logo-icon><ion-icon name=person></ion-icon></div><div class=logo-text>about</div></a></li><li style=--bg:#15c095><a href=https://aikenh.cn/hugotest/search title="search (Alt + /)" accesskey=/><div class=logo-icon><ion-icon name=search></ion-icon></div><div class=logo-text>search</div></a></li></div><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt +T)"><li><div class=logo-icon id=moon><ion-icon name=moon-outline></ion-icon></div><div class=logo-icon id=sun><ion-icon name=sunny-outline></ion-icon></div></li></button></div></ul></div><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://aikenh.cn/hugotest/>Home</a>&nbsp;»&nbsp;<a href=https://aikenh.cn/hugotest/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">AIGC05 Stable Diffusion Model Training</h1><div class=post-meta><span title='2023-05-06 23:43:41 +0000 UTC'>May 6, 2023</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;501 words&nbsp;·&nbsp;aikenhong&nbsp;·&nbsp;<a href=/tags/ai> AI</a>&nbsp;|&nbsp;<a href=https://github.com/%3cpath_to_repo%3e/content/posts/StableDIffusionTraining.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><figure class=entry-cover><img loading=eager src=https://aikenh.cn/cover/cover12.jpeg alt></figure><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#train-lora aria-label="Train LoRA">Train LoRA</a><ul class=header-level-2><li><a href=#%e4%ba%8b%e5%89%8d%e5%87%86%e5%a4%87 aria-label=事前准备>事前准备</a></li><li><a href=#%e8%ae%ad%e7%bb%83%e5%8c%85%e5%87%86%e5%a4%87 aria-label=训练包准备>训练包准备</a></li><li><a href=#%e6%95%b0%e6%8d%ae%e9%9b%86%e5%87%86%e5%a4%87 aria-label=数据集准备>数据集准备</a></li><li><a href=#%e8%ae%ad%e7%bb%83%e8%84%9a%e6%9c%ac%e7%bc%96%e8%be%91 aria-label=训练脚本编辑>训练脚本编辑</a></li><li><a href=#lora-%e6%a8%a1%e5%9e%8b%e6%b5%8b%e8%af%95%e5%92%8c%e9%80%89%e6%8b%a9 aria-label="LoRA 模型测试和选择">LoRA 模型测试和选择</a></li><li><a href=#%e6%88%90%e5%93%81%e5%b1%95%e7%a4%ba--%e7%ae%80%e5%8d%95%e5%bf%83%e5%be%97 aria-label="成品展示 & 简单心得">成品展示 & 简单心得</a></li></ul></li><li><a href=#dreambooth aria-label=Dreambooth>Dreambooth</a></li><li><a href=#textual-inversion aria-label="Textual Inversion">Textual Inversion</a></li><li><a href=#hypernetworks aria-label=HyperNetworks>HyperNetworks</a></li><li><a href=#fi aria-label=FI>FI</a></li></ul></div></details></div></aside><script>let activeElement,elements;document.addEventListener("DOMContentLoaded",function(){if(checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),elements.length>0){activeElement=elements[0];const e=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${e}"]`).classList.add("active")}const t=document.getElementById("top-link");t&&t.addEventListener("click",e=>{e.preventDefault(),window.scrollTo({top:0,behavior:"smooth"})})},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{const e=window.pageYOffset||document.documentElement.scrollTop;if(e===0)return;elements&&elements.length>0&&(elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase(),n=document.querySelector(`.inner ul li a[href="#${t}"]`);n.classList.remove("read")}),activeElement=Array.from(elements).find(t=>{if(getOffsetTop(t)-e>0&&getOffsetTop(t)-e<window.innerHeight/2)return t})||activeElement,elements.forEach((t)=>{const o=encodeURI(t.getAttribute("id")).toLowerCase(),s=document.querySelector(`.inner ul li a[href="#${o}"]`);if(t===activeElement){s.classList.add("active");const e=document.querySelector(".toc .inner"),t=s.offsetTop,n=e.clientHeight,o=s.clientHeight,i=t-n/2+o/2;e.scrollTo({top:i,behavior:"smooth"})}else getOffsetTop(t)<e&&s.classList.add("read"),s.classList.remove("active")}))},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return!document.querySelector(".hugo-encryptor-prompt")&&elements.length!=0&&(elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),console.log("Elements re-queried:",elements)),0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><blockquote><p>该章节主要介绍 Stable-Diffusion 中模型的训练，考虑到硬件条件的限制，实际上这里介绍的训练，都是针对大模型的各种微调技术（Lora，Dreambooth，HyperNetwork, &mldr;），这里会以 LoRA 模型的训练为主。</p></blockquote><p>参考文献：</p><ul><li><a href=http://www.gamelook.com.cn/2023/04/514936 target=_blank rel=noopener>AIGC教程：Stable Diffusion精进，如何训练特定画风LoRA模型？ | 游戏大观 | GameLook.com.cn</a></li><li><a href=https://www.cnblogs.com/wangiqngpei557/p/17301360.html target=_blank rel=noopener>stable diffusion打造自己专属的LORA模型 - 王清培 - 博客园 (cnblogs.com)</a></li><li><a href=https://github.com/kohya-ss/sd-scripts/blob/main/train_README-zh.md target=_blank rel=noopener>sd-scripts/train_README-zh.md at main · kohya-ss/sd-scripts · GitHub</a></li></ul><h2 id=train-lora>Train LoRA<a hidden class=anchor aria-hidden=true href=#train-lora>#</a></h2><blockquote><p>LoRA 的优势就是其模型更小，且更加模块化；也就是说其的训练成本和要求都更低，同时使用代价小，可以作为某种风格插件或者角色插件来使用。</p></blockquote><ul><li><a href=https://huggingface.co/blog/zh/lora target=_blank rel=noopener>使用 LoRA 进行 Stable Diffusion 的高效参数微调 (huggingface.co)</a></li><li><a href=https://arxiv.org/abs/2106.09685 target=_blank rel=noopener>[2106.09685] LoRA: Low-Rank Adaptation of Large Language Models (arxiv.org)</a></li></ul><p><div class=post-img-view><a data-fancybox=gallery href=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/mac/20230706171541.png><img alt=image.png loading=lazy src=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/mac/20230706171541.png class=responsive-image src=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/mac/20230706171541.png style="display:block;margin:0 auto" alt=image.png></a></div><script>document.addEventListener("DOMContentLoaded",function(){var e=document.querySelectorAll(".responsive-image"),t=window.innerHeight/2.5;e.forEach(function(e){e.style.maxHeight=t+"px"})})</script></p><p>其中蓝色的是预训练好的源网络，而橙色的是新加的网络，通过控制 R 的宽度（文章主要论证了大模型的参数可能存在较低维度的秩，因此可以使用较小的 R 来对大模型的参数造成有效的影响），可以有效的减少需要训练的网络的 Size。</p><h3 id=事前准备>事前准备<a hidden class=anchor aria-hidden=true href=#事前准备>#</a></h3><blockquote><p>这里只介绍本地训练，训练也可以在 Colab Notebook 等在线训练集群中进行，这里就不进行介绍了</p></blockquote><ol><li>WebUI + 想训练的基础 SD 模型</li><li><code>.txt</code> 带说明的文本文件</li><li>Training Repo（<a href=https://github.com/kohya-ss/sd-scripts target=_blank rel=noopener>sd-script</a>
、<a href=https://github.com/Akegarasu/lora-scripts target=_blank rel=noopener>lora-script</a>
）</li><li>数据集准备（准备好训练图像）</li></ol><h3 id=训练包准备>训练包准备<a hidden class=anchor aria-hidden=true href=#训练包准备>#</a></h3><p>这里我们使用 lora-script 来进行模型训练，lora-script 实际上是 sd-script 之外在包了一层，新增了一些可视化的功能和一些其他的脚本，让 sd-script 更加易用，它调用 sd 中的脚本来实现训练，但是封装了一些注释和整理，此外还支持的 tensorboard 可视化。</p><blockquote><p>sd-script 本身包含了训练 lora、dreambooth、text-embedding、UNet、Text Encoder、图像生成、模型转换等多种功能。lora-script 还是主要专注于 LoRA 训练</p></blockquote><p>查看 repo 也能知道 lora-script 中包含了 sd-script，所以我们部署的时候只需</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>git clone --recurse-submodules https://github.com/Akegarasu/lora-scripts
</span></span></code></pre></td></tr></table></div></div><p>即可将需要的库安装下来，然后安装环境和相关以来只需要执行 <code>.\install.ps1</code> 即可（该脚本有 cn 版本，但是可能会出现问题），其会安装 sd-scripts 和 lora-scripts 需要的库。具体的可以参考相关 repo（sd-script 详细说明，lora-script 有简化版说明）。</p><blockquote><p>安装的时候可能会出现虚拟环境未激活的问题，我们可以提前在改目录执行一次 python -m venv venv 一次即可。</p></blockquote><p>Finish.</p><h3 id=数据集准备>数据集准备<a hidden class=anchor aria-hidden=true href=#数据集准备>#</a></h3><blockquote><p>准备数据集的时候，要根据当前的设备显存对图片进行预处理，避免图片的分辨率过大，导致显存爆了，这里可以使用微软自己的 Powertoys 对文件进行批量 resize。</p></blockquote><p><strong>数据需求：</strong></p><ul><li>如果希望有<strong>更好的泛化性</strong>，训练素材中应该包含各种：角度、表情、光线。<ul><li>如果是角色的话，建议尽可能的手机正面，侧面，背面，头像特写，</li><li>画风素材可以多一点</li><li>可以考虑扣白底： <a href=https://pickwant.com/home target=_blank rel=noopener>https://pickwant.com/home</a></li></ul></li><li>简单预处理：调整分辨率（64 的倍数），裁剪。</li><li>数据量：如果是角色训练集在 20~50 左右足够，但是重要的是训练数据和训练轮次之间需要根据可视化做一个协调，避免过拟合（有时候可能也允许过拟合，取决于使用的场景）或者欠拟合的情况发生，原则上讲数据数量和轮次是正相关的关系。</li></ul><p>预处理之<strong>生成图像描述</strong>：图像描述实际上是作为训练的标签存在，而显然，我们需要自动生成描述，如果还记得之前的文档，图生图功能中有<strong>反向推演提示词</strong>的方法，同样我们也会用该 Deepbooru 方法去生成标签，webui 中提供了一个专门的入口：</p><p><div class=post-img-view><a data-fancybox=gallery href=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/3070PC/20230507164237.png><img alt=image.png loading=lazy src=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/3070PC/20230507164237.png class=responsive-image src=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/3070PC/20230507164237.png style="display:block;margin:0 auto" alt=image.png></a></div><script>document.addEventListener("DOMContentLoaded",function(){var e=document.querySelectorAll(".responsive-image"),t=window.innerHeight/2.5;e.forEach(function(e){e.style.maxHeight=t+"px"})})</script></p><p>在 resize 完成后，我们执行改生成图像描述的方法，指定好原图像位置和预处理之后的文件夹名字位置即可。</p><ul><li>预处理之后的文件夹会包含原图和对应生成的描述 txt 文件。</li><li>生成的描述文件可以 check 一下进行手动修改，错误的标签可能会引导出错误的训练方向。</li><li>为了便于 LoRA 的后续描述生成相同的效果，可以删除部分不必要的标签，令重要的标签和效果的相关度提高，并覆盖部分细节标签，可以理解成把一系列标签打包成一个标签。</li></ul><p><strong>训练数据就位：</strong> 将训练数据转移到 <code>&lt;...>/lora-scripts/train/</code> 中，如果没有该 train 目录就创建一个，单次的训练数据放在一个文件夹中，例如 <code>&lt;..>/train/train_1_person/</code></p><p><strong>正则化数据准备</strong>(option)：在 train 中新建一个 reg 文件夹，用于正则化训练，命名和此次训练的文件夹名相同，例如：<code>&lt;..>/train/reg/train_1_person/</code>，在其中放的数据和训练数据应该是同类的不同对象。</p><blockquote><p>正则化的作用通常是避免模型过拟合，对模型添加额外的约束。例如我们训练一个猫，正则文件夹里应该同样存放猫（别的猫）的照片。</p></blockquote><p><strong>基础模型就位</strong>：将训练用到的基础模型复制到 <code>&lt;...>/lora-scripts/sd-models/</code> 中，lora 是针对基础模型的注入（额外的 FC），对原模型进行部分的调整。</p><p><strong>注意事项：</strong> 避免文件名重复，使用相同的文件后缀能够避免文件名重复的问题。</p><h3 id=训练脚本编辑>训练脚本编辑<a hidden class=anchor aria-hidden=true href=#训练脚本编辑>#</a></h3><p>better read: <a href=https://github.com/kohya-ss/sd-scripts/blob/main/train_network_README-zh.md target=_blank rel=noopener>sd-scripts/train_network_README-zh.md at main · kohya-ss/sd-scripts (github.com)</a></p><p>接下来就是编辑训练脚本中的基础设置，lora-scripts 中的注释已经非常详细的写好了每个参数的含义，这里就介绍一些可能需要或者常用的参数设置：</p><p>训练相关的一些设置如下</p><p><div class=post-img-view><a data-fancybox=gallery href=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/3070PC/20230507170822.png><img alt=image.png loading=lazy src=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/3070PC/20230507170822.png class=responsive-image src=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/3070PC/20230507170822.png style="display:block;margin:0 auto" alt=image.png></a></div><script>document.addEventListener("DOMContentLoaded",function(){var e=document.querySelectorAll(".responsive-image"),t=window.innerHeight/2.5;e.forEach(function(e){e.style.maxHeight=t+"px"})})</script></p><ul><li><code>save_every_n_epochs </code>会决定我们最终获得几个模型，这里也可以设置一下。</li><li><code>reg_data_dir</code> 设置正则化的目录，为空为默认不启用。</li><li>&mldr; 其他的看看注释</li></ul><p>还有一些输出相关的设置，包括输出模型的目录和名称，名称最好修改，避免一直都是默认的错误覆盖了：</p><blockquote><p>目录最好用 10_&lt;EN> 或者纯数字的目录，然后路径填写的时候只填写到上级目录，比如说数据存放在 <code>&lt;...>/train/10_ron</code>，那么脚本中就填写到 <code>./train</code>，正则化目录同理。</p></blockquote><p><div class=post-img-view><a data-fancybox=gallery href=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/3070PC/20230507171041.png><img alt=image.png loading=lazy src=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/3070PC/20230507171041.png class=responsive-image src=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/3070PC/20230507171041.png style="display:block;margin:0 auto" alt=image.png></a></div><script>document.addEventListener("DOMContentLoaded",function(){var e=document.querySelectorAll(".responsive-image"),t=window.innerHeight/2.5;e.forEach(function(e){e.style.maxHeight=t+"px"})})</script></p><p>其他参数设置简单推荐：</p><ul><li>unet_lr 一般=lr</li><li>lr_scheduler: <code>constant_with_warmup</code> 用的蛮多</li><li>优化器：<ul><li>use_lion 目前最好的，但是有正则化素材最好别开</li></ul></li></ul><p>编辑完成后直接执行该 <code>ps1</code> 脚本即可。</p><blockquote><p>脚本运行过程中可能会出现 nomodule named triton 的错误，可能是由于 windows 不支持该模块导致的，但是不影响最后的生成。</p></blockquote><h3 id=lora-模型测试和选择>LoRA 模型测试和选择<a hidden class=anchor aria-hidden=true href=#lora-模型测试和选择>#</a></h3><p>将模型导入 <code>/extensions/sd-webui-additional-networks/models/lora</code> 中，然后利用之前介绍过的 scripts 的 xyzplot 功能，分别测试和对比各个模型的效果，选择其中效果好的保留。</p><h3 id=成品展示--简单心得>成品展示 & 简单心得<a hidden class=anchor aria-hidden=true href=#成品展示--简单心得>#</a></h3><blockquote><p>用我家猫小荣的 10-14 张照片训练出来了小荣的 Lora 模型（DreamBooth）也训了一下，但是显存（3070）不太够。</p></blockquote><p>训练 Lora 中使用了如下的参数配置：</p><ul><li>Clilloutmix 作为底层模型</li><li>使用训练 DreamBooth 中生成的 700 张猫的图片作为正则化数据集</li><li>BatchSize=2，Lr=e-4, Lion, Cosine_with_restarts, Max_train_epoch=10</li></ul><p>此外，由于图片少（一致性太强）训练轮次多，应该是有些过拟合，Lora 调用时只能在 3 以下，不然难以和原图区分，可以对训练轮次和图片数量做一下调整。</p><p>训练的时候打标签除了自动生成的，最好还是主动去修改，各种细节剖开来，描述好姿势，颜色，背景，表情等，后面学到的模型对各个部分进行修改的化比较方便区分，避免全都耦合在一起。</p><p>此外标签中要给特定的对象一个特定的标签，方便我们产出该指定效果（或者角色）。</p><!doctype html><html lang=en><head><meta name=viewport content="user-scalable=no,width=device-width,initial-scale=1,maximum-scale=1"><script src=https://cdn.jsdmirror.com/npm/jquery@3.3.1/dist/jquery.min.js></script><link rel=stylesheet href=https://cdn.jsdmirror.com/npm/nanogallery2@3.0.5/dist/css/nanogallery2.min.css><script src=https://cdn.jsdmirror.com/npm/nanogallery2@3.0.5/dist/jquery.nanogallery2.min.js></script></head><body><div data-nanogallery2='{
	 "thumbnailDisplayTransition":          "none",
     "thumbnailDisplayTransitionDuration":  500,
     "thumbnailDisplayInterval":            30,
     "galleryDisplayTransition":            "none",
     "galleryDisplayTransitionDuration":    500,
     "galleryDisplayMode": "rows",
     "thumbnailDisplayOutsideScreen": "false",
     "eventsDebounceDelay": 10,
     "thumbnailL1BorderHorizontal": 0,
     "thumbnailL1BorderVertical": 0,
     "thumbnailLabel": {
        "titleFontSize": "0.6em"
     },
     "thumbnailHoverEffect2": "image_scale_1.00_1.10|label_backgroundColor_rgba(0,0,0,0)_rgba(255,255,255,0)",
     "galleryTheme": {
        "thumbnail": {
            "borderRadius": "8px"
        }
     },
     "thumbnailToolbarImage": {
        "topLeft": "",
        "topRight": "",
        "bottomLeft": "",
        "bottomRight": ""
     },
     "viewerToolbar":   {
        "display": true,
        "standard": "label"
     },
     "viewerTools":     {
        "topLeft":    "pageCounter, playPauseButton",
        "topRight":   "downloadButton, rotateLeft, zoomButton, fullscreenButton, closeButton"
     },
     "viewerGalleryTWidth": 40,
     "viewerGalleryTHeight": 40
}'><a href=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/3070PC/00009-3706840146.png data-ngthumb=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/3070PC/00009-3706840146.png>00009-3706840146.png</a>
<a href=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/3070PC/00021-1903439770.png data-ngthumb=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/3070PC/00021-1903439770.png>00021-1903439770.png</a>
<a href=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/3070PC/00031-4141742147.png data-ngthumb=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/3070PC/00031-4141742147.png>00031-4141742147.png</a>
<a href=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/3070PC/00034-1003575578.png data-ngthumb=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/3070PC/00034-1003575578.png>00034-1003575578.png</a>
<a href=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/3070PC/00000-4244496505.png data-ngthumb=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/3070PC/00000-4244496505.png>00000-4244496505.png</a>
<a href=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/3070PC/00021-3282103832.png data-ngthumb=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/3070PC/00021-3282103832.png>00021-3282103832.png</a>
<a href=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/3070PC/00024-4215830362.png data-ngthumb=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/3070PC/00024-4215830362.png>00024-4215830362.png</a>
<a href=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/3070PC/00056-533810957.png data-ngthumb=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/3070PC/00056-533810957.png>00056-533810957.png</a>
<a href=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/3070PC/00001-2335627500.png data-ngthumb=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/3070PC/00001-2335627500.png>00001-2335627500.png</a></div></body></html><h2 id=dreambooth>Dreambooth<a hidden class=anchor aria-hidden=true href=#dreambooth>#</a></h2><blockquote><p>DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation。
这种训练方式旨在微调 diffusion model 使其生成的图像专注于某个 object，具体可以看下面的示例图</p></blockquote><p>相关资源链接： <a href=https://github.com/d8ahazard/sd_dreambooth_extension target=_blank rel=noopener>webui训练插件</a>
| <a href=https://github.com/google/dreambooth target=_blank rel=noopener>offical repo</a>
| <a href=https://arxiv.org/abs/2208.12242 target=_blank rel=noopener>Paper</a>
| <a href=https://juejin.cn/post/7219968440760582205 target=_blank rel=noopener>Dreambooth原理与实践</a></p><blockquote><p>如果需要深入了解实现原理的可以查看官方论文和对应的 repo 内容。</p></blockquote><p><div class=post-img-view><a data-fancybox=gallery href=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/mac/20230511172006.png><img alt=image.png loading=lazy src=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/mac/20230511172006.png class=responsive-image src=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/mac/20230511172006.png style="display:block;margin:0 auto" alt=image.png></a></div><script>document.addEventListener("DOMContentLoaded",function(){var e=document.querySelectorAll(".responsive-image"),t=window.innerHeight/2.5;e.forEach(function(e){e.style.maxHeight=t+"px"})})</script></p><p>其具备以下的优势：</p><ul><li>仅需要同一个<strong>主体（动物，模型，等）</strong> 的少量图片即可（3 到 5 张可以，但是越多越好）</li><li>可以基于该主体生成各种图片</li></ul><blockquote><p>针对该场景考虑将训练数据扣成白底可能也会使得生成的效果更好。</p></blockquote><p>具体的训练的详细介绍，可以参考训练插件页面的说明进行尝试，这里简单介绍一下工作流程：</p><p><strong>A. 模型创建</strong>：Dreambooth tab -> &ldquo;Create Model&rdquo; sub-tab -> 确定一个新的模型名称 ->选择本地的模型 / HF(model URL & token) -> 确定后源 ckpt 会暂时存储在 <code>models\dreambooth\MODELNAME\working</code> -> 点击 Create</p><p><strong>B. 参数设置</strong>：Settings tab -> 是否使用 LoRA、BatchSize、Epochs 、学习率等等的设置都是基本的参数就不在介绍</p><blockquote><p>设置(settings)中的 <code>performance wizard (WIP)</code> 中可以查看建议的参数。</p></blockquote><p><strong>C. Concepts 设置</strong>：主要有两个类别：数据路径 + （class + prompt）</p><ul><li>数据路径不在赘述</li><li>Prompt 指的是我们的目标样本应该用什么提示词</li><li>Class 填入的是与目标同类但是不同个体的样本，避免过拟合</li></ul><p><strong>D. Saving 设置</strong>： 主要是设置我们各种模型的保存策略，类似步长之类的。</p><p><strong>E. Generate 设置</strong>：生成过程中的调整</p><p>设置完成后开始训练即可，可以看训练中的 loss 和模型训练过程中生成的图片，AFK，训练完成后就可以看到训练好的 CKPT 了。</p><h2 id=textual-inversion>Textual Inversion<a hidden class=anchor aria-hidden=true href=#textual-inversion>#</a></h2><p>相关资源链接：<a href=https://arxiv.org/abs/2208.01618 target=_blank rel=noopener>论文</a>
 | <a href=https://github.com/rinongal/textual_inversion target=_blank rel=noopener>官方代码</a>
| <a href=https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Textual-Inversion target=_blank rel=noopener>webui说明</a>
| <a href=https://textual-inversion.github.io/ target=_blank rel=noopener>textual-inversion.github.io</a></p><blockquote><p>其全称为 An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion</p></blockquote><p><strong>Textual Inversion</strong>： 功能和其名字相互对应，反转 Text2Img 的过程，Img2Text 地建立图像与指定文本（Prompt）的关联。也就是说，当我们希望输入特定的 Prompt 能稳定的产生某个效果的时候，可以利用 Textual Inversion 技术来对模型进行改造，使得该特定的 prompt 能取得特定效果。</p><p>实际上该技术也能取得和 Dreambooth 类似的效果，具体可以看论文和例子，这里也只简单介绍一下训练和使用。</p><p><div class=post-img-view><a data-fancybox=gallery href=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/3070PC/20230512085236.png><img alt=image.png loading=lazy src=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/3070PC/20230512085236.png class=responsive-image src=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/3070PC/20230512085236.png style="display:block;margin:0 auto" alt=image.png></a></div><script>document.addEventListener("DOMContentLoaded",function(){var e=document.querySelectorAll(".responsive-image"),t=window.innerHeight/2.5;e.forEach(function(e){e.style.maxHeight=t+"px"})})</script></p><p><strong>A. 创建 Word Embedding</strong>（指定 Prompt）：</p><p>在 Train 标签页下创建 Embedding 页面，name 指定一个关键字（prompt），该 prompt 代表我们后续要训练出来的概念，创建完成后，就可以在 <code>/&lt;...>/stable-diffusion-webui/embeddings</code> 路径下看到我们创建的 pt 文件</p><p><strong>B.数据预处理</strong>：同 Lora 介绍的预处理图像，准备好训练数据</p><p><strong>C.参数设置</strong>：</p><ul><li>Train-> Train 页面中，只训练 Embedding，所以 Hypernetwork 的地方放空。</li><li>填写的基本学习率等参数就不再赘述，</li><li><strong>填写</strong>：数据集目录、Log 目录等即可</li><li>&ldquo;Prompt template"需要注意下, 提供了几种可选的训练模式:<ul><li>style_filewords.txt: 表示训练画风</li><li>subject_filewords.txt: 表示训练人物或物体</li></ul></li></ul><p><strong>D.选择训练 Embedding</strong>：即可开始训练</p><p><strong>E.使用</strong>：放在 <code>/&lt;...>/stable-diffusion-webui/embeddings</code> 中，在相关生成过程中，填写 Prompt 的时候像 easynagetive 启用即可。</p><h2 id=hypernetworks>HyperNetworks<a hidden class=anchor aria-hidden=true href=#hypernetworks>#</a></h2><p><a href=https://blog.novelai.net/novelai-improvements-on-stable-diffusion-e10d38db82ac target=_blank rel=noopener>NovelAI Improvements on Stable Diffusion | by NovelAI | Medium</a></p><blockquote><p>Hypernetwork 是一种微调技术（by Novel AI ），它是一个<strong>小型附加神经网络</strong>附加在 Stable Diffusion 模型上以修改其风格，这种方式和当时的论文并不一致，当时的 HyperNetwork 是通过修改权重来进行调整，NovelAI 中使用到的则是添加一个小型的线性附加网络。</p></blockquote><p><div class=post-img-view><a data-fancybox=gallery href=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/mac/20230706170343.png><img alt=image.png loading=lazy src=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/mac/20230706170343.png class=responsive-image src=https://picture-bed-001-1310572365.cos.ap-guangzhou.myqcloud.com/mac/20230706170343.png style="display:block;margin:0 auto" alt=image.png></a></div><script>document.addEventListener("DOMContentLoaded",function(){var e=document.querySelectorAll(".responsive-image"),t=window.innerHeight/2.5;e.forEach(function(e){e.style.maxHeight=t+"px"})})</script></p><p>具体而言，其插入到噪声预测器 UNet 的交叉注意力模块中，（通常情况下是一个简单的神经网络：具有 dropout 和激活函数的全连接线性网络）通过插入两个转换 key 和 query 向量的网络来劫持交叉注意力模块。</p><p>其训练过程与 Text Inversion 几乎一致，这里就不在赘述，只描述部分不同：</p><ul><li>创建模型的时候在 Train/Create Hypernetwork 选项卡</li><li>训练的时候选择 train hypernetwork</li><li>使用时存放的目录（应该也可以存放在 Addition Network 的文件夹中）为： <code>stable_difusion\stable-diffusion-webui\models\hypernetworks</code> 像 Lora 一样启用即可。</li></ul><h2 id=fi>FI<a hidden class=anchor aria-hidden=true href=#fi>#</a></h2><p>stable diffusion 的介绍暂时就到这边了，原理相关的东西就先不介绍了，以后如果有必要的话，或者谁有需求的话可以联系我更新。</p><ul><li><input checked disabled type=checkbox> 通过训练小荣来给出一个详细的参数设置（Dreambooth 和 Lora 和 TextInversion）</li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://aikenh.cn/hugotest/tags/ai/>AI</a></li></ul><nav class=paginav><a class=prev href=https://aikenh.cn/hugotest/posts/linuxzellij/><span class=title>« Prev</span><br><span>Terminal multiplexer Zellij</span>
</a><a class=next href=https://aikenh.cn/hugotest/posts/immich_backup/><span class=title>Next »</span><br><span>Docker-App 1 Backup Photos by Immich</span></a></nav></footer><div id=disqus_thread></div><script>function loadDisqus(){var e=document,t=e.createElement("script");t.src="https://aiken-hugo.disqus.com/embed.js",t.setAttribute("data-timestamp",+new Date),(e.head||e.body).appendChild(t),window.disqus_config=function(){this.page.url=window.location.href,this.page.identifier=window.location.href.substring(18)}}var runningOnBrowser=typeof window!="undefined",isBot=runningOnBrowser&&!("onscroll"in window)||typeof navigator!="undefined"&&/(gle|ing|ro|msn)bot|crawl|spider|yand|duckgo/i.test(navigator.userAgent),supportsIntersectionObserver=runningOnBrowser&&"IntersectionObserver"in window;setTimeout(function(){if(!isBot&&supportsIntersectionObserver){var e=new IntersectionObserver(function(t){t[0].isIntersecting&&(loadDisqus(),e.disconnect())},{threshold:[0]});e.observe(document.getElementById("disqus_thread"))}else loadDisqus()},1)</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by
Disqus.</a></noscript></article></main><footer class=footer><span>&copy; 2024 <a href=https://aikenh.cn/hugotest/>aiken's blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a>
</span><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><span id=busuanzi_container>Visitors: <span id=busuanzi_value_site_uv></span>
Views: <span id=busuanzi_value_site_pv></span></span></footer><script>document.addEventListener("DOMContentLoaded",function(){const e=document.getElementById("busuanzi_value_site_uv"),t=document.getElementById("busuanzi_value_site_pv"),o=13863,i=16993;if(!e||!t){console.error("Busuanzi elements not found.");return}const n=new MutationObserver(e=>{for(let t of e)if(t.type==="childList"){n.disconnect(),t.target.innerHTML=parseInt(t.target.innerHTML||0)+o;break}}),s=new MutationObserver(e=>{for(let t of e)if(t.type==="childList"){s.disconnect(),t.target.innerHTML=parseInt(t.target.innerHTML||0)+i;break}});n.observe(e,{childList:!0}),s.observe(t,{childList:!0})})</script><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><span class=topInner><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
<span id=read_progress></span>
</span></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))}),document.getElementById("theme-toggle-nav").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script><script>document.addEventListener("scroll",function(){const t=document.getElementById("read_progress"),n=document.documentElement.scrollHeight,s=document.documentElement.clientHeight,o=document.documentElement.scrollTop||document.body.scrollTop;t.innerText=((o/(n-s)).toFixed(2)*100).toFixed(0)})</script><script>(function(e,t){var s=document,o="script",n=s.createElement(o),i=s.getElementsByTagName(o)[0];n.src=e,t&&n.addEventListener("load",function(e){t(e)}),i.parentNode.insertBefore(n,i)})("/js/pangu.js",function(){pangu.spacingPage()})</script></body></html>