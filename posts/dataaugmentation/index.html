<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Data Augmentation | aiken's blog</title>
<meta name=keywords content="Augmentation,Pytorch,Machine Learning"><meta name=description content="intergrate with those augmentation method.
this doc will

Record those theory and the effect after transformation
Show the codes for ez use

And the complete .py will be intergrate in my classification pipeline
reference below:arrow_down_small:, if use them,start it for respect for his work.

aleju/imgaug

:star:albumentations-team/albumentations: 

torchvision

PIL/ImageEnhance CCBS

opencv

Principle
Principle 1 of coding: Don’t reinvent the wheel unless it’s needed

具体而言，仅在函数的拓展性较差，无法对其定制化，满足我们的日常需求的时候，我们会自行编写函数从而满足我们的需求，否则我们直接引用已知的库，提升我们的实现效率。


Principle 2 of coding 图像增强的两种使用方式："><meta name=author content="aikenhong"><link rel=canonical href=https://aikenh.cn/hugotest/posts/dataaugmentation/><link crossorigin=anonymous href=/hugotest/assets/css/stylesheet.2f85ca17c12c62fa86b1e474b8a51aca4856f0d645debfe4922a4d5ddc6aa978.css integrity="sha256-L4XKF8EsYvqGseR0uKUaykhW8NZF3r/kkipNXdxqqXg=" rel="preload stylesheet" as=style><link rel=icon href=https://aikenh.cn/favicon/ghost.ico><link rel=icon type=image/png sizes=16x16 href=https://aikenh.cn/favicon/ghost-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://aikenh.cn/favicon/ghost-32x32.png><link rel=apple-touch-icon href=https://aikenh.cn/favicon/ghost-apple-touch-icon.png><link rel=mask-icon href=https://aikenh.cn/favicon/ghost-192x192.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://aikenh.cn/hugotest/posts/dataaugmentation/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script src=https://cdn.jsdmirror.com/npm/jquery@3.5.1/dist/jquery.min.js></script><link rel=stylesheet href=https://cdn.jsdmirror.com/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css><script src=https://cdn.jsdmirror.com/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js></script><link rel=stylesheet href=https://cdn.jsdmirror.com/npm/katex@0.16.11/dist/katex.min.css><script defer src=https://cdn.jsdmirror.com/npm/katex@0.16.11/dist/katex.min.js></script><script defer src=https://cdn.jsdmirror.com/npm/katex@0.16.11/dist/contrib/auto-render.min.js onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><link rel=stylesheet href=https://cdn.jsdmirror.com/npm/lxgw-wenkai-webfont@1.1.0/style.css><link rel=stylesheet href=https://cdn.jsdmirror.com/npm/lxgw-wenkai-lite-webfont@1.1.0/style.css><link rel=stylesheet href=https://cdn.jsdmirror.com/npm/lxgw-wenkai-tc-webfont@1.0.0/style.css><link rel=stylesheet href=https://cdn.jsdmirror.com/npm/lxgw-wenkai-screen-webfont@1.1.0/style.css><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Open+Sans:ital,wght@0,300..800;1,300..800&family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&family=Ubuntu+Mono:ital,wght@0,400;0,700;1,400;1,700&family=Ubuntu:ital,wght@0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap" rel=stylesheet><meta property="og:url" content="https://aikenh.cn/hugotest/posts/dataaugmentation/"><meta property="og:site_name" content="aiken's blog"><meta property="og:title" content="Data Augmentation"><meta property="og:description" content="intergrate with those augmentation method.
this doc will
Record those theory and the effect after transformation Show the codes for ez use And the complete .py will be intergrate in my classification pipeline
reference below:arrow_down_small:, if use them,start it for respect for his work.
aleju/imgaug :star:albumentations-team/albumentations: torchvision PIL/ImageEnhance CCBS opencv Principle Principle 1 of coding: Don’t reinvent the wheel unless it’s needed
具体而言，仅在函数的拓展性较差，无法对其定制化，满足我们的日常需求的时候，我们会自行编写函数从而满足我们的需求，否则我们直接引用已知的库，提升我们的实现效率。 Principle 2 of coding 图像增强的两种使用方式："><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-11-28T06:24:20+00:00"><meta property="article:modified_time" content="2021-11-28T06:24:20+00:00"><meta property="article:tag" content="Augmentation"><meta property="article:tag" content="Pytorch"><meta property="article:tag" content="Machine Learning"><meta property="og:image" content="https://aikenh.cn/cover/cover12.jpeg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://aikenh.cn/cover/cover12.jpeg"><meta name=twitter:title content="Data Augmentation"><meta name=twitter:description content="intergrate with those augmentation method.
this doc will

Record those theory and the effect after transformation
Show the codes for ez use

And the complete .py will be intergrate in my classification pipeline
reference below:arrow_down_small:, if use them,start it for respect for his work.

aleju/imgaug

:star:albumentations-team/albumentations: 

torchvision

PIL/ImageEnhance CCBS

opencv

Principle
Principle 1 of coding: Don’t reinvent the wheel unless it’s needed

具体而言，仅在函数的拓展性较差，无法对其定制化，满足我们的日常需求的时候，我们会自行编写函数从而满足我们的需求，否则我们直接引用已知的库，提升我们的实现效率。


Principle 2 of coding 图像增强的两种使用方式："><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://aikenh.cn/hugotest/posts/"},{"@type":"ListItem","position":2,"name":"Data Augmentation","item":"https://aikenh.cn/hugotest/posts/dataaugmentation/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Data Augmentation","name":"Data Augmentation","description":"intergrate with those augmentation method.\nthis doc will\nRecord those theory and the effect after transformation Show the codes for ez use And the complete .py will be intergrate in my classification pipeline\nreference below:arrow_down_small:, if use them,start it for respect for his work.\naleju/imgaug :star:albumentations-team/albumentations: torchvision PIL/ImageEnhance CCBS opencv Principle Principle 1 of coding: Don’t reinvent the wheel unless it’s needed\n具体而言，仅在函数的拓展性较差，无法对其定制化，满足我们的日常需求的时候，我们会自行编写函数从而满足我们的需求，否则我们直接引用已知的库，提升我们的实现效率。 Principle 2 of coding 图像增强的两种使用方式：\n","keywords":["Augmentation","Pytorch","Machine Learning"],"articleBody":"intergrate with those augmentation method.\nthis doc will\nRecord those theory and the effect after transformation Show the codes for ez use And the complete .py will be intergrate in my classification pipeline\nreference below:arrow_down_small:, if use them,start it for respect for his work.\naleju/imgaug :star:albumentations-team/albumentations: torchvision PIL/ImageEnhance CCBS opencv Principle Principle 1 of coding: Don’t reinvent the wheel unless it’s needed\n具体而言，仅在函数的拓展性较差，无法对其定制化，满足我们的日常需求的时候，我们会自行编写函数从而满足我们的需求，否则我们直接引用已知的库，提升我们的实现效率。 Principle 2 of coding 图像增强的两种使用方式：\n做全集的增强后存储在本地，然后通过随机载入或者按一定batch的载入来实现我们增强的作用，（or contrasive），这种方式实际上是使用空间来换时间，由于处理是一次性的，所以如果空间充足的话，是更为充足的方式。\n动态的在线增强：这种方式比较消耗io和cpu，不推荐，但是如果本地的空间不够，就只能采用这种方式了。\nPrinciple 3 of saving 如果我们要存储本地副本的话，推荐的存储格式和方式\n文件格式：npz 由于多种增强，实际上这种方式还蛮适合使用npz格式作为我们的存储，这种既保留了对应的np还可以保留对应的字典信息，此外这种方式的存取速度也不算慢，（相较之下好像没有特别突出的一种格式）\n路径格式：imagenet也就是对应的train-class-data的层级关系，通过这种约定俗称的存储关系我们得以在我们框架中的dataset格式方便读入\nPIL，CV2，SkImage的选择 pytorch图像的加载/读取方式 | cv2、PIL、matplotlib 在这里讲解三个模块之间的基本区别和其中的选择，目前希望将已有的算法从PIL转向CV2，后续也会添加一下三个模块对于Torch的适配性等等\n数据读写 基于下列的代码和注释给出相应的之间的区别，通过这些区别我们可以知道几乎所有的格式在使用的时候都是需要对应的转换的，\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 import cv2 img = cv2.imread('lena.png') # numpy格式，HWC，【0，255】，BGR img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB) # 除了最下方的转换方式外也可以通过这种方式进行转换 cv2.imwrite('lena.jpg', img) # img 需为BGR格式的numpy array import PIL from PIL import Image img = Image.open('lena.png') # PIL格式，RGB，【0，255】 img = np.array(img) # 转换为numpy格式, HWC Img.fromarray(img).save('lena.png') # x需要转换为PIL.Image格式才能保存 from skimage import io img = io.imread('lena.png') # numpy 格式， RGB，【0，1】 HWC io.imsave('lena.jpg') import matplotlib.pyplot as plt img = plt.imread('lena.png') # numpy 格式，HWC [0,255] RGB plt.imsave('lena.jpg') # BGR 转换 RGB, 转回来也是一样的操作 img_rgb = img_bgr[:,:,::-1] 基于上述的代码，我们可以总结出：\n当我们使用matplotlib的时候\n当我们用cv2读取图像的时候若要进行matplotlib的展示，我们需要对其做bgr2rgb的转换，PIL则需要做到numpy的转换，skimage考虑归一化方面的问题 当我们要使用tensror格式的时候\n我们需要对cv2转换成RGB\n对三个方法都是用transpose或者类似的方法转换到CHW\n1 2 3 import torch import numpy as np torch_img = torch.from_numpy(np.transpose(img,(2,0,1))) 简单图像增强 在这里由于opencv更为强大且全面，我们将框架转移到opencv中进行图像增强处理，于是本章节会主要介绍opencv, torchvision中的图像增强，同时也会对pillow,skimage`进行简单的介绍。\nTorchVision torchvision的使用实际上是最简单便捷的，为了协调统一，该类变换我们在totensor后进行使用，接在其他所有变换的后面，实际上有一些变换是可以获取参数的，要调用的对应函数我们可以在对应文档 中查询\n**随机机制：**现已向后兼容torch\n1 2 3 4 5 6 7 8 9 # 随机种子采用torch import torch torch.manual_seed(17) # KEY FUNCTION: 给一个transformers加上概率，以一定的概率执行该操作 transformers.RandomApply(transforms,p=0.5) # Key Function：从给定的一系列transforms中选一个进行操作 transformers.RandomChoice(transforms) 由于存在randomapply这个函数，所以实际上我们在调用变换的时候，我们可以用prefix random去搜补全，如果没有的话，也可以使用RandomApply来手动赋予随机性。\n**组合机制：**同时使用多种变换，这种方法将一组强关联的变换进行组合，简化后续的使用，但是对于我们如果需要做多种增强的话，实际上并不是一个合适的方式。\n1 2 3 4 5 6 # a simple example for torchvision's transformer transform = transforms.Compose([ transforms.CenterCrop(10), transforms.PILtoTensor(), transforms.ConvertImageDtype(torch.float) ]) 常见的一些增强：列在下面 裁剪系列：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # cental crop transforms.CenterCrop(size), # five corners and the cental crop transforms.FiveCrop(size), lambda(lambda crops: torch.stack[ToTensor()(crop) for crop in crops]) # 随机裁剪到对应的尺寸， transforms.RandomCrop(size, padding=None, pad_if_needed=False, fill=0, padding_mode='constant') # 随机裁剪后resize到指定的尺寸 transforms.RandomResizedCropsize, scale=(0.08, 1.0), ratio=(0.75, 1.33), interpolation=\u003cInterpolationMode.BILINEAR: 'bilinear'\u003e) # Padding 无需多言 transformers.Pad(padding,fill=0,padding_mode='constant') 色彩变换增强\n1 2 3 4 5 # 色彩抖动,随机改变亮度对比度饱和度和色调 transforms.Colorjitter(brightness=0, contrast=0, saturation=0, hue=0) # 图像转换为灰度 transforms.grayscale(num_output_channel=1) 几何变换增强\n1 2 3 4 5 6 7 8 # 仿射变换 transformers.RandomAffine(degrees, translate=None, scale=None, shear=None, interpolation=\u003cInterpolationMode.NEAREST: 'nearest'\u003e, fill=0, fillcolor=None, resample=None) # 可以通过以下函数获取变换矩阵 get_params(degrees: List[float], translate: Optional[List[float]], scale_ranges: Optional[List[float]], shears: Optional[List[float]], img_size: List[int]) → Tuple[float, Tuple[int, int], float, Tuple[float, float]] transforms.RandomHorizontalFlip(p=0.5) transforms.RandomPerspective(distortion_scale=0.5, p=0.5, interpolation=\u003cInterpolationMode.BILINEAR: 'bilinear'\u003e, fill=0) RandomResizedCrop OpenCV Install Ubuntu：安装opencv的python版本\n1 2 apt-get -y python3-opencv pip install opencv-python Usage 这一部分可能主要还是对OpenCV基础使用方式的介绍，至于数据增强方面，上面有一个库已经集成了很大一部分图像增强的操作，且在效率上也有了很高的优化和证实，简单的使用方式如下。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 try: import albumentations as A except ImportError: os.system(\"pip install - U albumentations\") import albumentations as A import cv2 # Declare an augmentation pipeline transform = A.Compose([ A.RandomCrop(width=256, height=256), A.HorizontalFlip(p=0.5), A.RandomBrightnessContrast(p=0.2), ]) # Read an image with OpenCV and convert it to the RGB colorspace image = cv2.imread(\"image.jpg\") image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # Augment an image transformed = transform(image=image) transformed_image = transformed[\"image\"] 下面我们会讲一些opencv中的简单使用方式：\n(一)OpenCV-Python学习—基础知识 - silence_cho - 博客园 (cnblogs.com) Pillow Using PIL do some augmentation CCBS： color，contrast，brightness，shapen 1 2 3 4 # way1：PIL from PIL import ImageEnhance ccbs_img = ImageEnhance.Color(img).enhance(factor1) ... Blur，Detail，Edge_Enhance，Smooth，Sharpen…\n自行在文档中找到对应的方法，实际上不是很多\n1 2 3 4 5 from PIL import Image from PIL.ImageFillter import SMOOTH,BLUR img = Image.open(image_pah) img_1 = img.filtter(BLUR) img_2 = img.filtter(SMOOTH) SkImage …\n混合图像增强（For ML） ==ALL in one 不是什么好点子== 为了对Machine Learning中的任务进行图像增强任务，我们在过程中可能会使用一些Github Repo，主要可能就是albumation，在进行图像增强和数据混合的过程中，我们会遇到的问题包括：\nPIL和NP，Tensor的三者格式不对应的关系 Channel混杂的关系 调用transformer的形式不统一的问题 为此我们特地开了这个专题，介绍一下使用的方式，以及使我们在后续的使用过程中注意最终转换到datalist中最好采用统一的存储形式\npath（load by cv or pil，st np） data （np（else），tensor（cuda）for instant use） 以此来规范我们的dataset，and sampler or mixer\n我们可以尝试使用如下的方式来进行transformer的管理操作，此外如果我们要用的是纯粹的数据增强而不是用来dataloader的transformer，我们就不要用这个逻辑去做，我们直接集成Augmentation就好了 ch\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 class mytransformer(): # if augs is not none we need to add ToPIL to get the right data type def get_transformers(): self.transformer = { 'augs': Augs.Compose([]), 'trans': transformers.Compose([ tranformers.ToPIL(), ... ]), } # then we using k-v pair to do all my transformer, def __call__(img): for k,v in transformer: img = transformer['augs'](image=img)['image'] img = transformer['trans'](img) return img 特殊图像增强 …\n","wordCount":"624","inLanguage":"en","image":"https://aikenh.cn/cover/cover12.jpeg","datePublished":"2021-11-28T06:24:20Z","dateModified":"2021-11-28T06:24:20Z","author":[{"@type":"Person","name":"aikenhong"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://aikenh.cn/hugotest/posts/dataaugmentation/"},"publisher":{"@type":"Organization","name":"aiken's blog","logo":{"@type":"ImageObject","url":"https://aikenh.cn/favicon/ghost.ico"}}}</script></head><body id=top><script type=module src=https://cdn.jsdmirror.com/npm/ionicons@7.1.0/dist/ionicons/ionicons.esm.js defer></script><script nomodule src=https://cdn.jsdmirror.com/npm/ionicons@7.1.0/dist/ionicons/ionicons.js defer></script><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://aikenh.cn/hugotest/ accesskey=h title="aiken's blog (Alt + H)">aiken's blog</a><div class=logo-switches><button id=theme-toggle-nav accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://aikenh.cn/hugotest/ title=home><span>home</span></a></li><li><a href=https://aikenh.cn/hugotest/posts/ title=posts><span>posts</span></a></li><li><a href=https://aikenh.cn/hugotest/tags/ title=tags><span>tags</span></a></li><li><a href=https://aikenh.cn/hugotest/categories/ title=categories><span>categories</span></a></li><li><a href=https://aikenh.cn/hugotest/archives/ title=archives><span>archives</span></a></li><li><a href=https://aikenh.cn/hugotest/about/ title=about><span>about</span></a></li><li><a href=https://aikenh.cn/hugotest/search title="search (Alt + /)" accesskey=/><span>search</span></a></li></ul></nav></header><div class=sidebar><ul><li class=logo style=--bg:#333><a href=#><div class=logo-icon><img src=/logo/logo.png></div><div class=logo-text>Aiken's Blog</div></a></li><div class=menulist><li style=--bg:#f44336><a href=https://aikenh.cn/hugotest/ title=home><div class=logo-icon><ion-icon name=home-outline></ion-icon></div><div class=logo-text>home</div></a></li><li style=--bg:#b145e9><a href=https://aikenh.cn/hugotest/posts/ title=posts><div class=logo-icon><ion-icon name=newspaper-outline></ion-icon></div><div class=logo-text>posts</div></a></li><li style=--bg:#0f93c7><a href=https://aikenh.cn/hugotest/tags/ title=tags><div class=logo-icon><ion-icon name=pricetags-outline></ion-icon></div><div class=logo-text>tags</div></a></li><li style=--bg:#ffa117><a href=https://aikenh.cn/hugotest/categories/ title=categories><div class=logo-icon><ion-icon name=grid-outline></ion-icon></div><div class=logo-text>categories</div></a></li><li style=--bg:#0fc70f><a href=https://aikenh.cn/hugotest/archives/ title=archives><div class=logo-icon><ion-icon name=folder-outline></ion-icon></div><div class=logo-text>archives</div></a></li><li style=--bg:#d16111><a href=https://aikenh.cn/hugotest/about/ title=about><div class=logo-icon><ion-icon name=person></ion-icon></div><div class=logo-text>about</div></a></li><li style=--bg:#15c095><a href=https://aikenh.cn/hugotest/search title="search (Alt + /)" accesskey=/><div class=logo-icon><ion-icon name=search></ion-icon></div><div class=logo-text>search</div></a></li></div><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt +T)"><li><div class=logo-icon id=moon><ion-icon name=moon-outline></ion-icon></div><div class=logo-icon id=sun><ion-icon name=sunny-outline></ion-icon></div></li></button></div></ul></div><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://aikenh.cn/hugotest/>Home</a>&nbsp;»&nbsp;<a href=https://aikenh.cn/hugotest/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Data Augmentation</h1><div class=post-meta><span title='2021-11-28 06:24:20 +0000 UTC'>November 28, 2021</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;624 words&nbsp;·&nbsp;aikenhong&nbsp;·&nbsp;<a href=/tags/augmentation> Augmentation</a>&nbsp;·&nbsp;<a href=/tags/pytorch> Pytorch</a>&nbsp;·&nbsp;<a href=/tags/machine-learning> Machine Learning</a>&nbsp;|&nbsp;<a href=https://github.com/%3cpath_to_repo%3e/content/posts/DataAugmentation.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><figure class=entry-cover><img loading=eager src=https://aikenh.cn/cover/cover12.jpeg alt></figure><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#principle aria-label=Principle>Principle</a></li><li><a href=#pilcv2skimage%e7%9a%84%e9%80%89%e6%8b%a9 aria-label=PIL，CV2，SkImage的选择>PIL，CV2，SkImage的选择</a><ul class=header-level-2><li><a href=#%e6%95%b0%e6%8d%ae%e8%af%bb%e5%86%99 aria-label=数据读写>数据读写</a></li></ul></li><li><a href=#%e7%ae%80%e5%8d%95%e5%9b%be%e5%83%8f%e5%a2%9e%e5%bc%ba aria-label=简单图像增强>简单图像增强</a><ul class=header-level-2><li><a href=#torchvision aria-label=TorchVision>TorchVision</a></li><li><a href=#opencv aria-label=OpenCV>OpenCV</a><ul class=header-level-3><li><a href=#install aria-label=Install>Install</a></li><li><a href=#usage aria-label=Usage>Usage</a></li></ul></li><li><a href=#pillow aria-label=Pillow>Pillow</a></li><li><a href=#skimage aria-label=SkImage>SkImage</a></li></ul></li><li><a href=#%e6%b7%b7%e5%90%88%e5%9b%be%e5%83%8f%e5%a2%9e%e5%bc%bafor-ml aria-label="混合图像增强（For ML）">混合图像增强（For ML）</a></li><li><a href=#%e7%89%b9%e6%ae%8a%e5%9b%be%e5%83%8f%e5%a2%9e%e5%bc%ba aria-label=特殊图像增强>特殊图像增强</a></li></ul></div></details></div></aside><script>let activeElement,elements;document.addEventListener("DOMContentLoaded",function(){if(checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),elements.length>0){activeElement=elements[0];const e=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${e}"]`).classList.add("active")}const t=document.getElementById("top-link");t&&t.addEventListener("click",e=>{e.preventDefault(),window.scrollTo({top:0,behavior:"smooth"})})},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{const e=window.pageYOffset||document.documentElement.scrollTop;if(e===0)return;elements&&elements.length>0&&(elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase(),n=document.querySelector(`.inner ul li a[href="#${t}"]`);n.classList.remove("read")}),activeElement=Array.from(elements).find(t=>{if(getOffsetTop(t)-e>0&&getOffsetTop(t)-e<window.innerHeight/2)return t})||activeElement,elements.forEach((t)=>{const o=encodeURI(t.getAttribute("id")).toLowerCase(),s=document.querySelector(`.inner ul li a[href="#${o}"]`);if(t===activeElement){s.classList.add("active");const e=document.querySelector(".toc .inner"),t=s.offsetTop,n=e.clientHeight,o=s.clientHeight,i=t-n/2+o/2;e.scrollTo({top:i,behavior:"smooth"})}else getOffsetTop(t)<e&&s.classList.add("read"),s.classList.remove("active")}))},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return!document.querySelector(".hugo-encryptor-prompt")&&elements.length!=0&&(elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),console.log("Elements re-queried:",elements)),0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><p>intergrate with those augmentation method.</p><p>this doc will</p><ul><li>Record those theory and the effect after transformation</li><li>Show the codes for ez use</li></ul><p>And the complete <code>.py</code> will be intergrate in my classification pipeline</p><p><strong>reference</strong> below:arrow_down_small:, if use them,start it for respect for his work.</p><ul><li><a href=https://github.com/aleju/imgaug#documentation target=_blank rel=noopener>aleju/imgaug</a></li><li>:star:<a href=https://github.com/albumentations-team/albumentations target=_blank rel=noopener>albumentations-team/albumentations:</a></li><li><a href=https://pytorch.org/vision/stable/transforms.html#transforms-on-pil-image-and-torch-tensor target=_blank rel=noopener>torchvision</a></li><li><a href=https://pillow.readthedocs.io/en/stable/reference/ImageEnhance.html target=_blank rel=noopener>PIL/ImageEnhance CCBS</a></li><li>opencv</li></ul><h2 id=principle>Principle<a hidden class=anchor aria-hidden=true href=#principle>#</a></h2><p><strong>Principle 1</strong> of coding: Don’t reinvent the wheel unless it’s needed</p><ul><li>具体而言，仅在函数的拓展性较差，无法对其定制化，满足我们的日常需求的时候，我们会自行编写函数从而满足我们的需求，否则我们直接引用已知的库，提升我们的实现效率。</li></ul><p><strong>Principle 2</strong> of coding 图像增强的两种使用方式：</p><ul><li><p>做全集的增强后存储在本地，然后通过<strong>随机</strong>载入或者按一定<strong>batch</strong>的载入来实现我们增强的作用，（or contrasive），这种方式实际上是使用空间来换时间，由于处理是一次性的，所以如果空间充足的话，是更为充足的方式。</p></li><li><p>动态的在线增强：这种方式比较消耗io和cpu，不推荐，但是如果本地的空间不够，就只能采用这种方式了。</p></li></ul><p><strong>Principle 3</strong> of saving 如果我们要存储本地副本的话，推荐的存储格式和方式</p><ul><li><p><strong>文件格式</strong>：<code>npz</code> 由于多种增强，实际上这种方式还蛮适合使用<code>npz</code>格式作为我们的存储，这种既保留了对应的<code>np</code>还可以保留对应的字典信息，此外这种方式的存取速度也不算慢，（相较之下好像没有特别突出的一种格式）</p></li><li><p><strong>路径格式</strong>：<code>imagenet</code>也就是对应的train-class-data的层级关系，通过这种约定俗称的存储关系我们得以在我们框架中的dataset格式方便读入</p></li></ul><h2 id=pilcv2skimage的选择>PIL，CV2，SkImage的选择<a hidden class=anchor aria-hidden=true href=#pilcv2skimage的选择>#</a></h2><p><a href=https://www.jianshu.com/p/cfca9c4338e7 target=_blank rel=noopener>pytorch图像的加载/读取方式</a>
| <a href=https://zhuanlan.zhihu.com/p/357069891 target=_blank rel=noopener>cv2、PIL、matplotlib</a></p><p>在这里讲解三个模块之间的基本区别和其中的选择，目前希望将已有的算法从PIL转向CV2，后续也会添加一下三个模块对于Torch的适配性等等</p><h3 id=数据读写>数据读写<a hidden class=anchor aria-hidden=true href=#数据读写>#</a></h3><p>基于下列的代码和注释给出相应的之间的区别，通过这些区别我们可以知道几乎所有的格式在使用的时候都是需要对应的转换的，</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>cv2</span>
</span></span><span class=line><span class=cl><span class=n>img</span>  <span class=o>=</span> <span class=n>cv2</span><span class=o>.</span><span class=n>imread</span><span class=p>(</span><span class=s1>&#39;lena.png&#39;</span><span class=p>)</span> <span class=c1># numpy格式，HWC，【0，255】，BGR</span>
</span></span><span class=line><span class=cl><span class=n>img</span> <span class=o>=</span> <span class=n>cv2</span><span class=o>.</span><span class=n>cvtColor</span><span class=p>(</span><span class=n>img</span><span class=p>,</span><span class=n>cv2</span><span class=o>.</span><span class=n>COLOR_BGR2RGB</span><span class=p>)</span> <span class=c1># 除了最下方的转换方式外也可以通过这种方式进行转换</span>
</span></span><span class=line><span class=cl><span class=n>cv2</span><span class=o>.</span><span class=n>imwrite</span><span class=p>(</span><span class=s1>&#39;lena.jpg&#39;</span><span class=p>,</span> <span class=n>img</span><span class=p>)</span> <span class=c1># img 需为BGR格式的numpy array</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>PIL</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>PIL</span> <span class=kn>import</span> <span class=n>Image</span>
</span></span><span class=line><span class=cl><span class=n>img</span> <span class=o>=</span> <span class=n>Image</span><span class=o>.</span><span class=n>open</span><span class=p>(</span><span class=s1>&#39;lena.png&#39;</span><span class=p>)</span> <span class=c1># PIL格式，RGB，【0，255】</span>
</span></span><span class=line><span class=cl><span class=n>img</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>img</span><span class=p>)</span> <span class=c1># 转换为numpy格式, HWC</span>
</span></span><span class=line><span class=cl><span class=n>Img</span><span class=o>.</span><span class=n>fromarray</span><span class=p>(</span><span class=n>img</span><span class=p>)</span><span class=o>.</span><span class=n>save</span><span class=p>(</span><span class=s1>&#39;lena.png&#39;</span><span class=p>)</span> <span class=c1># x需要转换为PIL.Image格式才能保存</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>skimage</span> <span class=kn>import</span> <span class=n>io</span> 
</span></span><span class=line><span class=cl><span class=n>img</span> <span class=o>=</span> <span class=n>io</span><span class=o>.</span><span class=n>imread</span><span class=p>(</span><span class=s1>&#39;lena.png&#39;</span><span class=p>)</span> <span class=c1># numpy 格式， RGB，【0，1】 HWC</span>
</span></span><span class=line><span class=cl><span class=n>io</span><span class=o>.</span><span class=n>imsave</span><span class=p>(</span><span class=s1>&#39;lena.jpg&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>
</span></span><span class=line><span class=cl><span class=n>img</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>imread</span><span class=p>(</span><span class=s1>&#39;lena.png&#39;</span><span class=p>)</span> <span class=c1># numpy 格式，HWC [0,255] RGB</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>imsave</span><span class=p>(</span><span class=s1>&#39;lena.jpg&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># BGR 转换 RGB, 转回来也是一样的操作 </span>
</span></span><span class=line><span class=cl><span class=n>img_rgb</span> <span class=o>=</span> <span class=n>img_bgr</span><span class=p>[:,:,::</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span>
</span></span></code></pre></td></tr></table></div></div><p>基于上述的代码，我们可以总结出：</p><p>当我们使用matplotlib的时候</p><ul><li>当我们用cv2读取图像的时候若要进行matplotlib的展示，我们需要对其做<code>bgr2rgb</code>的转换，PIL则需要做到numpy的转换，skimage考虑归一化方面的问题</li></ul><p>当我们要使用tensror格式的时候</p><ul><li><p>我们需要对cv2转换成RGB</p></li><li><p>对三个方法都是用<code>transpose</code>或者类似的方法转换到CHW</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span> 
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl><span class=n>torch_img</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>from_numpy</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=n>img</span><span class=p>,(</span><span class=mi>2</span><span class=p>,</span><span class=mi>0</span><span class=p>,</span><span class=mi>1</span><span class=p>)))</span>
</span></span></code></pre></td></tr></table></div></div></li></ul><h2 id=简单图像增强>简单图像增强<a hidden class=anchor aria-hidden=true href=#简单图像增强>#</a></h2><p>在这里由于opencv更为强大且全面，我们将框架转移到opencv中进行图像增强处理，于是本章节会主要介绍<code>opencv</code>, <code>torchvision中的图像增强，同时也会对</code>pillow<code>,</code>skimage`进行简单的介绍。</p><h3 id=torchvision>TorchVision<a hidden class=anchor aria-hidden=true href=#torchvision>#</a></h3><p>torchvision的使用实际上是最简单便捷的，为了协调统一，该类变换我们在totensor后进行使用，接在其他所有变换的后面，实际上有一些变换是可以获取参数的，要调用的对应函数我们可以在对应<a href=https://pytorch.org/vision/stable/transforms.html#scriptable-transforms target=_blank rel=noopener>文档</a>
中查询</p><p>**随机机制：**现已向后兼容torch</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span><span class=lnt>9
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 随机种子采用torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span> 
</span></span><span class=line><span class=cl><span class=n>torch</span><span class=o>.</span><span class=n>manual_seed</span><span class=p>(</span><span class=mi>17</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># KEY FUNCTION: 给一个transformers加上概率，以一定的概率执行该操作</span>
</span></span><span class=line><span class=cl><span class=n>transformers</span><span class=o>.</span><span class=n>RandomApply</span><span class=p>(</span><span class=n>transforms</span><span class=p>,</span><span class=n>p</span><span class=o>=</span><span class=mf>0.5</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Key Function：从给定的一系列transforms中选一个进行操作</span>
</span></span><span class=line><span class=cl><span class=n>transformers</span><span class=o>.</span><span class=n>RandomChoice</span><span class=p>(</span><span class=n>transforms</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>由于存在<code>randomapply</code>这个函数，所以实际上我们在调用变换的时候，我们可以用prefix random去搜补全，如果没有的话，也可以使用<code>RandomApply</code>来手动赋予随机性。</p><p>**组合机制：**同时使用多种变换，这种方法将一组强关联的变换进行组合，简化后续的使用，但是对于我们如果需要做多种增强的话，实际上并不是一个合适的方式。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># a simple example for torchvision&#39;s transformer</span>
</span></span><span class=line><span class=cl><span class=n>transform</span> <span class=o>=</span> <span class=n>transforms</span><span class=o>.</span><span class=n>Compose</span><span class=p>([</span>
</span></span><span class=line><span class=cl>	<span class=n>transforms</span><span class=o>.</span><span class=n>CenterCrop</span><span class=p>(</span><span class=mi>10</span><span class=p>),</span>
</span></span><span class=line><span class=cl>	<span class=n>transforms</span><span class=o>.</span><span class=n>PILtoTensor</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>	<span class=n>transforms</span><span class=o>.</span><span class=n>ConvertImageDtype</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>float</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>])</span>
</span></span></code></pre></td></tr></table></div></div><p><strong>常见的一些增强：</strong><a href=https://www.bilibili.com/read/cv7313702/ target=_blank rel=noopener>列在下面</a></p><ul><li><p>裁剪系列：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># cental crop</span>
</span></span><span class=line><span class=cl><span class=n>transforms</span><span class=o>.</span><span class=n>CenterCrop</span><span class=p>(</span><span class=n>size</span><span class=p>),</span> 
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># five corners and the cental crop</span>
</span></span><span class=line><span class=cl><span class=n>transforms</span><span class=o>.</span><span class=n>FiveCrop</span><span class=p>(</span><span class=n>size</span><span class=p>),</span> 
</span></span><span class=line><span class=cl><span class=k>lambda</span><span class=p>(</span><span class=k>lambda</span> <span class=n>crops</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>stack</span><span class=p>[</span><span class=n>ToTensor</span><span class=p>()(</span><span class=n>crop</span><span class=p>)</span> <span class=k>for</span> <span class=n>crop</span> <span class=ow>in</span> <span class=n>crops</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 随机裁剪到对应的尺寸，</span>
</span></span><span class=line><span class=cl><span class=n>transforms</span><span class=o>.</span><span class=n>RandomCrop</span><span class=p>(</span><span class=n>size</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>pad_if_needed</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>fill</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span> <span class=n>padding_mode</span><span class=o>=</span><span class=s1>&#39;constant&#39;</span><span class=p>)</span> 
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 随机裁剪后resize到指定的尺寸</span>
</span></span><span class=line><span class=cl><span class=n>transforms</span><span class=o>.</span><span class=n>RandomResizedCropsize</span><span class=p>,</span> <span class=n>scale</span><span class=o>=</span><span class=p>(</span><span class=mf>0.08</span><span class=p>,</span> <span class=mf>1.0</span><span class=p>),</span> <span class=n>ratio</span><span class=o>=</span><span class=p>(</span><span class=mf>0.75</span><span class=p>,</span> <span class=mf>1.33</span><span class=p>),</span> <span class=n>interpolation</span><span class=o>=&lt;</span><span class=n>InterpolationMode</span><span class=o>.</span><span class=n>BILINEAR</span><span class=p>:</span> <span class=s1>&#39;bilinear&#39;</span><span class=o>&gt;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Padding 无需多言</span>
</span></span><span class=line><span class=cl><span class=n>transformers</span><span class=o>.</span><span class=n>Pad</span><span class=p>(</span><span class=n>padding</span><span class=p>,</span><span class=n>fill</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span><span class=n>padding_mode</span><span class=o>=</span><span class=s1>&#39;constant&#39;</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div></li><li><p>色彩变换增强</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 色彩抖动,随机改变亮度对比度饱和度和色调</span>
</span></span><span class=line><span class=cl><span class=n>transforms</span><span class=o>.</span><span class=n>Colorjitter</span><span class=p>(</span><span class=n>brightness</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span> <span class=n>contrast</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span> <span class=n>saturation</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span> <span class=n>hue</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 图像转换为灰度</span>
</span></span><span class=line><span class=cl><span class=n>transforms</span><span class=o>.</span><span class=n>grayscale</span><span class=p>(</span><span class=n>num_output_channel</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div></li><li><p>几何变换增强</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 仿射变换</span>
</span></span><span class=line><span class=cl><span class=n>transformers</span><span class=o>.</span><span class=n>RandomAffine</span><span class=p>(</span><span class=n>degrees</span><span class=p>,</span> <span class=n>translate</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>scale</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>shear</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>interpolation</span><span class=o>=&lt;</span><span class=n>InterpolationMode</span><span class=o>.</span><span class=n>NEAREST</span><span class=p>:</span> <span class=s1>&#39;nearest&#39;</span><span class=o>&gt;</span><span class=p>,</span> <span class=n>fill</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span> <span class=n>fillcolor</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>resample</span><span class=o>=</span><span class=kc>None</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># 可以通过以下函数获取变换矩阵</span>
</span></span><span class=line><span class=cl><span class=n>get_params</span><span class=p>(</span><span class=n>degrees</span><span class=p>:</span> <span class=n>List</span><span class=p>[</span><span class=nb>float</span><span class=p>],</span> <span class=n>translate</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>List</span><span class=p>[</span><span class=nb>float</span><span class=p>]],</span> <span class=n>scale_ranges</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>List</span><span class=p>[</span><span class=nb>float</span><span class=p>]],</span> <span class=n>shears</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>List</span><span class=p>[</span><span class=nb>float</span><span class=p>]],</span> <span class=n>img_size</span><span class=p>:</span> <span class=n>List</span><span class=p>[</span><span class=nb>int</span><span class=p>])</span> <span class=err>→</span> <span class=n>Tuple</span><span class=p>[</span><span class=nb>float</span><span class=p>,</span> <span class=n>Tuple</span><span class=p>[</span><span class=nb>int</span><span class=p>,</span> <span class=nb>int</span><span class=p>],</span> <span class=nb>float</span><span class=p>,</span> <span class=n>Tuple</span><span class=p>[</span><span class=nb>float</span><span class=p>,</span> <span class=nb>float</span><span class=p>]]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>transforms</span><span class=o>.</span><span class=n>RandomHorizontalFlip</span><span class=p>(</span><span class=n>p</span><span class=o>=</span><span class=mf>0.5</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>transforms</span><span class=o>.</span><span class=n>RandomPerspective</span><span class=p>(</span><span class=n>distortion_scale</span><span class=o>=</span><span class=mf>0.5</span><span class=p>,</span> <span class=n>p</span><span class=o>=</span><span class=mf>0.5</span><span class=p>,</span> <span class=n>interpolation</span><span class=o>=&lt;</span><span class=n>InterpolationMode</span><span class=o>.</span><span class=n>BILINEAR</span><span class=p>:</span> <span class=s1>&#39;bilinear&#39;</span><span class=o>&gt;</span><span class=p>,</span> <span class=n>fill</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>RandomResizedCrop</span>
</span></span></code></pre></td></tr></table></div></div></li></ul><h3 id=opencv>OpenCV<a hidden class=anchor aria-hidden=true href=#opencv>#</a></h3><h4 id=install>Install<a hidden class=anchor aria-hidden=true href=#install>#</a></h4><p>Ubuntu：安装opencv的python版本</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>apt-get -y python3-opencv
</span></span><span class=line><span class=cl>pip install opencv-python
</span></span></code></pre></td></tr></table></div></div><h4 id=usage>Usage<a hidden class=anchor aria-hidden=true href=#usage>#</a></h4><p>这一部分可能主要还是对OpenCV基础使用方式的介绍，至于数据增强方面，上面有一个库已经集成了很大一部分图像增强的操作，且在效率上也有了很高的优化和证实，简单的使用方式如下。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>try</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=kn>import</span> <span class=nn>albumentations</span> <span class=k>as</span> <span class=nn>A</span>
</span></span><span class=line><span class=cl><span class=k>except</span> <span class=ne>ImportError</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>os</span><span class=o>.</span><span class=n>system</span><span class=p>(</span><span class=s2>&#34;pip install - U albumentations&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=kn>import</span> <span class=nn>albumentations</span> <span class=k>as</span> <span class=nn>A</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>cv2</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Declare an augmentation pipeline</span>
</span></span><span class=line><span class=cl><span class=n>transform</span> <span class=o>=</span> <span class=n>A</span><span class=o>.</span><span class=n>Compose</span><span class=p>([</span>
</span></span><span class=line><span class=cl>    <span class=n>A</span><span class=o>.</span><span class=n>RandomCrop</span><span class=p>(</span><span class=n>width</span><span class=o>=</span><span class=mi>256</span><span class=p>,</span> <span class=n>height</span><span class=o>=</span><span class=mi>256</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>A</span><span class=o>.</span><span class=n>HorizontalFlip</span><span class=p>(</span><span class=n>p</span><span class=o>=</span><span class=mf>0.5</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>A</span><span class=o>.</span><span class=n>RandomBrightnessContrast</span><span class=p>(</span><span class=n>p</span><span class=o>=</span><span class=mf>0.2</span><span class=p>),</span>
</span></span><span class=line><span class=cl><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Read an image with OpenCV and convert it to the RGB colorspace</span>
</span></span><span class=line><span class=cl><span class=n>image</span> <span class=o>=</span> <span class=n>cv2</span><span class=o>.</span><span class=n>imread</span><span class=p>(</span><span class=s2>&#34;image.jpg&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>image</span> <span class=o>=</span> <span class=n>cv2</span><span class=o>.</span><span class=n>cvtColor</span><span class=p>(</span><span class=n>image</span><span class=p>,</span> <span class=n>cv2</span><span class=o>.</span><span class=n>COLOR_BGR2RGB</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Augment an image</span>
</span></span><span class=line><span class=cl><span class=n>transformed</span> <span class=o>=</span> <span class=n>transform</span><span class=p>(</span><span class=n>image</span><span class=o>=</span><span class=n>image</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>transformed_image</span> <span class=o>=</span> <span class=n>transformed</span><span class=p>[</span><span class=s2>&#34;image&#34;</span><span class=p>]</span>
</span></span></code></pre></td></tr></table></div></div><p>下面我们会讲一些opencv中的简单使用方式：</p><p><a href=https://www.cnblogs.com/silence-cho/p/10926248.html target=_blank rel=noopener>(一)OpenCV-Python学习—基础知识 - silence_cho - 博客园 (cnblogs.com)</a></p><h3 id=pillow>Pillow<a hidden class=anchor aria-hidden=true href=#pillow>#</a></h3><p>Using PIL do <a href=https://zhuanlan.zhihu.com/p/74053773 target=_blank rel=noopener>some augmentation</a></p><ol><li>CCBS： color，contrast，brightness，shapen</li></ol><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># way1：PIL</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>PIL</span> <span class=kn>import</span> <span class=n>ImageEnhance</span>
</span></span><span class=line><span class=cl><span class=n>ccbs_img</span> <span class=o>=</span> <span class=n>ImageEnhance</span><span class=o>.</span><span class=n>Color</span><span class=p>(</span><span class=n>img</span><span class=p>)</span><span class=o>.</span><span class=n>enhance</span><span class=p>(</span><span class=n>factor1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=o>...</span>
</span></span></code></pre></td></tr></table></div></div><ol start=2><li><p>Blur，Detail，Edge_Enhance，Smooth，Sharpen…</p><p>自行在文档中找到对应的方法，实际上不是很多</p></li></ol><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>PIL</span> <span class=kn>import</span> <span class=n>Image</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>PIL.ImageFillter</span> <span class=kn>import</span> <span class=n>SMOOTH</span><span class=p>,</span><span class=n>BLUR</span>
</span></span><span class=line><span class=cl><span class=n>img</span> <span class=o>=</span> <span class=n>Image</span><span class=o>.</span><span class=n>open</span><span class=p>(</span><span class=n>image_pah</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>img_1</span> <span class=o>=</span> <span class=n>img</span><span class=o>.</span><span class=n>filtter</span><span class=p>(</span><span class=n>BLUR</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>img_2</span> <span class=o>=</span> <span class=n>img</span><span class=o>.</span><span class=n>filtter</span><span class=p>(</span><span class=n>SMOOTH</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=skimage>SkImage<a hidden class=anchor aria-hidden=true href=#skimage>#</a></h3><p>&mldr;</p><h2 id=混合图像增强for-ml>混合图像增强（For ML）<a hidden class=anchor aria-hidden=true href=#混合图像增强for-ml>#</a></h2><p>==ALL in one 不是什么好点子==
为了对Machine Learning中的任务进行图像增强任务，我们在过程中可能会使用一些Github Repo，主要可能就是albumation，在进行图像增强和数据混合的过程中，我们会遇到的问题包括：</p><ul><li>PIL和NP，Tensor的三者格式不对应的关系</li><li>Channel混杂的关系</li><li>调用transformer的形式不统一的问题</li></ul><p>为此我们特地开了这个专题，介绍一下使用的方式，以及使我们在后续的使用过程中注意最终转换到datalist中最好采用统一的存储形式</p><ol><li>path（load by cv or pil，st np）</li><li>data （np（else），tensor（cuda）for instant use）</li></ol><p>以此来规范我们的dataset，and sampler or mixer</p><p>我们可以尝试使用如下的方式来进行transformer的管理操作，此外如果我们要用的是纯粹的数据增强而不是用来dataloader的transformer，我们就不要用这个逻辑去做，我们直接集成Augmentation就好了
ch</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>mytransformer</span><span class=p>():</span>
</span></span><span class=line><span class=cl>	<span class=c1># if augs is not none we need to add ToPIL to get the right data type</span>
</span></span><span class=line><span class=cl>	<span class=k>def</span> <span class=nf>get_transformers</span><span class=p>():</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>transformer</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>			<span class=s1>&#39;augs&#39;</span><span class=p>:</span> <span class=n>Augs</span><span class=o>.</span><span class=n>Compose</span><span class=p>([]),</span>
</span></span><span class=line><span class=cl>			<span class=s1>&#39;trans&#39;</span><span class=p>:</span> <span class=n>transformers</span><span class=o>.</span><span class=n>Compose</span><span class=p>([</span>
</span></span><span class=line><span class=cl>				<span class=n>tranformers</span><span class=o>.</span><span class=n>ToPIL</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>				<span class=o>...</span>
</span></span><span class=line><span class=cl>			<span class=p>]),</span>
</span></span><span class=line><span class=cl>		<span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>	<span class=c1># then we using k-v pair to do all my transformer, </span>
</span></span><span class=line><span class=cl>	<span class=k>def</span> <span class=fm>__call__</span><span class=p>(</span><span class=n>img</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		<span class=k>for</span> <span class=n>k</span><span class=p>,</span><span class=n>v</span> <span class=ow>in</span> <span class=n>transformer</span><span class=p>:</span>
</span></span><span class=line><span class=cl>			<span class=n>img</span> <span class=o>=</span> <span class=n>transformer</span><span class=p>[</span><span class=s1>&#39;augs&#39;</span><span class=p>](</span><span class=n>image</span><span class=o>=</span><span class=n>img</span><span class=p>)[</span><span class=s1>&#39;image&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>			<span class=n>img</span> <span class=o>=</span> <span class=n>transformer</span><span class=p>[</span><span class=s1>&#39;trans&#39;</span><span class=p>](</span><span class=n>img</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=k>return</span> <span class=n>img</span>
</span></span><span class=line><span class=cl>	
</span></span></code></pre></td></tr></table></div></div><h2 id=特殊图像增强>特殊图像增强<a hidden class=anchor aria-hidden=true href=#特殊图像增强>#</a></h2><p>&mldr;</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://aikenh.cn/hugotest/tags/augmentation/>Augmentation</a></li><li><a href=https://aikenh.cn/hugotest/tags/pytorch/>Pytorch</a></li><li><a href=https://aikenh.cn/hugotest/tags/machine-learning/>Machine Learning</a></li></ul><nav class=paginav><a class=prev href=https://aikenh.cn/hugotest/posts/linux/><span class=title>« Prev</span><br><span>Linux 基础操作 01</span>
</a><a class=next href=https://aikenh.cn/hugotest/posts/mlflow/><span class=title>Next »</span><br><span>MLFlow</span></a></nav></footer><div id=disqus_thread></div><script>function loadDisqus(){var e=document,t=e.createElement("script");t.src="https://aiken-hugo.disqus.com/embed.js",t.setAttribute("data-timestamp",+new Date),(e.head||e.body).appendChild(t),window.disqus_config=function(){this.page.url=window.location.href,this.page.identifier=window.location.href.substring(18)}}var runningOnBrowser=typeof window!="undefined",isBot=runningOnBrowser&&!("onscroll"in window)||typeof navigator!="undefined"&&/(gle|ing|ro|msn)bot|crawl|spider|yand|duckgo/i.test(navigator.userAgent),supportsIntersectionObserver=runningOnBrowser&&"IntersectionObserver"in window;setTimeout(function(){if(!isBot&&supportsIntersectionObserver){var e=new IntersectionObserver(function(t){t[0].isIntersecting&&(loadDisqus(),e.disconnect())},{threshold:[0]});e.observe(document.getElementById("disqus_thread"))}else loadDisqus()},1)</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by
Disqus.</a></noscript></article></main><footer class=footer><span>&copy; 2024 <a href=https://aikenh.cn/hugotest/>aiken's blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a>
</span><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><span id=busuanzi_container>Visitors: <span id=busuanzi_value_site_uv></span>
Views: <span id=busuanzi_value_site_pv></span></span></footer><script>document.addEventListener("DOMContentLoaded",function(){const e=document.getElementById("busuanzi_value_site_uv"),t=document.getElementById("busuanzi_value_site_pv"),o=13863,i=16993;if(!e||!t){console.error("Busuanzi elements not found.");return}const n=new MutationObserver(e=>{for(let t of e)if(t.type==="childList"){n.disconnect(),t.target.innerHTML=parseInt(t.target.innerHTML||0)+o;break}}),s=new MutationObserver(e=>{for(let t of e)if(t.type==="childList"){s.disconnect(),t.target.innerHTML=parseInt(t.target.innerHTML||0)+i;break}});n.observe(e,{childList:!0}),s.observe(t,{childList:!0})})</script><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><span class=topInner><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
<span id=read_progress></span>
</span></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))}),document.getElementById("theme-toggle-nav").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script><script>document.addEventListener("scroll",function(){const t=document.getElementById("read_progress"),n=document.documentElement.scrollHeight,s=document.documentElement.clientHeight,o=document.documentElement.scrollTop||document.body.scrollTop;t.innerText=((o/(n-s)).toFixed(2)*100).toFixed(0)})</script><script>(function(e,t){var s=document,o="script",n=s.createElement(o),i=s.getElementsByTagName(o)[0];n.src=e,t&&n.addEventListener("load",function(e){t(e)}),i.parentNode.insertBefore(n,i)})("/js/pangu.js",function(){pangu.spacingPage()})</script></body></html>